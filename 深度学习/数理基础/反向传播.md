为了清晰梳理神经网络反向传播的核心公式,以下按“前向传播→损失函数→误差项(δ)→参数梯度→参数更新”的逻辑,结合**多层网络(L层)** 和**多个样本(小批量)** 场景,统一符号并推导关键公式。


### 一、基础符号定义(通用)
- **层数**:输入层为第0层,隐藏层为第1~L-1层,输出层为第L层。  
- **样本**:小批量含\( B \)个样本,第\( i \)个样本记为\( (x_i, y_i) \)(\( i=1,2,...,B \))。  
- **变量**(第\( l \)层,第\( i \)个样本):  
  - 输入/激活值:\( a_i^{(l)} \)(向量,维度=第\( l \)层神经元数),输入层\( a_i^{(0)} = x_i \)。  
  - 加权和:\( z_i^{(l)} = w^{(l)} \cdot a_i^{(l-1)} + b^{(l)} \)(向量,与\( a_i^{(l)} \)同维度)。  
  - 激活函数:\( a_i^{(l)} = \sigma_l(z_i^{(l)}) \)(\( \sigma_l \)为第\( l \)层激活函数,如sigmoid、softmax)。  
  - 权重矩阵:\( w^{(l)} \)(维度=第\( l \)层神经元数 x 第\( l-1 \)层神经元数)。  
  - 偏置向量:\( b^{(l)} \)(维度=第\( l \)层神经元数 x 1)。  


### 二、前向传播公式(单样本)
对第\( i \)个样本,从输入层到输出层的计算:  
\[
\begin{align*}
z_i^{(1)} &= w^{(1)} \cdot a_i^{(0)} + b^{(1)} \quad \text{(第1层加权和)} \\
a_i^{(1)} &= \sigma_1(z_i^{(1)}) \quad \text{(第1层激活值)} \\
&\vdots \\
z_i^{(L)} &= w^{(L)} \cdot a_i^{(L-1)} + b^{(L)} \quad \text{(输出层加权和)} \\
a_i^{(L)} &= \sigma_L(z_i^{(L)}) \quad \text{(输出层激活值,即预测结果)}
\end{align*}
\]


### 三、损失函数(总损失=批量样本损失的平均)
设第\( i \)个样本的损失为\( C_i \),小批量总损失:  
\[ C_{\text{总}} = \frac{1}{B} \sum_{i=1}^B C_i \]  


#### 1. 多分类任务(输出层用 $softmax$,\( K \)个类别)
- 输出层激活值(概率):\( a_i^{(L)} = \text{softmax}(z_i^{(L)}) \),即\( a_{i,k}^{(L)} = \frac{e^{z_{i,k}^{(L)}}}{\sum_{j=1}^K e^{z_{i,j}^{(L)}}} \)(\( k \)为类别索引)。  
- 单个样本损失(交叉熵):\( C_i = -\sum_{k=1}^K y_{i,k} \cdot \log(a_{i,k}^{(L)}) \)(\( y_i \)为独热编码,正确类别\( y_{i,c}=1 \),其余为0)。  


#### 2. 二分类任务(输出层用sigmoid,1个输出)
- 输出层激活值:\( a_i^{(L)} = \sigma(z_i^{(L)}) = \frac{1}{1+e^{-z_i^{(L)}}} \)。  
- 单个样本损失(二元交叉熵):\( C_i = -[y_i \cdot \log(a_i^{(L)}) + (1-y_i) \cdot \log(1-a_i^{(L)})] \)。  


#### 3. 回归任务(输出层无激活,连续值)
- 输出层激活值:\( a_i^{(L)} = z_i^{(L)} \)(直接输出加权和)。  
- 单个样本损失(均方误差MSE):\( C_i = \frac{1}{2} \| a_i^{(L)} - y_i \|^2 \)(\( \| \cdot \| \)为L2范数)。  


### 四、误差项(δ):损失对加权和\( z \)的偏导(核心！)
误差项\( \delta_i^{(l)} = \frac{\partial C_i}{\partial z_i^{(l)}} \),是反向传播的“误差载体”,从输出层反向计算到第1层。  


#### 1. 输出层(\( l=L \))的δ(单样本)
根据输出层激活函数和损失函数,分场景推导:  

- **多分类(softmax+交叉熵)**:  
  因\( \frac{\partial C_i}{\partial z_{i,k}^{(L)}} = a_{i,k}^{(L)} - y_{i,k} \),故:  
  \[ \delta_i^{(L)} = a_i^{(L)} - y_i \quad \text{(向量,维度=输出层神经元数)} \]  

- **二分类(sigmoid+二元交叉熵)**:  
  \( \frac{\partial C_i}{\partial z_i^{(L)}} = a_i^{(L)} - y_i \),故:  
  \[ \delta_i^{(L)} = a_i^{(L)} - y_i \quad \text{(标量,因输出层1个神经元)} \]  

- **回归(无激活+MSE)**:  
  \( \frac{\partial C_i}{\partial z_i^{(L)}} = a_i^{(L)} - y_i \),故:  
  \[ \delta_i^{(L)} = a_i^{(L)} - y_i \quad \text{(向量,维度=输出层神经元数)} \]  


#### 2. 隐藏层(\( l=1,2,...,L-1 \))的δ(单样本)
通过下一层的δ和权重矩阵转置传递误差,结合当前层激活函数的导数:  
\[ \delta_i^{(l)} = \left( (w^{(l+1)})^T \cdot \delta_i^{(l+1)} \right) \odot \sigma_l'(z_i^{(l)}) \]  
- \( (w^{(l+1)})^T \):下一层权重矩阵的转置(维度=第\( l \)层神经元数 x 第\( l+1 \)层神经元数),用于“反向传递误差”。  
- \( \odot \):逐元素相乘(Hadamard积)。  
- \( \sigma_l'(z_i^{(l)}) \):第\( l \)层激活函数的导数(如sigmoid导数\( \sigma'(z) = a(1-a) \),即\( \sigma_l'(z_i^{(l)}) = a_i^{(l)} \odot (1 - a_i^{(l)}) \))。  


#### 3. 小批量的平均δ(用于多样本梯度聚合)
对批量内所有样本的δ求平均,得到总损失对\( z^{(l)} \)的平均偏导:  
\[ \delta_{\text{平均}}^{(l)} = \frac{1}{B} \sum_{i=1}^B \delta_i^{(l)} \]  


### 五、参数梯度(权重\( w \)和偏置\( b \)的梯度)
梯度是参数更新的依据,总梯度=批量内单个样本梯度的平均。  


#### 1. 偏置\( b^{(l)} \)的梯度(单样本+批量)
- 单样本梯度:偏置直接影响\( z \),故\( \frac{\partial C_i}{\partial b^{(l)}} = \delta_i^{(l)} \)。  
- 批量平均梯度:  
  \[ \frac{\partial C_{\text{总}}}{\partial b^{(l)}} = \delta_{\text{平均}}^{(l)} = \frac{1}{B} \sum_{i=1}^B \delta_i^{(l)} \]  


#### 2. 权重\( w^{(l)} \)的梯度(单样本+批量)
- 单样本梯度:权重通过\( z = w \cdot a_{\text{前}} + b \)影响损失,故\( \frac{\partial C_i}{\partial w^{(l)}} = \delta_i^{(l)} \cdot (a_i^{(l-1)})^T \)(外积,矩阵维度与\( w^{(l)} \)一致)。  
- 批量平均梯度:  
  \[ \frac{\partial C_{\text{总}}}{\partial w^{(l)}} = \frac{1}{B} \sum_{i=1}^B \left( \delta_i^{(l)} \cdot (a_i^{(l-1)})^T \right) \]  


### 六、参数更新公式(梯度下降)
用批量平均梯度更新权重和偏置,\( \eta \)为学习率:  
\[
\begin{align*}
w_{\text{新}}^{(l)} &= w_{\text{旧}}^{(l)} - \eta \cdot \frac{\partial C_{\text{总}}}{\partial w^{(l)}} \\
b_{\text{新}}^{(l)} &= b_{\text{旧}}^{(l)} - \eta \cdot \frac{\partial C_{\text{总}}}{\partial b^{(l)}}
\end{align*}
\]


### 核心逻辑链
1. **前向传播**:用\( w, b \)计算各层\( z, a \),得到输出\( a^{(L)} \)。  
2. **损失计算**:用\( a^{(L)} \)和真实标签\( y \)计算总损失\( C_{\text{总}} \)。  
3. **反向传播**:  
   - 从输出层开始,计算\( \delta_i^{(L)} \)(依赖损失和激活函数)。  
   - 逐层向前计算隐藏层\( \delta_i^{(l)} \)(用下一层\( \delta \)和权重转置传递)。  
4. **梯度聚合**:对批量内所有样本的\( w, b \)梯度求平均。  
5. **参数更新**:用平均梯度和学习率更新\( w, b \),最小化损失。  

所有公式均围绕“链式法则”展开,误差项\( \delta \)是连接各层梯度的核心载体,多样本场景通过“平均梯度”平衡效率与稳定性。