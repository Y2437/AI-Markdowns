### **算法设计与分析综合笔记 - 大纲**

**第一部分：算法基础**

*   **1. 算法的角色与基础**
    *   1.1. 算法的定义：计算过程、解决问题的工具。
    *   1.2. 算法分析的重要性：效率（时间与空间）、硬件与算法的关系。
    *   1.3. 贯穿全篇的核心问题：排序（插入排序 vs. 归并排序）、大数乘法、斐波那契数列等。

*   **2. 算法分析框架**
    *   2.1. 计算模型：RAM（随机存取机）模型及其基本指令。
    *   2.2. 算法描述：伪代码规范。
    *   2.3. 正确性分析：循环不变式（以插入排序为例）。
    *   2.4. 性能分析：最坏情况、平均情况分析。

*   **3. 函数的增长与渐进记法**
    *   3.1. 渐进记法：O, Ω, Θ, o, ω 的定义与图解。
    *   3.2. 记法间的关系：定理与性质（传递性、自反性、对称性等）。
    *   3.3. 常见函数增长率比较：对数、多项式、指数。

*   **4. 递归与分治策略**
    *   4.1. 分治思想：分解、解决、合并。
    *   4.2. 递归式分析：
        *   4.2.1. 代入法：求解递归式的猜测与验证。
        *   4.2.2. 递归树方法：以归并排序为例。
        *   4.2.3. 主方法（Master Theorem）：定理内容、三种情况及应用举例。

*   **5. 概率分析与随机算法**
    *   5.1. 雇佣问题：最坏情况与概率分析。
    *   5.2. 指示器随机变量：定义、引理及应用（期望线性性）。
    *   5.3. 随机算法：随机排列数组（排序思想、原地算法）。

**第二部分：排序与顺序统计**

*   **6. 排序算法**
    *   6.1. 堆排序：堆的性质、维护堆、建堆、排序过程与性能分析。
    *   6.2. 快速排序：划分思想、算法实现、性能分析（最坏、最佳、平均情况）、随机化快排。
    *   6.3. 比较排序的下界：决策树模型与Ω(n log n)的证明。

*   **7. 中位数与顺序统计**
    *   7.1. 查找最小值和最大值。
    *   7.2. 期望为线性时间的选择算法：RANDOMIZED-SELECT。

**第三部分：高级设计与分析技术**

*   **8. 动态规划 (Dynamic Programming)**
    *   8.1. 核心思想：最优子结构、重叠子问题。
    *   8.2. 钢条切割：问题描述、递归解法、带备忘的自顶向下法、自底向上法。
    *   8.3. 矩阵链乘法：问题描述、刻画最优解结构、递归解、计算最优代价、构造最优解。
    *   8.4. 最长公共子序列 (LCS)：问题描述、最优子结构、递归解、计算LCS长度、构造LCS。
    *   8.5. 最优二叉搜索树 (OBST)：问题描述、最优子结构、递归解、计算期望搜索代价。

*   **9. 贪心算法 (Greedy Algorithms)**
    *   9.1. 核心思想：贪心选择性质、最优子结构。
    *   9.2. 活动选择问题：问题描述、最优子结构、贪心选择、递归与迭代贪心算法。
    *   9.3. 贪心算法基础：与动态规划的对比。
    *   9.4. 霍夫曼编码 (Huffman Codes)：前缀码、编码树、贪心算法构造过程与正确性。
    *   9.5. 背包问题：0-1背包问题（动态规划解）与分数背包问题（贪心解）的对比。

*   **10. 摊还分析 (Amortized Analysis)**
    *   10.1. 聚合分析：栈操作、二进制计数器。
    *   10.2. 核算法（记账法）：栈操作、二进制计数器。
    *   10.3. 势能法：栈操作、二进制计数器、动态表。

**第四部分：图算法**

*   **11. 基本图算法**
    *   11.1. 图的表示：邻接链表、邻接矩阵。
    *   11.2. 广度优先搜索 (BFS)：算法过程、属性（最短路径）、广度优先树。
    *   11.3. 深度优先搜索 (DFS)：算法过程、属性（时间戳、括号定理）、边的分类。
    *   11.4. 拓扑排序。
    *   11.5. 强连通分量。

*   **12. 最小生成树 (MST)**
    *   12.1. Kruskal 算法。
    *   12.2. Prim 算法。

*   **13. 单源最短路径**
    *   13.1. 最短路径与松弛操作 (Relaxation)。
    *   13.2. Bellman-Ford 算法：处理负权边。
    *   13.3. 有向无环图 (DAG) 的单源最短路径。
    *   13.4. Dijkstra 算法：贪心策略。

*   **14. 所有结点对的最短路径**
    *   14.1. 基于矩阵乘法的解法。
    *   14.2. Floyd-Warshall 算法。

*   **15. 最大流**
    *   15.1. 流网络与最大流问题。
    *   15.2. Ford-Fulkerson 方法：残留网络、增广路径、最大流最小割定理。
    *   15.3. Edmonds-Karp 算法。
    *   15.4. 最大二分匹配。

**第五部分：特定领域算法**

*   **16. 字符串匹配**
    *   16.1. 朴素算法。
    *   16.2. Rabin-Karp 算法。
    *   16.3. 有限自动机方法。
    *   16.4. Knuth-Morris-Pratt (KMP) 算法。

*   **17. 计算几何**
    *   17.1. 线段的性质：叉积及其应用。
    *   17.2. 判断线段是否相交。
    *   17.3. 凸包问题：Graham 扫描法、Jarvis 步进法。

*   **18. 多项式与快速傅里叶变换 (FFT)**
    *   18.1. 多项式表示：系数表示法、点值表示法。
    *   18.2. 离散傅里叶变换 (DFT) 与逆变换。
    *   18.3. 快速傅里叶变换 (FFT)：分治策略、迭代实现。

---

好的，我们开始第一部分的笔记内容。本部分将涵盖大纲中的“1. 算法的角色与基础”和“2. 算法分析框架”，为后续所有内容的学习奠定理论基础。

---

### **第一部分：算法基础**

#### **1. 算法的角色与基础**

##### **1.1. 算法的定义与特性**

1.  **定义**：
    *   一个**算法 (Algorithm)** 是一个良定义的 (well-defined) 计算过程，它接收一个或多个值作为**输入 (Input)**，并产生一个或多个值作为**输出 (Output)**。
    *   换言之，算法是将输入转换为输出的一系列计算步骤。
    *   算法可以被视为解决一个明确定义的**计算问题 (Computational Problem)** 的工具。问题陈述了期望的输入/输出关系，而算法则描述了一个具体的计算过程，以实现该关系。

2.  **实例：排序问题**
    *   **问题描述**：将一个由n个数字组成的序列 $<a₁, a₂, ..., aₙ>$ 重新排列成一个非递减序列 $<a'₁, a'₂, ..., a'ₙ>$，使得 $a'₁ ≤ a'₂ ≤ ... ≤ a'ₙ$。
    *   **输入**：一个n个数的序列。
    *   **输出**：输入序列的一个排列（重排序），满足非递减顺序。
    *   **实例 (Instance)**：输入序列 $<31, 41, 59, 26, 41, 58>$ 是排序问题的一个实例。
    *   **算法**：插入排序 (Insertion Sort) 和归并排序 (Merge Sort) 都是解决此问题的具体算法。

3.  **算法的特性**：
    *   **正确性 (Correctness)**：一个正确的算法对于每一个合法的输入实例，都会在有限时间内**停机 (halt)** 并产生正确的输出。
    *   **可行性 (Feasible)**：算法的每一步都必须是清晰、明确且可执行的。
    *   **输出 (Output)**：一个算法必须有至少一个输出。
    *   **不正确的算法**：对于某些输入可能不会停机，或者停机但给出了错误的答案。在某些情况下，如果其错误率可以被控制，这类算法也可能有用。

##### **1.2. 算法作为一种技术**

算法是计算机科学的核心技术。其效率的重要性甚至可以超越硬件的进步。

*   **效率的重要性**：
    *   不同算法在解决同一问题时，其效率可能存在巨大差异。例如，在处理大规模数据时（如人类基因组测序，n ≈ 3×10⁹），一个运行时间为 O(n²) 的算法（如插入排序）和一个运行时间为 O(n log n) 的算法（如归并排序）在性能上会有天壤之别。
    *   **计算示例**：假设一台计算机每秒执行 10⁹ 条指令。
        *   对于插入排序（最坏情况），所需时间约为 $(3×10⁹)² / 10⁹ = 9×10⁹$ 秒，约等于 **285.39年**。
        *   对于归并排序，所需时间约为 $(3×10⁹ * log(3×10⁹)) / 10⁹ ≈ 3 * log(3×10⁹)$ 秒，约等于 **94.45秒**。
    *   这个例子表明，算法的选择对效率的影响远大于硬件性能的提升。一个高效的算法可以在普通硬件上快速完成任务，而一个低效的算法即使在最快的硬件上也可能无法在可接受的时间内完成。

*   **算法与其他技术的关系**：
    *   现代计算机技术，如高速硬件架构（流水线、超标量）、图形用户界面（GUI）、面向对象系统、网络路由等，其底层实现都严重依赖于高效的算法。

#### **2. 算法分析框架**

为了科学地研究和比较算法，我们需要一个统一的框架来描述和分析它们。

##### **2.1. 计算模型：RAM模型**

为了使分析独立于具体的计算机硬件，我们采用一个通用的抽象计算模型——**随机存取机 (Random-Access Machine, RAM)** 模型。

*   **核心特性**：
    *   **单处理器**：指令是顺序执行的，不存在并发操作。
    *   **基本指令**：包含算术运算（加、减、乘、除、取余）、数据移动（加载、存储、复制）和控制流（条件分支、子程序调用、返回）等常见指令。
    *   **恒定时间**：模型假设执行每一条基本指令都需要一个**常数时间**。

##### **2.2. 算法描述：伪代码**

我们使用伪代码来描述算法，它比自然语言更精确，比具体的编程语言更简洁、更易于理解。

*   **伪代码约定**：
    *   **缩进**：表示代码块结构。
    *   **循环**：$for$, $while$, $repeat$ 等。
    *   **条件**：$if$, $then$, $else$。
    *   **注释**：以 $//$ 或 $▷$ 开头。
    *   **变量**：作用域通常局限于所在过程。
    *   **数组访问**：通过 $A[i]$ 访问单个元素，$A[1..j]$ 表示子数组。
    *   **对象与属性**：复合数据通过对象及其属性（如 $node.key$）来组织。
    *   **参数传递**：按值传递。

##### **2.3. 正确性分析：循环不变式**

**循环不变式 (Loop Invariant)** 是一种用于证明算法（特别是循环结构）正确性的重要工具。它是一个在循环的每次迭代中都保持为真的性质。证明循环不变式需要验证三个属性：

1.  **初始化 (Initialization)**：在循环的第一次迭代开始之前，循环不变式为真。
2.  **保持 (Maintenance)**：如果在某一次迭代开始前循环不变式为真，那么在下一次迭代开始前，它仍然为真。
3.  **终止 (Termination)**：当循环终止时，循环不变式提供一个有用的性质，可以帮助证明算法的正确性。

*   **实例：插入排序的正确性证明**
    *   **算法**：$INSERTION-SORT(A)$
    *   **循环不变式**：在 $for$ 循环的每次迭代开始时，子数组 $A[1..j-1]$ 包含了原来在 $A[1..j-1]$ 中的元素，但现在是已排序的状态。
    *   **证明**：
        1.  **初始化**：当 $j=2$ 时，循环第一次开始。子数组 $A[1..1]$ 只包含一个元素，它本身就是有序的。因此不变式成立。
        2.  **保持**：假设在某次迭代开始前，$A[1..j-1]$ 是有序的。循环体内部会将 $A[j]$ 插入到已排序的子数组 $A[1..j-1]$ 的正确位置，从而形成一个新的、长度为 $j$ 的有序子数组 $A[1..j]$。因此，在下一次迭代（此时 $j$ 增加1）开始前，新的 $A[1..j-1]$ 仍然是有序的。
        3.  **终止**：循环在 $j = n+1$ 时终止。根据循环不变式，此时子数组 $A[1..n]$ 包含了原来 $A[1..n]$ 中的所有元素，并且是已排序的。这正是我们期望算法完成的目标。

##### **2.4. 性能分析**

分析算法意味着预测其所需的资源，最主要的是**运行时间 (Running Time)**。

*   **输入规模 (Input Size)**：用 $n$ 表示，其具体含义取决于问题，例如数组中的元素个数、图中的顶点和边数等。
*   **运行时间 T(n)**：在RAM模型下，定义为算法在输入规模为 $n$ 时执行的基本操作（或“步”）的数量。
*   **分析类型**：
    *   **最坏情况分析 (Worst-case analysis)**：分析在所有规模为 $n$ 的输入中，算法的最长运行时间。这是最常用的分析方法，因为它提供了一个运行时间的上界保证。
    *   **平均情况分析 (Average-case analysis)**：分析在所有规模为 $n$ 的输入上，算法的期望运行时间。这需要对输入的分布做出假设。

好的，接下来是第三部分的内容，我们将深入探讨用于描述算法运行时间增长趋势的数学工具——渐进记法。

---

### **第三部分：函数的增长与渐进记法**

在算法分析中，我们更关心的是当输入规模 $n$ 变得非常大时，运行时间的增长率，而不是其精确值。**渐进效率 (Asymptotic Efficiency)** 描述了这种在大输入规模下的行为。我们通过忽略低阶项和常数因子来抽象和简化分析。

#### **3.1. Θ-记法 (渐近紧界)**

**定义**：对于一个给定的函数 $g(n)$，$Θ(g(n))$ 表示一个函数集合：
$Θ(g(n)) = {f(n) : 存在正常数 c₁, c₂, 和 n₀，使得对于所有 n ≥ n₀，有 0 ≤ c₁g(n) ≤ f(n) ≤ c₂g(n)}$

*   **含义**：如果 $f(n) = Θ(g(n))$，我们称 $g(n)$ 是 $f(n)$ 的一个**渐近紧界 (Asymptotically Tight Bound)**。这意味着当 $n$ 足够大时，$f(n)$ 的增长率与 $g(n)$ 的增长率相同，仅相差一个常数因子。
*   **图示**：函数 $f(n)$ 被夹在 $c₁g(n)$ 和 $c₂g(n)$ 两条曲线之间。
*   **示例**：证明 $n²/2 - 3n = Θ(n²)$。
    我们需要找到 $c₁, c₂, n₀$ 使得 $c₁n² ≤ n²/2 - 3n ≤ c₂n²$ 对所有 $n ≥ n₀$ 成立。
    将不等式两边同除以 $n²$，得到 $c₁ ≤ 1/2 - 3/n ≤ c₂$。
    *   当 $n$ 增大时，$3/n$ 减小。
    *   为保证 $1/2 - 3/n > 0$，需要 $n > 6$。
    *   取 $n₀ = 7$。当 $n ≥ 7$ 时，$1/2 - 3/n ≥ 1/2 - 3/7 = 1/14$。
    *   因此，我们可以选择 $c₁ = 1/14$。
    *   同时，$1/2 - 3/n ≤ 1/2$，所以我们可以选择 $c₂ = 1/2$。
    *   综上，当 $c₁ = 1/14$, $c₂ = 1/2$, $n₀ = 7$ 时，不等式成立。故 $n²/2 - 3n = Θ(n²)$。

#### **3.2. O-记法 (渐近上界)**

**定义**：对于一个给定的函数 $g(n)$，$O(g(n))$ 表示一个函数集合：
$O(g(n)) = {f(n) : 存在正常数 c 和 n₀，使得对于所有 n ≥ n₀，有 0 ≤ f(n) ≤ cg(n)}$

*   **含义**：如果 $f(n) = O(g(n))$，我们称 $g(n)$ 是 $f(n)$ 的一个**渐近上界 (Asymptotic Upper Bound)**。这意味着 $f(n)$ 的增长率**不会快于** $g(n)$ 的增长率。
*   **注意**：$O-记法$ 描述的是上界，但不一定是紧界。例如，$2n = O(n²)$ 是正确的，但 $n²$ 并不是 $2n$ 的紧界。
*   **关系**：如果 $f(n) = Θ(g(n))$，那么必然有 $f(n) = O(g(n))$。即 $Θ(g(n)) ⊆ O(g(n))$。

#### **3.3. Ω-记法 (渐近下界)**

**定义**：对于一个给定的函数 $g(n)$，$Ω(g(n))$ 表示一个函数集合：
$Ω(g(n)) = {f(n) : 存在正常数 c 和 n₀，使得对于所有 n ≥ n₀，有 0 ≤ cg(n) ≤ f(n)}$

*   **含义**：如果 $f(n) = Ω(g(n))$，我们称 $g(n)$ 是 $f(n)$ 的一个**渐近下界 (Asymptotic Lower Bound)**。这意味着 $f(n)$ 的增长率**不会慢于** $g(n)$ 的增长率。
*   **定理 3.1**：对于任意两个函数 $f(n)$ 和 $g(n)$，$f(n) = Θ(g(n))$ 的充要条件是 $f(n) = O(g(n))$ 且 $f(n) = Ω(g(n))$。

#### **3.4. o-记法 (非渐近紧上界)**

**定义**：对于一个给定的函数 $g(n)$，$o(g(n))$ 表示一个函数集合：
$o(g(n)) = {f(n) : 对于任意正常数 c > 0，都存在一个常数 n₀ > 0，使得对于所有 n ≥ n₀，有 0 ≤ f(n) < cg(n)}$

*   **含义**：$o-记法$ 表示一个**非渐近紧的上界**。如果 $f(n) = o(g(n))$，意味着 $f(n)$ 的增长率**远小于** $g(n)$ 的增长率。
*   **与 O-记法的区别**：$O-记法$ 中，$f(n) ≤ cg(n)$ 只需要对**某个**常数 $c$ 成立；而 $o-记法$ 中，$f(n) < cg(n)$ 必须对**任意**常数 $c$ 成立。
*   **极限定义**：如果 $lim (f(n) / g(n)) = 0$ (当 $n → ∞$)，则 $f(n) = o(g(n))$。
*   **示例**：$2n = o(n²)$，因为 $lim (2n / n²) = lim (2/n) = 0$。但 $2n² ≠ o(n²)$。

#### **3.5. ω-记法 (非渐近紧下界)**

**定义**：对于一个给定的函数 $g(n)$，$ω(g(n))$ 表示一个函数集合：
$ω(g(n)) = {f(n) : 对于任意正常数 c > 0，都存在一个常数 n₀ > 0，使得对于所有 n ≥ n₀，有 0 ≤ cg(n) < f(n)}$

*   **含义**：$ω-记法$ 表示一个**非渐近紧的下界**。如果 $f(n) = ω(g(n))$，意味着 $f(n)$ 的增长率**远大于** $g(n)$ 的增长率。
*   **极限定义**：如果 $lim (f(n) / g(n)) = ∞$ (当 $n → ∞$)，则 $f(n) = ω(g(n))$。
*   **示例**：$n²/2 = ω(n)$，因为 $lim ((n²/2) / n) = lim (n/2) = ∞$。但 $n²/2 ≠ ω(n²)$。

#### **3.6. 渐近记法的性质**

*   **传递性 (Transitivity)**：
    *   $f(n) = Θ(g(n))$ 且 $g(n) = Θ(h(n))$ ⇒ $f(n) = Θ(h(n))$
    *   (同样适用于 O, Ω, o, ω)
*   **自反性 (Reflexivity)**：
    *   $f(n) = Θ(f(n))$
    *   $f(n) = O(f(n))$
    *   $f(n) = Ω(f(n))$
*   **对称性 (Symmetry)**：
    *   $f(n) = Θ(g(n))$ 当且仅当 $g(n) = Θ(f(n))$
*   **转置对称性 (Transpose Symmetry)**：
    *   $f(n) = O(g(n))$ 当且仅当 $g(n) = Ω(f(n))$
    *   $f(n) = o(g(n))$ 当且仅当 $g(n) = ω(f(n))$

#### **3.7. 常见函数增长率比较**

以下是常见函数按增长率从低到高的排序：
$1 < log n < √n < n < n log n < n² < n³ < ... < 2ⁿ < 3ⁿ < ... < n!$

### **第四部分：递归与分治策略**

#### **4.1. 分治策略 (Divide-and-Conquer)**

分治是一种重要的算法设计范式。它将一个难以直接解决的大问题，分割成一些规模较小的相同问题，以便于分而治之。

分治算法的执行过程遵循三个步骤：
1.  **分解 (Divide)**：将原问题分解为若干个规模较小、相互独立、且与原问题形式相同的子问题。
2.  **解决 (Conquer)**：若子问题规模足够小，则直接求解。否则，递归地求解各个子问题。
3.  **合并 (Combine)**：将各个子问题的解合并，从而构成原问题的解。

**归并排序 (Merge Sort)** 是分治策略的一个经典应用：
*   **分解**：将待排序的 $n$ 个元素的序列分解成各含 $n/2$ 个元素的两个子序列。
*   **解决**：递归地使用归并排序对两个子序列进行排序。
*   **合并**：合并两个已排序的子序列，以产生最终的排序结果。

#### **4.2. 递归式分析**

**递归式 (Recurrence)** 是一个等式或不等式，它通过更小的输入的函数值来描述一个函数。在算法分析中，递归式常用于描述分治算法的运行时间。
例如，归并排序的运行时间可以描述为：
$T(n) = 2T(n/2) + \Theta(n)$

我们介绍三种求解递归式的方法：

##### **4.2.1. 代入法 (Substitution Method)**

代入法包含两个步骤：
1.  **猜测**解的形式。
2.  用**数学归纳法**证明猜测的正确性。

*   **示例**：求解 $T(n) = 2T(\lfloor n/2 \rfloor) + n$。
    1.  **猜测**：我们猜测解为 $T(n) = O(n \log n)$。
    2.  **证明**：我们需要证明存在常数 $c > 0$，使得 $T(n) \le cn \log n$。
        *   **归纳假设**：假设对于所有小于 $n$ 的正整数 $m$，该猜测成立，即 $T(m) \le cm \log m$。
        *   **归纳步骤**：
            $T(n) \le 2(c \lfloor n/2 \rfloor \log(\lfloor n/2 \rfloor)) + n$
            $\le 2(c (n/2) \log(n/2)) + n$
            $= cn \log(n/2) + n$
            $= cn(\log n - \log 2) + n$
            $= cn \log n - cn + n$
            $\le cn \log n$
            这个不等式在 $cn - n \ge 0$，即 $c \ge 1$ 时成立。
        *   **边界条件**：我们需要处理边界情况。当 $n=1$ 时，$T(1) = 1$。但我们的证明要求 $T(1) \le c \cdot 1 \cdot \log 1 = 0$，这产生了矛盾。

*   **处理边界条件的技巧**：
    渐进记法只要求我们对 $n \ge n_0$ 的情况证明即可。我们可以选择一个足够大的 $n_0$，使得对于 $n < n_0$ 的情况，我们可以直接验证。例如，对于 $T(2)$ 和 $T(3)$，我们可以直接计算其值，然后选择一个足够大的 $c$ 使得归纳假设在这些点上成立。

*   **加强归纳假设**：
    有时，直接的归纳证明会失败。例如，对于 $T(n) = T(\lfloor n/2 \rfloor) + T(\lceil n/2 \rceil) + 1$，如果我们猜测 $T(n) \le cn$，则会得到 $T(n) \le c(\lfloor n/2 \rfloor) + c(\lceil n/2 \rceil) + 1 = cn + 1$，这无法证明 $T(n) \le cn$。
    解决方法是**减去一个低阶项**来加强归纳假设。我们猜测 $T(n) \le cn - b$，其中 $b \ge 0$ 是一个常数。
    $T(n) \le (c\lfloor n/2 \rfloor - b) + (c\lceil n/2 \rceil - b) + 1 = cn - 2b + 1$
    只要我们选择 $b \ge 1$，那么 $cn - 2b + 1 \le cn - b$。这样归纳证明就可以继续。

##### **4.2.2. 递归树方法 (Recursion-Tree Method)**

递归树是一种将递归式迭代过程可视化的方法，非常适合用于生成一个好的猜测，然后可以用代入法来验证。

*   **构建过程**：
    1.  树的根节点代表原始问题，其代价为 $f(n)$。
    2.  根节点的子节点代表其递归调用产生的子问题。
    3.  持续展开这个过程，直到达到递归的边界条件。

*   **示例**：求解 $T(n) = 2T(n/2) + n$。
    *   **树的结构**：这是一个完全二叉树。
    *   **每层代价**：
        *   第0层（根）：代价为 $n$。
        *   第1层：有两个子问题，每个规模为 $n/2$，总代价为 $2 \times (n/2) = n$。
        *   第 $i$ 层：有 $2^i$ 个子问题，每个规模为 $n/2^i$，总代价为 $2^i \times (n/2^i) = n$。
    *   **树的高度**：当子问题规模降为1时，即 $n/2^k = 1$，解得 $k = \log_2 n$。所以树的高度为 $\log n$。
    *   **总代价**：将所有层的代价相加。共有 $\log n + 1$ 层，每层代价为 $n$（最后一层除外，但可以合并计算）。总代价约为 $n \times \log n$。因此，我们猜测 $T(n) = \Theta(n \log n)$。

##### **4.2.3. 主方法 (Master Method)**

主方法为形如 $T(n) = aT(n/b) + f(n)$ 的递归式提供了一种“菜谱式”的求解方案，其中 $a \ge 1, b > 1$ 是常数，$f(n)$ 是一个渐进正函数。

主方法需要比较函数 $f(n)$ 和 $n^{\log_b a}$ 的大小。共有三种情况：

1.  **情况1**：如果存在常数 $\epsilon > 0$，使得 $f(n) = O(n^{\log_b a - \epsilon})$，则 $T(n) = \Theta(n^{\log_b a})$。
    *   **直观理解**：$f(n)$ 的增长率被 $n^{\log_b a}$ **多项式地**压制。递归树的代价主要由叶子节点决定。
    *   **示例**：$T(n) = 9T(n/3) + n$。
        这里 $a=9, b=3, f(n)=n$。$n^{\log_b a} = n^{\log_3 9} = n^2$。
        $f(n) = n = O(n^{2-1})$，满足情况1（$\epsilon=1$）。
        因此，$T(n) = \Theta(n^2)$。

2.  **情况2**：如果 $f(n) = \Theta(n^{\log_b a})$，则 $T(n) = \Theta(n^{\log_b a} \log n)$。
    *   **直观理解**：$f(n)$ 的增长率与 $n^{\log_b a}$ 相同。递归树的每一层代价大致相等。
    *   **示例**：$T(n) = T(2n/3) + 1$。
        这里 $a=1, b=3/2, f(n)=1$。$n^{\log_b a} = n^{\log_{3/2} 1} = n^0 = 1$。
        $f(n) = 1 = \Theta(n^0)$，满足情况2。
        因此，$T(n) = \Theta(n^0 \log n) = \Theta(\log n)$。

3.  **情况3**：如果存在常数 $\epsilon > 0$，使得 $f(n) = \Omega(n^{\log_b a + \epsilon})$，并且对于某个常数 $c < 1$ 和所有足够大的 $n$，满足**正则条件** $af(n/b) \le cf(n)$，则 $T(n) = \Theta(f(n))$。
    * **直观理解**：$f(n)$ 的增长率**多项式地**超过 $n^{\log_b a}$，并且 $f(n)$ 的增长足够“规则”。递归树的代价主要由根节点的代价决定。
    
    *   **示例**：$T(n) = 3T(n/4) + n \log n$。
        这里 $a=3, b=4, f(n)=n \log n$。$n^{\log_b a} = n^{\log_4 3} \approx n^{0.793}$。
        $f(n) = n \log n = \Omega(n^{0.793 + \epsilon})$（例如取 $\epsilon=0.2$）。
        检查正则条件：$3(n/4)\log(n/4) \le c(n \log n)$。取 $c=3/4$ 时，对于足够大的 $n$，该条件成立。
        因此，$T(n) = \Theta(n \log n)$。
        

好的，我们继续第五部分的内容。本部分将介绍概率分析和随机算法，这两种技术在处理不确定性或避免最坏情况时非常有用。

---

### **第五部分：概率分析与随机算法**

#### **5.1. 雇佣问题 (The Hiring Problem)**

这是一个经典的例子，用来说明概率分析和随机算法的应用。

*   **问题描述**：
    *   你需要雇佣一名新的办公助理。
    *   一家雇佣代理每天会给你推荐一位候选人，共推荐 $n$ 位。
    *   你需要面试每一位候选人，并立即决定是否雇佣他/她。
    *   如果决定雇佣，你必须解雇当前的助理，然后雇佣新的。
    *   面试需要支付给代理一笔固定的费用（面试费），雇佣新助理也需要一笔固定的费用（雇佣费）。雇佣费远高于面试费。
    *   **目标**：雇佣到最优秀的候选人，同时希望总费用（特别是雇佣费用）尽可能低。

*   **算法**：
    1.  面试第一位候选人，并直接雇佣他/她。
    2.  对于后续的每一位候选人，如果他/她比当前雇佣的助理更优秀，则解雇当前助理，雇佣这位新的候选人。

*   **成本分析**：
    *   设面试成本为 $c_i$，雇佣成本为 $c_h$。
    *   总面试成本为 $n \cdot c_i$。
    *   总雇佣成本为 $m \cdot c_h$，其中 $m$ 是雇佣的次数。
    *   总成本 $T(n) = n c_i + m c_h$。

*   **最坏情况分析**：
    如果候选人按能力递增的顺序出现，那么每次面试后都会进行一次雇佣。此时 $m=n$，总成本达到最大。

#### **5.2. 概率分析 (Probabilistic Analysis)**

概率分析是在**假设输入服从某种概率分布**的前提下，分析算法的期望性能。

*   **假设**：我们假设 $n$ 位候选人以一个**随机的顺序**出现。这意味着每一种排列都是等可能的（共有 $n!$ 种排列，每种出现的概率为 $1/n!$）。

*   **指示器随机变量 (Indicator Random Variables)**：
    这是一个强大的分析工具。对于一个事件 $A$，其指示器随机变量 $I\{A\}$ 定义为：
    $I\{A\} = \begin{cases} 1 & \text{如果 } A \text{ 发生} \\ 0 & \text{如果 } A \text{ 不发生} \end{cases}$

    **引理**：一个事件 $A$ 的指示器随机变量的期望值等于该事件发生的概率：$E[I\{A\}] = \text{Pr}\{A\}$。

*   **分析雇佣问题的期望雇佣次数**：
    1.  令 $X$ 为我们雇佣新助理的总次数的随机变量。
    2.  令 $X_i$ 为一个指示器随机变量，表示第 $i$ 位候选人被雇佣的事件。即 $X_i = I\{\text{candidate } i \text{ is hired}\}$。
    3.  那么总雇佣次数 $X = \sum_{i=1}^{n} X_i$。
    4.  根据期望的线性性，$E[X] = E[\sum_{i=1}^{n} X_i] = \sum_{i=1}^{n} E[X_i]$。
    5.  根据引理，$E[X_i] = \text{Pr}\{\text{candidate } i \text{ is hired}\}$。
    6.  第 $i$ 位候选人被雇佣的条件是：他/她比前 $i-1$ 位所有候选人都优秀。由于候选人是随机排列的，前 $i$ 位候选人中的任何一位都等可能地是这 $i$ 位中最优秀的。因此，第 $i$ 位候选人是前 $i$ 位中最优秀的概率是 $1/i$。
    7.  所以，$\text{Pr}\{\text{candidate } i \text{ is hired}\} = 1/i$。
    8.  因此，$E[X] = \sum_{i=1}^{n} (1/i)$。
    9.  这个和是**调和级数 (Harmonic Series)**，$H_n = \sum_{i=1}^{n} (1/i) \approx \ln n + O(1)$。

*   **结论**：在输入随机的假设下，雇佣新助理的平均次数约为 $\ln n$。这远小于最坏情况下的 $n$ 次。

#### **5.3. 随机算法 (Randomized Algorithms)**

与概率分析不同，随机算法不依赖于输入的分布，而是**在算法内部使用随机性**来影响其执行流程。这使得算法对于任何输入，其期望性能都是固定的。

*   **核心思想**：与其假设输入是随机的，不如通过算法来**强制**产生一个随机的排列。

*   **应用于雇佣问题**：
    1.  在开始面试之前，将 $n$ 位候选人的列表进行一次随机置换。
    2.  然后按照这个新的随机顺序依次面试。

*   **分析**：
    *   由于我们人为地创造了一个随机排列，其分析过程与上述概率分析完全相同。
    *   期望的雇佣次数仍然是 $H_n \approx \ln n$。
    *   **关键区别**：这个期望值现在不依赖于输入候选人的初始顺序，而是由算法内部的随机数生成器保证。对于**任何**输入，算法的期望性能都是好的。

*   **随机排列数组**：
    如何生成一个均匀随机排列？
    1.  **Permute-by-Sorting**：
        *   为数组 $A$ 的每个元素 $A[i]$ 赋予一个随机的优先级 $P[i]$。
        *   根据这些优先级对数组 $A$ 进行排序。
        *   如果所有优先级都唯一，则可以产生均匀随机排列。为了保证唯一性，优先级通常在一个较大的范围内选取，例如 $[1, n^3]$。
        *   时间复杂度为 $O(n \log n)$（主要由排序决定）。

    2.  **Randomize-in-Place** (原地随机置换)：
        这是一个更高效的 $O(n)$ 算法。
        ```
        RANDOMIZE-IN-PLACE(A)
        n = A.length
        for i = 1 to n
            swap A[i] with A[RANDOM(i, n)]
        ```
        *   **循环不变式**：在 `for` 循环的第 $i$ 次迭代开始之前，对于每个可能的 $(i-1)$-排列，子数组 $A[1..i-1]$ 包含该 $(i-1)$-排列的概率为 $(n-i+1)!/n!$。
        *   通过这个循环不变式可以证明，该算法最终产生的是一个均匀随机排列。


---

### **第二部分：排序与顺序统计**

#### **6. 排序算法**

##### **6.1. 堆排序 (Heapsort)**

堆排序是一种高效的、基于比较的排序算法。它利用了一种名为“堆”的数据结构。

*   **堆 (Heap)**：
    *   堆可以看作是一个近似**完全二叉树 (Complete Binary Tree)**，它可以通过一个数组来表示。
    *   **数组表示**：对于数组中索引为 $i$ 的节点：
        *   `PARENT(i)` 返回 $\lfloor i/2 \rfloor$
        *   `LEFT(i)` 返回 $2i$
        *   `RIGHT(i)` 返回 $2i+1$
    *   **堆的性质**：
        *   **最大堆 (Max-Heap)**：除了根节点外，每个节点 $i$ 的值都小于或等于其父节点的值，即 $A[\text{PARENT}(i)] \ge A[i]$。这意味着堆中的最大元素存储在根节点 $A$。
        *   **最小堆 (Min-Heap)**：每个节点的值都大于或等于其父节点的值。

*   **堆排序中的核心操作**：
    1.  **`MAX-HEAPIFY(A, i)`**：
        *   **功能**：维护最大堆的性质。
        *   **前提**：假设以 `LEFT(i)` 和 `RIGHT(i)` 为根的两个子树都已经是最大堆。
        *   **过程**：让 $A[i]$ 的值在最大堆中“逐级下降”，直到以 $i$ 为根的子树也满足最大堆性质。它通过比较 $A[i]$ 与其左右孩子的值，找出最大者，如果最大者不是 $A[i]$，则交换它们，并对被交换下去的子树递归调用 `MAX-HEAPIFY`。
        *   **运行时间**：$O(\log n)$，因为操作路径是从根到叶子的一条路径，其长度与树高 $h = \Theta(\log n)$ 成正比。

    2.  **`BUILD-MAX-HEAP(A)`**：
        *   **功能**：将一个无序的输入数组构建成一个最大堆。
        *   **过程**：对数组中所有非叶子节点（从 $\lfloor n/2 \rfloor$ 到 1）倒序调用 `MAX-HEAPIFY`。
        *   **运行时间**：虽然直观上是 $O(n \log n)$（调用了约 $n/2$ 次 `MAX-HEAPIFY`），但更紧确的界是 $O(n)$。这是因为大部分节点的树高都很小。

*   **堆排序算法 (`HEAPSORT(A)`)**：
    1.  调用 `BUILD-MAX-HEAP(A)` 将输入数组构建成一个最大堆。
    2.  进行 $n-1$ 次循环：
        *   将堆的根元素 $A$（当前最大元素）与堆的最后一个元素 $A[i]$ 交换。
        *   将堆的大小减 1（逻辑上将已排序的最大元素从堆中移除）。
        *   对新的根节点 $A$ 调用 `MAX-HEAPIFY(A, 1)` 来维护最大堆性质。

*   **性能分析**：
    *   `BUILD-MAX-HEAP` 的时间为 $O(n)$。
    *   `for` 循环执行 $n-1$ 次，每次 `MAX-HEAPIFY` 的时间为 $O(\log n)$。
    *   总运行时间为 $O(n) + (n-1)O(\log n) = O(n \log n)$。

##### **6.2. 快速排序 (Quicksort)**

快速排序是另一种基于分治思想的排序算法，在实践中通常是最高效的排序算法之一。

*   **核心思想**：
    *   **分解**：通过一个**划分 (Partition)** 过程，选取一个主元 (pivot)，将数组划分为两个（可能为空的）子数组 $A[p..q-1]$ 和 $A[q+1..r]$，使得前者中的每个元素都小于等于 $A[q]$，后者中的每个元素都大于等于 $A[q]$。下标 $q$ 在划分过程中确定。
    *   **解决**：通过递归调用快速排序，对子数组 $A[p..q-1]$ 和 $A[q+1..r]$ 进行排序。
    *   **合并**：因为子数组是就地排序的，所以不需要合并操作。整个数组在递归调用结束后就已经排好序。

*   **划分 (`PARTITION(A, p, r)`)**：
    *   这是快速排序的关键。一个经典的实现（如Lomuto划分）是选取数组最后一个元素 $A[r]$ 作为主元。
    *   维护一个索引 $i$，使得 $A[p..i]$ 中的元素都小于等于主元。
    *   遍历 $A[p..r-1]$，如果发现一个元素小于等于主元，则将其与 $A[i+1]$ 交换，并递增 $i$。
    *   最后将主元与 $A[i+1]$ 交换。
    *   **运行时间**：$\Theta(n)$，其中 $n=r-p+1$。

*   **性能分析**：
    *   **最坏情况**：当划分过程每次都产生一个 $n-1$ 个元素和一个 0 个元素的子问题时（例如，当数组已经排好序或逆序，且总是选择第一个或最后一个元素为主元）。
        递归式为 $T(n) = T(n-1) + \Theta(n)$，解为 $T(n) = \Theta(n^2)$。
    *   **最佳情况**：当划分过程每次都产生两个大小为 $n/2$ 的子问题。
        递归式为 $T(n) = 2T(n/2) + \Theta(n)$，解为 $T(n) = \Theta(n \log n)$。
    *   **平均情况**：即使划分不是完美的，例如每次都按 9:1 的比例划分，递归式为 $T(n) = T(9n/10) + T(n/10) + \Theta(n)$，其解仍然是 $T(n) = \Theta(n \log n)$。可以证明，在所有输入排列都是等概率的情况下，快速排序的期望运行时间是 $\Theta(n \log n)$。

*   **随机化快速排序 (Randomized Quicksort)**：
    为了避免最坏情况的频繁发生（例如对于特定模式的输入），可以引入随机性。
    *   **方法**：在划分之前，从子数组中随机选择一个元素作为主元，并将其与最后一个元素交换。
    *   **效果**：这使得算法的运行时间不依赖于输入的初始排列，而是依赖于随机数生成器的输出。对于任何输入，其期望运行时间都是 $\Theta(n \log n)$。

##### **6.3. 比较排序的下界**

*   **比较排序 (Comparison Sort)**：这类算法仅通过比较元素之间的相对大小来确定排序顺序（如插入排序、归并排序、堆排序、快速排序）。

*   **决策树模型 (Decision-Tree Model)**：
    *   任何一个比较排序算法都可以被抽象为一个**决策树**。
    *   树的每个内部节点代表一次比较（如 $a_i \le a_j$）。
    *   树的每个叶子节点代表一种可能的排序结果（一个输入元素的排列）。
    *   对于一个特定的输入，算法的执行对应于从根到某个叶子的一条路径。
    *   算法在最坏情况下的比较次数等于决策树的高度。

*   **下界证明**：
    1.  对于 $n$ 个不同的元素，共有 $n!$ 种可能的排列。
    2.  因此，决策树必须至少有 $n!$ 个叶子节点来覆盖所有可能的输出。
    3.  一个高度为 $h$ 的二叉树最多有 $2^h$ 个叶子节点。
    4.  所以，$n! \le 2^h$。
    5.  两边取对数，得到 $h \ge \log(n!)$。
    6.  根据斯特林近似公式，$n! \approx \sqrt{2\pi n} (n/e)^n$，可以得出 $\log(n!) = \Omega(n \log n)$。
    7.  因此，$h = \Omega(n \log n)$。

*   **结论**：任何基于比较的排序算法，在最坏情况下都需要至少 $\Omega(n \log n)$ 次比较。堆排序和归并排序都是渐近最优的比较排序算法。

好的，我们继续第七部分的内容。本部分将讨论在一组数据中查找特定顺序统计量的问题，特别是中位数。

### **第二部分：排序与顺序统计 (续)**

#### **7. 中位数与顺序统计 (Medians and Order Statistics)**

##### **7.1. 问题定义**

*   **第 $i$ 顺序统计量 (i-th Order Statistic)**：在一个包含 $n$ 个元素的集合中，第 $i$ 小的元素。
    *   **最小值 (Minimum)** 是第 1 顺序统计量 ($i=1$)。
    *   **最大值 (Maximum)** 是第 $n$ 顺序统计量 ($i=n$)。
    *   **中位数 (Median)** 是集合的“中间”元素。
        *   当 $n$ 为奇数时，中位数是唯一的，位于 $i=(n+1)/2$ 的位置。
        *   当 $n$ 为偶数时，存在两个中位数，分别位于 $i=n/2$（下中位数）和 $i=n/2+1$（上中位数）的位置。

*   **选择问题 (Selection Problem)**：给定一个包含 $n$ 个（不一定不同）数的集合和一个整数 $i$（$1 \le i \le n$），找出该集合中第 $i$ 小的元素。

##### **7.2. 查找最小值和最大值**

*   **查找最小值**：
    可以通过一次遍历完成，需要 $n-1$ 次比较。
    ```
    MINIMUM(A)
    min = A[1]
    for i = 2 to A.length
        if min > A[i]
            min = A[i]
    return min
    ```
*   **同时查找最小值和最大值**：
    *   **朴素方法**：分别调用 `MINIMUM` 和 `MAXIMUM`，总共需要 $2(n-1)$ 次比较。
    *   **更优方法**：成对处理元素。
        1.  将输入元素两两配对。
        2.  对每一对进行一次比较，得到较大者和较小者。
        3.  在所有较大者中寻找最大值，在所有较小者中寻找最小值。
        *   **比较次数**：
            *   配对比较：$\lfloor n/2 \rfloor$ 次。
            *   找最大值：$\lceil n/2 \rceil - 1$ 次。
            *   找最小值：$\lceil n/2 \rceil - 1$ 次。
            *   总次数约为 $3n/2$ 次，优于 $2n-2$ 次。

##### **7.3. 期望为线性时间的选择算法**

我们可以通过排序然后在 $O(1)$ 时间内索引第 $i$ 个元素来解决选择问题，总时间为 $O(n \log n)$。但我们可以做得更好。

`RANDOMIZED-SELECT` 算法利用了与快速排序相同的划分思想，但只处理划分后的一部分。

*   **算法 `RANDOMIZED-SELECT(A, p, r, i)`**：
    *   **功能**：在子数组 `A[p..r]` 中查找第 $i$ 小的元素。
    1.  如果 $p=r$，则返回 $A[p]$。
    2.  使用 `RANDOMIZED-PARTITION` 将数组 `A[p..r]` 划分为两个子数组 `A[p..q-1]` 和 `A[q+1..r]`，其中 $A[q]$ 是主元。
    3.  计算主元 $A[q]$ 在子数组 `A[p..r]` 中的位次 $k = q - p + 1$。
    4.  **比较 $i$ 和 $k$**：
        *   如果 $i = k$，则主元 $A[q]$ 就是要找的第 $i$ 小的元素，返回 $A[q]$。
        *   如果 $i < k$，则第 $i$ 小的元素位于左侧子数组 `A[p..q-1]` 中。递归调用 `RANDOMIZED-SELECT(A, p, q-1, i)`。
        *   如果 $i > k$，则第 $i$ 小的元素位于右侧子数组 `A[q+1..r]` 中。我们需要在该子数组中寻找第 $(i-k)$ 小的元素。递归调用 `RANDOMIZED-SELECT(A, q+1, r, i-k)`。

*   **性能分析**：
    *   **最坏情况**：与快速排序类似，如果每次划分都极不均衡（例如，划分出的子问题规模为 $n-1$），则算法需要递归 $n-1$ 次。
        递归式为 $T(n) = T(n-1) + \Theta(n)$，解为 $T(n) = \Theta(n^2)$。
    *   **期望运行时间**：由于每次只对划分后的一边进行递归，我们可以期望算法的运行时间是线性的。
        *   **直观分析**：最好的情况是每次都划分为一半，递归式为 $T(n) = T(n/2) + \Theta(n)$，解为 $T(n) = \Theta(n)$。即使最好和最坏情况交替出现，平均性能也很好。
        *   **严格分析**：使用指示器随机变量可以证明，`RANDOMIZED-SELECT` 的期望运行时间为 $O(n)$。分析的关键在于，一个元素被选为主元的概率是均匀的，因此期望情况下，每次划分都能有效地减小问题规模。





