### **算法设计与分析综合笔记 - 大纲**

**第一部分：算法基础**

*   **1. 算法的角色与基础**
    *   1.1. 算法的定义：计算过程、解决问题的工具。
    *   1.2. 算法分析的重要性：效率（时间与空间）、硬件与算法的关系。
    *   1.3. 贯穿全篇的核心问题：排序（插入排序 vs. 归并排序）、大数乘法、斐波那契数列等。

*   **2. 算法分析框架**
    *   2.1. 计算模型：RAM（随机存取机）模型及其基本指令。
    *   2.2. 算法描述：伪代码规范。
    *   2.3. 正确性分析：循环不变式（以插入排序为例）。
    *   2.4. 性能分析：最坏情况、平均情况分析。

*   **3. 函数的增长与渐进记法**
    *   3.1. 渐进记法：O, Ω, Θ, o, ω 的定义与图解。
    *   3.2. 记法间的关系：定理与性质（传递性、自反性、对称性等）。
    *   3.3. 常见函数增长率比较：对数、多项式、指数。

*   **4. 递归与分治策略**
    *   4.1. 分治思想：分解、解决、合并。
    *   4.2. 递归式分析：
        *   4.2.1. 代入法：求解递归式的猜测与验证。
        *   4.2.2. 递归树方法：以归并排序为例。
        *   4.2.3. 主方法（Master Theorem）：定理内容、三种情况及应用举例。

*   **5. 概率分析与随机算法**
    *   5.1. 雇佣问题：最坏情况与概率分析。
    *   5.2. 指示器随机变量：定义、引理及应用（期望线性性）。
    *   5.3. 随机算法：随机排列数组（排序思想、原地算法）。

**第二部分：排序与顺序统计**

*   **6. 排序算法**
    *   6.1. 堆排序：堆的性质、维护堆、建堆、排序过程与性能分析。
    *   6.2. 快速排序：划分思想、算法实现、性能分析（最坏、最佳、平均情况）、随机化快排。
    *   6.3. 比较排序的下界：决策树模型与Ω(n log n)的证明。

*   **7. 中位数与顺序统计**
    *   7.1. 查找最小值和最大值。
    *   7.2. 期望为线性时间的选择算法：RANDOMIZED-SELECT。

**第三部分：高级设计与分析技术**

*   **8. 动态规划 (Dynamic Programming)**
    *   8.1. 核心思想：最优子结构、重叠子问题。
    *   8.2. 钢条切割：问题描述、递归解法、带备忘的自顶向下法、自底向上法。
    *   8.3. 矩阵链乘法：问题描述、刻画最优解结构、递归解、计算最优代价、构造最优解。
    *   8.4. 最长公共子序列 (LCS)：问题描述、最优子结构、递归解、计算LCS长度、构造LCS。
    *   8.5. 最优二叉搜索树 (OBST)：问题描述、最优子结构、递归解、计算期望搜索代价。

*   **9. 贪心算法 (Greedy Algorithms)**
    *   9.1. 核心思想：贪心选择性质、最优子结构。
    *   9.2. 活动选择问题：问题描述、最优子结构、贪心选择、递归与迭代贪心算法。
    *   9.3. 贪心算法基础：与动态规划的对比。
    *   9.4. 霍夫曼编码 (Huffman Codes)：前缀码、编码树、贪心算法构造过程与正确性。
    *   9.5. 背包问题：0-1背包问题（动态规划解）与分数背包问题（贪心解）的对比。

*   **10. 摊还分析 (Amortized Analysis)**
    *   10.1. 聚合分析：栈操作、二进制计数器。
    *   10.2. 核算法（记账法）：栈操作、二进制计数器。
    *   10.3. 势能法：栈操作、二进制计数器、动态表。

**第四部分：图算法**

*   **11. 基本图算法**
    *   11.1. 图的表示：邻接链表、邻接矩阵。
    *   11.2. 广度优先搜索 (BFS)：算法过程、属性（最短路径）、广度优先树。
    *   11.3. 深度优先搜索 (DFS)：算法过程、属性（时间戳、括号定理）、边的分类。
    *   11.4. 拓扑排序。
    *   11.5. 强连通分量。

*   **12. 最小生成树 (MST)**
    *   12.1. Kruskal 算法。
    *   12.2. Prim 算法。

*   **13. 单源最短路径**
    *   13.1. 最短路径与松弛操作 (Relaxation)。
    *   13.2. Bellman-Ford 算法：处理负权边。
    *   13.3. 有向无环图 (DAG) 的单源最短路径。
    *   13.4. Dijkstra 算法：贪心策略。

*   **14. 所有结点对的最短路径**
    *   14.1. 基于矩阵乘法的解法。
    *   14.2. Floyd-Warshall 算法。

*   **15. 最大流**
    *   15.1. 流网络与最大流问题。
    *   15.2. Ford-Fulkerson 方法：残留网络、增广路径、最大流最小割定理。
    *   15.3. Edmonds-Karp 算法。
    *   15.4. 最大二分匹配。

**第五部分：特定领域算法**

*   **16. 字符串匹配**
    *   16.1. 朴素算法。
    *   16.2. Rabin-Karp 算法。
    *   16.3. 有限自动机方法。
    *   16.4. Knuth-Morris-Pratt (KMP) 算法。

*   **17. 计算几何**
    *   17.1. 线段的性质：叉积及其应用。
    *   17.2. 判断线段是否相交。
    *   17.3. 凸包问题：Graham 扫描法、Jarvis 步进法。

*   **18. 多项式与快速傅里叶变换 (FFT)**
    *   18.1. 多项式表示：系数表示法、点值表示法。
    *   18.2. 离散傅里叶变换 (DFT) 与逆变换。
    *   18.3. 快速傅里叶变换 (FFT)：分治策略、迭代实现。

---

好的，我们开始第一部分的笔记内容。本部分将涵盖大纲中的“1. 算法的角色与基础”和“2. 算法分析框架”，为后续所有内容的学习奠定理论基础。

---

### **第一部分：算法基础**

#### **1. 算法的角色与基础**

##### **1.1. 算法的定义与特性**

1.  **定义**：
    *   一个**算法 (Algorithm)** 是一个良定义的 (well-defined) 计算过程，它接收一个或多个值作为**输入 (Input)**，并产生一个或多个值作为**输出 (Output)**。
    *   换言之，算法是将输入转换为输出的一系列计算步骤。
    *   算法可以被视为解决一个明确定义的**计算问题 (Computational Problem)** 的工具。问题陈述了期望的输入/输出关系，而算法则描述了一个具体的计算过程，以实现该关系。

2.  **实例：排序问题**
    *   **问题描述**：将一个由n个数字组成的序列 $<a₁, a₂, ..., aₙ>$ 重新排列成一个非递减序列 $<a'₁, a'₂, ..., a'ₙ>$，使得 $a'₁ ≤ a'₂ ≤ ... ≤ a'ₙ$。
    *   **输入**：一个n个数的序列。
    *   **输出**：输入序列的一个排列（重排序），满足非递减顺序。
    *   **实例 (Instance)**：输入序列 $<31, 41, 59, 26, 41, 58>$ 是排序问题的一个实例。
    *   **算法**：插入排序 (Insertion Sort) 和归并排序 (Merge Sort) 都是解决此问题的具体算法。

3.  **算法的特性**：
    *   **正确性 (Correctness)**：一个正确的算法对于每一个合法的输入实例，都会在有限时间内**停机 (halt)** 并产生正确的输出。
    *   **可行性 (Feasible)**：算法的每一步都必须是清晰、明确且可执行的。
    *   **输出 (Output)**：一个算法必须有至少一个输出。
    *   **不正确的算法**：对于某些输入可能不会停机，或者停机但给出了错误的答案。在某些情况下，如果其错误率可以被控制，这类算法也可能有用。

##### **1.2. 算法作为一种技术**

算法是计算机科学的核心技术。其效率的重要性甚至可以超越硬件的进步。

*   **效率的重要性**：
    *   不同算法在解决同一问题时，其效率可能存在巨大差异。例如，在处理大规模数据时（如人类基因组测序，n ≈ 3×10⁹），一个运行时间为 O(n²) 的算法（如插入排序）和一个运行时间为 O(n log n) 的算法（如归并排序）在性能上会有天壤之别。
    *   **计算示例**：假设一台计算机每秒执行 10⁹ 条指令。
        *   对于插入排序（最坏情况），所需时间约为 $(3×10⁹)² / 10⁹ = 9×10⁹$ 秒，约等于 **285.39年**。
        *   对于归并排序，所需时间约为 $(3×10⁹ * log(3×10⁹)) / 10⁹ ≈ 3 * log(3×10⁹)$ 秒，约等于 **94.45秒**。
    *   这个例子表明，算法的选择对效率的影响远大于硬件性能的提升。一个高效的算法可以在普通硬件上快速完成任务，而一个低效的算法即使在最快的硬件上也可能无法在可接受的时间内完成。

*   **算法与其他技术的关系**：
    *   现代计算机技术，如高速硬件架构（流水线、超标量）、图形用户界面（GUI）、面向对象系统、网络路由等，其底层实现都严重依赖于高效的算法。

#### **2. 算法分析框架**

为了科学地研究和比较算法，我们需要一个统一的框架来描述和分析它们。

##### **2.1. 计算模型：RAM模型**

为了使分析独立于具体的计算机硬件，我们采用一个通用的抽象计算模型——**随机存取机 (Random-Access Machine, RAM)** 模型。

*   **核心特性**：
    *   **单处理器**：指令是顺序执行的，不存在并发操作。
    *   **基本指令**：包含算术运算（加、减、乘、除、取余）、数据移动（加载、存储、复制）和控制流（条件分支、子程序调用、返回）等常见指令。
    *   **恒定时间**：模型假设执行每一条基本指令都需要一个**常数时间**。

##### **2.2. 算法描述：伪代码**

我们使用伪代码来描述算法，它比自然语言更精确，比具体的编程语言更简洁、更易于理解。

*   **伪代码约定**：
    *   **缩进**：表示代码块结构。
    *   **循环**：$for$, $while$, $repeat$ 等。
    *   **条件**：$if$, $then$, $else$。
    *   **注释**：以 $//$ 或 $▷$ 开头。
    *   **变量**：作用域通常局限于所在过程。
    *   **数组访问**：通过 $A[i]$ 访问单个元素，$A[1..j]$ 表示子数组。
    *   **对象与属性**：复合数据通过对象及其属性（如 $node.key$）来组织。
    *   **参数传递**：按值传递。

##### **2.3. 正确性分析：循环不变式**

**循环不变式 (Loop Invariant)** 是一种用于证明算法（特别是循环结构）正确性的重要工具。它是一个在循环的每次迭代中都保持为真的性质。证明循环不变式需要验证三个属性：

1.  **初始化 (Initialization)**：在循环的第一次迭代开始之前，循环不变式为真。
2.  **保持 (Maintenance)**：如果在某一次迭代开始前循环不变式为真，那么在下一次迭代开始前，它仍然为真。
3.  **终止 (Termination)**：当循环终止时，循环不变式提供一个有用的性质，可以帮助证明算法的正确性。

*   **实例：插入排序的正确性证明**
    *   **算法**：$INSERTION-SORT(A)$
    *   **循环不变式**：在 $for$ 循环的每次迭代开始时，子数组 $A[1..j-1]$ 包含了原来在 $A[1..j-1]$ 中的元素，但现在是已排序的状态。
    *   **证明**：
        1.  **初始化**：当 $j=2$ 时，循环第一次开始。子数组 $A[1..1]$ 只包含一个元素，它本身就是有序的。因此不变式成立。
        2.  **保持**：假设在某次迭代开始前，$A[1..j-1]$ 是有序的。循环体内部会将 $A[j]$ 插入到已排序的子数组 $A[1..j-1]$ 的正确位置，从而形成一个新的、长度为 $j$ 的有序子数组 $A[1..j]$。因此，在下一次迭代（此时 $j$ 增加1）开始前，新的 $A[1..j-1]$ 仍然是有序的。
        3.  **终止**：循环在 $j = n+1$ 时终止。根据循环不变式，此时子数组 $A[1..n]$ 包含了原来 $A[1..n]$ 中的所有元素，并且是已排序的。这正是我们期望算法完成的目标。

##### **2.4. 性能分析**

分析算法意味着预测其所需的资源，最主要的是**运行时间 (Running Time)**。

*   **输入规模 (Input Size)**：用 $n$ 表示，其具体含义取决于问题，例如数组中的元素个数、图中的顶点和边数等。
*   **运行时间 T(n)**：在RAM模型下，定义为算法在输入规模为 $n$ 时执行的基本操作（或“步”）的数量。
*   **分析类型**：
    *   **最坏情况分析 (Worst-case analysis)**：分析在所有规模为 $n$ 的输入中，算法的最长运行时间。这是最常用的分析方法，因为它提供了一个运行时间的上界保证。
    *   **平均情况分析 (Average-case analysis)**：分析在所有规模为 $n$ 的输入上，算法的期望运行时间。这需要对输入的分布做出假设。

好的，接下来是第三部分的内容，我们将深入探讨用于描述算法运行时间增长趋势的数学工具——渐进记法。

---

### **第三部分：函数的增长与渐进记法**

在算法分析中，我们更关心的是当输入规模 $n$ 变得非常大时，运行时间的增长率，而不是其精确值。**渐进效率 (Asymptotic Efficiency)** 描述了这种在大输入规模下的行为。我们通过忽略低阶项和常数因子来抽象和简化分析。

#### **3.1. Θ-记法 (渐近紧界)**

**定义**：对于一个给定的函数 $g(n)$，$Θ(g(n))$ 表示一个函数集合：
$Θ(g(n)) = {f(n) : 存在正常数 c₁, c₂, 和 n₀，使得对于所有 n ≥ n₀，有 0 ≤ c₁g(n) ≤ f(n) ≤ c₂g(n)}$

*   **含义**：如果 $f(n) = Θ(g(n))$，我们称 $g(n)$ 是 $f(n)$ 的一个**渐近紧界 (Asymptotically Tight Bound)**。这意味着当 $n$ 足够大时，$f(n)$ 的增长率与 $g(n)$ 的增长率相同，仅相差一个常数因子。
*   **图示**：函数 $f(n)$ 被夹在 $c₁g(n)$ 和 $c₂g(n)$ 两条曲线之间。
*   **示例**：证明 $n²/2 - 3n = Θ(n²)$。
    我们需要找到 $c₁, c₂, n₀$ 使得 $c₁n² ≤ n²/2 - 3n ≤ c₂n²$ 对所有 $n ≥ n₀$ 成立。
    将不等式两边同除以 $n²$，得到 $c₁ ≤ 1/2 - 3/n ≤ c₂$。
    *   当 $n$ 增大时，$3/n$ 减小。
    *   为保证 $1/2 - 3/n > 0$，需要 $n > 6$。
    *   取 $n₀ = 7$。当 $n ≥ 7$ 时，$1/2 - 3/n ≥ 1/2 - 3/7 = 1/14$。
    *   因此，我们可以选择 $c₁ = 1/14$。
    *   同时，$1/2 - 3/n ≤ 1/2$，所以我们可以选择 $c₂ = 1/2$。
    *   综上，当 $c₁ = 1/14$, $c₂ = 1/2$, $n₀ = 7$ 时，不等式成立。故 $n²/2 - 3n = Θ(n²)$。

#### **3.2. O-记法 (渐近上界)**

**定义**：对于一个给定的函数 $g(n)$，$O(g(n))$ 表示一个函数集合：
$O(g(n)) = {f(n) : 存在正常数 c 和 n₀，使得对于所有 n ≥ n₀，有 0 ≤ f(n) ≤ cg(n)}$

*   **含义**：如果 $f(n) = O(g(n))$，我们称 $g(n)$ 是 $f(n)$ 的一个**渐近上界 (Asymptotic Upper Bound)**。这意味着 $f(n)$ 的增长率**不会快于** $g(n)$ 的增长率。
*   **注意**：$O-记法$ 描述的是上界，但不一定是紧界。例如，$2n = O(n²)$ 是正确的，但 $n²$ 并不是 $2n$ 的紧界。
*   **关系**：如果 $f(n) = Θ(g(n))$，那么必然有 $f(n) = O(g(n))$。即 $Θ(g(n)) ⊆ O(g(n))$。

#### **3.3. Ω-记法 (渐近下界)**

**定义**：对于一个给定的函数 $g(n)$，$Ω(g(n))$ 表示一个函数集合：
$Ω(g(n)) = {f(n) : 存在正常数 c 和 n₀，使得对于所有 n ≥ n₀，有 0 ≤ cg(n) ≤ f(n)}$

*   **含义**：如果 $f(n) = Ω(g(n))$，我们称 $g(n)$ 是 $f(n)$ 的一个**渐近下界 (Asymptotic Lower Bound)**。这意味着 $f(n)$ 的增长率**不会慢于** $g(n)$ 的增长率。
*   **定理 3.1**：对于任意两个函数 $f(n)$ 和 $g(n)$，$f(n) = Θ(g(n))$ 的充要条件是 $f(n) = O(g(n))$ 且 $f(n) = Ω(g(n))$。

#### **3.4. o-记法 (非渐近紧上界)**

**定义**：对于一个给定的函数 $g(n)$，$o(g(n))$ 表示一个函数集合：
$o(g(n)) = {f(n) : 对于任意正常数 c > 0，都存在一个常数 n₀ > 0，使得对于所有 n ≥ n₀，有 0 ≤ f(n) < cg(n)}$

*   **含义**：$o-记法$ 表示一个**非渐近紧的上界**。如果 $f(n) = o(g(n))$，意味着 $f(n)$ 的增长率**远小于** $g(n)$ 的增长率。
*   **与 O-记法的区别**：$O-记法$ 中，$f(n) ≤ cg(n)$ 只需要对**某个**常数 $c$ 成立；而 $o-记法$ 中，$f(n) < cg(n)$ 必须对**任意**常数 $c$ 成立。
*   **极限定义**：如果 $lim (f(n) / g(n)) = 0$ (当 $n → ∞$)，则 $f(n) = o(g(n))$。
*   **示例**：$2n = o(n²)$，因为 $lim (2n / n²) = lim (2/n) = 0$。但 $2n² ≠ o(n²)$。

#### **3.5. ω-记法 (非渐近紧下界)**

**定义**：对于一个给定的函数 $g(n)$，$ω(g(n))$ 表示一个函数集合：
$ω(g(n)) = {f(n) : 对于任意正常数 c > 0，都存在一个常数 n₀ > 0，使得对于所有 n ≥ n₀，有 0 ≤ cg(n) < f(n)}$

*   **含义**：$ω-记法$ 表示一个**非渐近紧的下界**。如果 $f(n) = ω(g(n))$，意味着 $f(n)$ 的增长率**远大于** $g(n)$ 的增长率。
*   **极限定义**：如果 $lim (f(n) / g(n)) = ∞$ (当 $n → ∞$)，则 $f(n) = ω(g(n))$。
*   **示例**：$n²/2 = ω(n)$，因为 $lim ((n²/2) / n) = lim (n/2) = ∞$。但 $n²/2 ≠ ω(n²)$。

#### **3.6. 渐近记法的性质**

*   **传递性 (Transitivity)**：
    *   $f(n) = Θ(g(n))$ 且 $g(n) = Θ(h(n))$ ⇒ $f(n) = Θ(h(n))$
    *   (同样适用于 O, Ω, o, ω)
*   **自反性 (Reflexivity)**：
    *   $f(n) = Θ(f(n))$
    *   $f(n) = O(f(n))$
    *   $f(n) = Ω(f(n))$
*   **对称性 (Symmetry)**：
    *   $f(n) = Θ(g(n))$ 当且仅当 $g(n) = Θ(f(n))$
*   **转置对称性 (Transpose Symmetry)**：
    *   $f(n) = O(g(n))$ 当且仅当 $g(n) = Ω(f(n))$
    *   $f(n) = o(g(n))$ 当且仅当 $g(n) = ω(f(n))$

#### **3.7. 常见函数增长率比较**

以下是常见函数按增长率从低到高的排序：
$1 < log n < √n < n < n log n < n² < n³ < ... < 2ⁿ < 3ⁿ < ... < n!$

### **第四部分：递归与分治策略**

#### **4.1. 分治策略 (Divide-and-Conquer)**

分治是一种重要的算法设计范式。它将一个难以直接解决的大问题，分割成一些规模较小的相同问题，以便于分而治之。

分治算法的执行过程遵循三个步骤：
1.  **分解 (Divide)**：将原问题分解为若干个规模较小、相互独立、且与原问题形式相同的子问题。
2.  **解决 (Conquer)**：若子问题规模足够小，则直接求解。否则，递归地求解各个子问题。
3.  **合并 (Combine)**：将各个子问题的解合并，从而构成原问题的解。

**归并排序 (Merge Sort)** 是分治策略的一个经典应用：
*   **分解**：将待排序的 $n$ 个元素的序列分解成各含 $n/2$ 个元素的两个子序列。
*   **解决**：递归地使用归并排序对两个子序列进行排序。
*   **合并**：合并两个已排序的子序列，以产生最终的排序结果。

#### **4.2. 递归式分析**

**递归式 (Recurrence)** 是一个等式或不等式，它通过更小的输入的函数值来描述一个函数。在算法分析中，递归式常用于描述分治算法的运行时间。
例如，归并排序的运行时间可以描述为：
$T(n) = 2T(n/2) + \Theta(n)$

我们介绍三种求解递归式的方法：

##### **4.2.1. 代入法 (Substitution Method)**

代入法包含两个步骤：
1.  **猜测**解的形式。
2.  用**数学归纳法**证明猜测的正确性。

*   **示例**：求解 $T(n) = 2T(\lfloor n/2 \rfloor) + n$。
    1.  **猜测**：我们猜测解为 $T(n) = O(n \log n)$。
    2.  **证明**：我们需要证明存在常数 $c > 0$，使得 $T(n) \le cn \log n$。
        *   **归纳假设**：假设对于所有小于 $n$ 的正整数 $m$，该猜测成立，即 $T(m) \le cm \log m$。
        *   **归纳步骤**：
            $T(n) \le 2(c \lfloor n/2 \rfloor \log(\lfloor n/2 \rfloor)) + n$
            $\le 2(c (n/2) \log(n/2)) + n$
            $= cn \log(n/2) + n$
            $= cn(\log n - \log 2) + n$
            $= cn \log n - cn + n$
            $\le cn \log n$
            这个不等式在 $cn - n \ge 0$，即 $c \ge 1$ 时成立。
        *   **边界条件**：我们需要处理边界情况。当 $n=1$ 时，$T(1) = 1$。但我们的证明要求 $T(1) \le c \cdot 1 \cdot \log 1 = 0$，这产生了矛盾。

*   **处理边界条件的技巧**：
    渐进记法只要求我们对 $n \ge n_0$ 的情况证明即可。我们可以选择一个足够大的 $n_0$，使得对于 $n < n_0$ 的情况，我们可以直接验证。例如，对于 $T(2)$ 和 $T(3)$，我们可以直接计算其值，然后选择一个足够大的 $c$ 使得归纳假设在这些点上成立。

*   **加强归纳假设**：
    有时，直接的归纳证明会失败。例如，对于 $T(n) = T(\lfloor n/2 \rfloor) + T(\lceil n/2 \rceil) + 1$，如果我们猜测 $T(n) \le cn$，则会得到 $T(n) \le c(\lfloor n/2 \rfloor) + c(\lceil n/2 \rceil) + 1 = cn + 1$，这无法证明 $T(n) \le cn$。
    解决方法是**减去一个低阶项**来加强归纳假设。我们猜测 $T(n) \le cn - b$，其中 $b \ge 0$ 是一个常数。
    $T(n) \le (c\lfloor n/2 \rfloor - b) + (c\lceil n/2 \rceil - b) + 1 = cn - 2b + 1$
    只要我们选择 $b \ge 1$，那么 $cn - 2b + 1 \le cn - b$。这样归纳证明就可以继续。

##### **4.2.2. 递归树方法 (Recursion-Tree Method)**

递归树是一种将递归式迭代过程可视化的方法，非常适合用于生成一个好的猜测，然后可以用代入法来验证。

*   **构建过程**：
    1.  树的根节点代表原始问题，其代价为 $f(n)$。
    2.  根节点的子节点代表其递归调用产生的子问题。
    3.  持续展开这个过程，直到达到递归的边界条件。

*   **示例**：求解 $T(n) = 2T(n/2) + n$。
    *   **树的结构**：这是一个完全二叉树。
    *   **每层代价**：
        *   第0层（根）：代价为 $n$。
        *   第1层：有两个子问题，每个规模为 $n/2$，总代价为 $2 \times (n/2) = n$。
        *   第 $i$ 层：有 $2^i$ 个子问题，每个规模为 $n/2^i$，总代价为 $2^i \times (n/2^i) = n$。
    *   **树的高度**：当子问题规模降为1时，即 $n/2^k = 1$，解得 $k = \log_2 n$。所以树的高度为 $\log n$。
    *   **总代价**：将所有层的代价相加。共有 $\log n + 1$ 层，每层代价为 $n$（最后一层除外，但可以合并计算）。总代价约为 $n \times \log n$。因此，我们猜测 $T(n) = \Theta(n \log n)$。

##### **4.2.3. 主方法 (Master Method)**

主方法为形如 $T(n) = aT(n/b) + f(n)$ 的递归式提供了一种“菜谱式”的求解方案，其中 $a \ge 1, b > 1$ 是常数，$f(n)$ 是一个渐进正函数。

主方法需要比较函数 $f(n)$ 和 $n^{\log_b a}$ 的大小。共有三种情况：

1.  **情况1**：如果存在常数 $\epsilon > 0$，使得 $f(n) = O(n^{\log_b a - \epsilon})$，则 $T(n) = \Theta(n^{\log_b a})$。
    *   **直观理解**：$f(n)$ 的增长率被 $n^{\log_b a}$ **多项式地**压制。递归树的代价主要由叶子节点决定。
    *   **示例**：$T(n) = 9T(n/3) + n$。
        这里 $a=9, b=3, f(n)=n$。$n^{\log_b a} = n^{\log_3 9} = n^2$。
        $f(n) = n = O(n^{2-1})$，满足情况1（$\epsilon=1$）。
        因此，$T(n) = \Theta(n^2)$。

2.  **情况2**：如果 $f(n) = \Theta(n^{\log_b a})$，则 $T(n) = \Theta(n^{\log_b a} \log n)$。
    *   **直观理解**：$f(n)$ 的增长率与 $n^{\log_b a}$ 相同。递归树的每一层代价大致相等。
    *   **示例**：$T(n) = T(2n/3) + 1$。
        这里 $a=1, b=3/2, f(n)=1$。$n^{\log_b a} = n^{\log_{3/2} 1} = n^0 = 1$。
        $f(n) = 1 = \Theta(n^0)$，满足情况2。
        因此，$T(n) = \Theta(n^0 \log n) = \Theta(\log n)$。

3.  **情况3**：如果存在常数 $\epsilon > 0$，使得 $f(n) = \Omega(n^{\log_b a + \epsilon})$，并且对于某个常数 $c < 1$ 和所有足够大的 $n$，满足**正则条件** $af(n/b) \le cf(n)$，则 $T(n) = \Theta(f(n))$。
    * **直观理解**：$f(n)$ 的增长率**多项式地**超过 $n^{\log_b a}$，并且 $f(n)$ 的增长足够“规则”。递归树的代价主要由根节点的代价决定。
    
    *   **示例**：$T(n) = 3T(n/4) + n \log n$。
        这里 $a=3, b=4, f(n)=n \log n$。$n^{\log_b a} = n^{\log_4 3} \approx n^{0.793}$。
        $f(n) = n \log n = \Omega(n^{0.793 + \epsilon})$（例如取 $\epsilon=0.2$）。
        检查正则条件：$3(n/4)\log(n/4) \le c(n \log n)$。取 $c=3/4$ 时，对于足够大的 $n$，该条件成立。
        因此，$T(n) = \Theta(n \log n)$。
        

好的，我们继续第五部分的内容。本部分将介绍概率分析和随机算法，这两种技术在处理不确定性或避免最坏情况时非常有用。

---

### **第五部分：概率分析与随机算法**

#### **5.1. 雇佣问题 (The Hiring Problem)**

这是一个经典的例子，用来说明概率分析和随机算法的应用。

*   **问题描述**：
    *   你需要雇佣一名新的办公助理。
    *   一家雇佣代理每天会给你推荐一位候选人，共推荐 $n$ 位。
    *   你需要面试每一位候选人，并立即决定是否雇佣他/她。
    *   如果决定雇佣，你必须解雇当前的助理，然后雇佣新的。
    *   面试需要支付给代理一笔固定的费用（面试费），雇佣新助理也需要一笔固定的费用（雇佣费）。雇佣费远高于面试费。
    *   **目标**：雇佣到最优秀的候选人，同时希望总费用（特别是雇佣费用）尽可能低。

*   **算法**：
    1.  面试第一位候选人，并直接雇佣他/她。
    2.  对于后续的每一位候选人，如果他/她比当前雇佣的助理更优秀，则解雇当前助理，雇佣这位新的候选人。

*   **成本分析**：
    *   设面试成本为 $c_i$，雇佣成本为 $c_h$。
    *   总面试成本为 $n \cdot c_i$。
    *   总雇佣成本为 $m \cdot c_h$，其中 $m$ 是雇佣的次数。
    *   总成本 $T(n) = n c_i + m c_h$。

*   **最坏情况分析**：
    如果候选人按能力递增的顺序出现，那么每次面试后都会进行一次雇佣。此时 $m=n$，总成本达到最大。

#### **5.2. 概率分析 (Probabilistic Analysis)**

概率分析是在**假设输入服从某种概率分布**的前提下，分析算法的期望性能。

*   **假设**：我们假设 $n$ 位候选人以一个**随机的顺序**出现。这意味着每一种排列都是等可能的（共有 $n!$ 种排列，每种出现的概率为 $1/n!$）。

*   **指示器随机变量 (Indicator Random Variables)**：
    这是一个强大的分析工具。对于一个事件 $A$，其指示器随机变量 $I\{A\}$ 定义为：
    $I\{A\} = \begin{cases} 1 & \text{如果 } A \text{ 发生} \\ 0 & \text{如果 } A \text{ 不发生} \end{cases}$

    **引理**：一个事件 $A$ 的指示器随机变量的期望值等于该事件发生的概率：$E[I\{A\}] = \text{Pr}\{A\}$。

*   **分析雇佣问题的期望雇佣次数**：
    1.  令 $X$ 为我们雇佣新助理的总次数的随机变量。
    2.  令 $X_i$ 为一个指示器随机变量，表示第 $i$ 位候选人被雇佣的事件。即 $X_i = I\{\text{candidate } i \text{ is hired}\}$。
    3.  那么总雇佣次数 $X = \sum_{i=1}^{n} X_i$。
    4.  根据期望的线性性，$E[X] = E[\sum_{i=1}^{n} X_i] = \sum_{i=1}^{n} E[X_i]$。
    5.  根据引理，$E[X_i] = \text{Pr}\{\text{candidate } i \text{ is hired}\}$。
    6.  第 $i$ 位候选人被雇佣的条件是：他/她比前 $i-1$ 位所有候选人都优秀。由于候选人是随机排列的，前 $i$ 位候选人中的任何一位都等可能地是这 $i$ 位中最优秀的。因此，第 $i$ 位候选人是前 $i$ 位中最优秀的概率是 $1/i$。
    7.  所以，$\text{Pr}\{\text{candidate } i \text{ is hired}\} = 1/i$。
    8.  因此，$E[X] = \sum_{i=1}^{n} (1/i)$。
    9.  这个和是**调和级数 (Harmonic Series)**，$H_n = \sum_{i=1}^{n} (1/i) \approx \ln n + O(1)$。

*   **结论**：在输入随机的假设下，雇佣新助理的平均次数约为 $\ln n$。这远小于最坏情况下的 $n$ 次。

#### **5.3. 随机算法 (Randomized Algorithms)**

与概率分析不同，随机算法不依赖于输入的分布，而是**在算法内部使用随机性**来影响其执行流程。这使得算法对于任何输入，其期望性能都是固定的。

*   **核心思想**：与其假设输入是随机的，不如通过算法来**强制**产生一个随机的排列。

*   **应用于雇佣问题**：
    1.  在开始面试之前，将 $n$ 位候选人的列表进行一次随机置换。
    2.  然后按照这个新的随机顺序依次面试。

*   **分析**：
    *   由于我们人为地创造了一个随机排列，其分析过程与上述概率分析完全相同。
    *   期望的雇佣次数仍然是 $H_n \approx \ln n$。
    *   **关键区别**：这个期望值现在不依赖于输入候选人的初始顺序，而是由算法内部的随机数生成器保证。对于**任何**输入，算法的期望性能都是好的。

*   **随机排列数组**：
    如何生成一个均匀随机排列？
    1.  **Permute-by-Sorting**：
        *   为数组 $A$ 的每个元素 $A[i]$ 赋予一个随机的优先级 $P[i]$。
        *   根据这些优先级对数组 $A$ 进行排序。
        *   如果所有优先级都唯一，则可以产生均匀随机排列。为了保证唯一性，优先级通常在一个较大的范围内选取，例如 $[1, n^3]$。
        *   时间复杂度为 $O(n \log n)$（主要由排序决定）。

    2.  **Randomize-in-Place** (原地随机置换)：
        这是一个更高效的 $O(n)$ 算法。
        ```
        RANDOMIZE-IN-PLACE(A)
        n = A.length
        for i = 1 to n
            swap A[i] with A[RANDOM(i, n)]
        ```
        *   **循环不变式**：在 `for` 循环的第 $i$ 次迭代开始之前，对于每个可能的 $(i-1)$-排列，子数组 $A[1..i-1]$ 包含该 $(i-1)$-排列的概率为 $(n-i+1)!/n!$。
        *   通过这个循环不变式可以证明，该算法最终产生的是一个均匀随机排列。


---

### **第二部分：排序与顺序统计**

#### **6. 排序算法**

##### **6.1. 堆排序 (Heapsort)**

堆排序是一种高效的、基于比较的排序算法。它利用了一种名为“堆”的数据结构。

*   **堆 (Heap)**：
    *   堆可以看作是一个近似**完全二叉树 (Complete Binary Tree)**，它可以通过一个数组来表示。
    *   **数组表示**：对于数组中索引为 $i$ 的节点：
        *   `PARENT(i)` 返回 $\lfloor i/2 \rfloor$
        *   `LEFT(i)` 返回 $2i$
        *   `RIGHT(i)` 返回 $2i+1$
    *   **堆的性质**：
        *   **最大堆 (Max-Heap)**：除了根节点外，每个节点 $i$ 的值都小于或等于其父节点的值，即 $A[\text{PARENT}(i)] \ge A[i]$。这意味着堆中的最大元素存储在根节点 $A$。
        *   **最小堆 (Min-Heap)**：每个节点的值都大于或等于其父节点的值。

*   **堆排序中的核心操作**：
    1.  **`MAX-HEAPIFY(A, i)`**：
        *   **功能**：维护最大堆的性质。
        *   **前提**：假设以 `LEFT(i)` 和 `RIGHT(i)` 为根的两个子树都已经是最大堆。
        *   **过程**：让 $A[i]$ 的值在最大堆中“逐级下降”，直到以 $i$ 为根的子树也满足最大堆性质。它通过比较 $A[i]$ 与其左右孩子的值，找出最大者，如果最大者不是 $A[i]$，则交换它们，并对被交换下去的子树递归调用 `MAX-HEAPIFY`。
        *   **运行时间**：$O(\log n)$，因为操作路径是从根到叶子的一条路径，其长度与树高 $h = \Theta(\log n)$ 成正比。

    2.  **`BUILD-MAX-HEAP(A)`**：
        *   **功能**：将一个无序的输入数组构建成一个最大堆。
        *   **过程**：对数组中所有非叶子节点（从 $\lfloor n/2 \rfloor$ 到 1）倒序调用 `MAX-HEAPIFY`。
        *   **运行时间**：虽然直观上是 $O(n \log n)$（调用了约 $n/2$ 次 `MAX-HEAPIFY`），但更紧确的界是 $O(n)$。这是因为大部分节点的树高都很小。

*   **堆排序算法 (`HEAPSORT(A)`)**：
    1.  调用 `BUILD-MAX-HEAP(A)` 将输入数组构建成一个最大堆。
    2.  进行 $n-1$ 次循环：
        *   将堆的根元素 $A$（当前最大元素）与堆的最后一个元素 $A[i]$ 交换。
        *   将堆的大小减 1（逻辑上将已排序的最大元素从堆中移除）。
        *   对新的根节点 $A$ 调用 `MAX-HEAPIFY(A, 1)` 来维护最大堆性质。

*   **性能分析**：
    *   `BUILD-MAX-HEAP` 的时间为 $O(n)$。
    *   `for` 循环执行 $n-1$ 次，每次 `MAX-HEAPIFY` 的时间为 $O(\log n)$。
    *   总运行时间为 $O(n) + (n-1)O(\log n) = O(n \log n)$。

##### **6.2. 快速排序 (Quicksort)**

快速排序是另一种基于分治思想的排序算法，在实践中通常是最高效的排序算法之一。

*   **核心思想**：
    *   **分解**：通过一个**划分 (Partition)** 过程，选取一个主元 (pivot)，将数组划分为两个（可能为空的）子数组 $A[p..q-1]$ 和 $A[q+1..r]$，使得前者中的每个元素都小于等于 $A[q]$，后者中的每个元素都大于等于 $A[q]$。下标 $q$ 在划分过程中确定。
    *   **解决**：通过递归调用快速排序，对子数组 $A[p..q-1]$ 和 $A[q+1..r]$ 进行排序。
    *   **合并**：因为子数组是就地排序的，所以不需要合并操作。整个数组在递归调用结束后就已经排好序。

*   **划分 (`PARTITION(A, p, r)`)**：
    *   这是快速排序的关键。一个经典的实现（如Lomuto划分）是选取数组最后一个元素 $A[r]$ 作为主元。
    *   维护一个索引 $i$，使得 $A[p..i]$ 中的元素都小于等于主元。
    *   遍历 $A[p..r-1]$，如果发现一个元素小于等于主元，则将其与 $A[i+1]$ 交换，并递增 $i$。
    *   最后将主元与 $A[i+1]$ 交换。
    *   **运行时间**：$\Theta(n)$，其中 $n=r-p+1$。

*   **性能分析**：
    *   **最坏情况**：当划分过程每次都产生一个 $n-1$ 个元素和一个 0 个元素的子问题时（例如，当数组已经排好序或逆序，且总是选择第一个或最后一个元素为主元）。
        递归式为 $T(n) = T(n-1) + \Theta(n)$，解为 $T(n) = \Theta(n^2)$。
    *   **最佳情况**：当划分过程每次都产生两个大小为 $n/2$ 的子问题。
        递归式为 $T(n) = 2T(n/2) + \Theta(n)$，解为 $T(n) = \Theta(n \log n)$。
    *   **平均情况**：即使划分不是完美的，例如每次都按 9:1 的比例划分，递归式为 $T(n) = T(9n/10) + T(n/10) + \Theta(n)$，其解仍然是 $T(n) = \Theta(n \log n)$。可以证明，在所有输入排列都是等概率的情况下，快速排序的期望运行时间是 $\Theta(n \log n)$。

*   **随机化快速排序 (Randomized Quicksort)**：
    为了避免最坏情况的频繁发生（例如对于特定模式的输入），可以引入随机性。
    *   **方法**：在划分之前，从子数组中随机选择一个元素作为主元，并将其与最后一个元素交换。
    *   **效果**：这使得算法的运行时间不依赖于输入的初始排列，而是依赖于随机数生成器的输出。对于任何输入，其期望运行时间都是 $\Theta(n \log n)$。

##### **6.3. 比较排序的下界**

*   **比较排序 (Comparison Sort)**：这类算法仅通过比较元素之间的相对大小来确定排序顺序（如插入排序、归并排序、堆排序、快速排序）。

*   **决策树模型 (Decision-Tree Model)**：
    *   任何一个比较排序算法都可以被抽象为一个**决策树**。
    *   树的每个内部节点代表一次比较（如 $a_i \le a_j$）。
    *   树的每个叶子节点代表一种可能的排序结果（一个输入元素的排列）。
    *   对于一个特定的输入，算法的执行对应于从根到某个叶子的一条路径。
    *   算法在最坏情况下的比较次数等于决策树的高度。

*   **下界证明**：
    1.  对于 $n$ 个不同的元素，共有 $n!$ 种可能的排列。
    2.  因此，决策树必须至少有 $n!$ 个叶子节点来覆盖所有可能的输出。
    3.  一个高度为 $h$ 的二叉树最多有 $2^h$ 个叶子节点。
    4.  所以，$n! \le 2^h$。
    5.  两边取对数，得到 $h \ge \log(n!)$。
    6.  根据斯特林近似公式，$n! \approx \sqrt{2\pi n} (n/e)^n$，可以得出 $\log(n!) = \Omega(n \log n)$。
    7.  因此，$h = \Omega(n \log n)$。

*   **结论**：任何基于比较的排序算法，在最坏情况下都需要至少 $\Omega(n \log n)$ 次比较。堆排序和归并排序都是渐近最优的比较排序算法。

好的，我们继续第七部分的内容。本部分将讨论在一组数据中查找特定顺序统计量的问题，特别是中位数。

### **第二部分：排序与顺序统计 (续)**

#### **7. 中位数与顺序统计 (Medians and Order Statistics)**

##### **7.1. 问题定义**

*   **第 $i$ 顺序统计量 (i-th Order Statistic)**：在一个包含 $n$ 个元素的集合中，第 $i$ 小的元素。
    *   **最小值 (Minimum)** 是第 1 顺序统计量 ($i=1$)。
    *   **最大值 (Maximum)** 是第 $n$ 顺序统计量 ($i=n$)。
    *   **中位数 (Median)** 是集合的“中间”元素。
        *   当 $n$ 为奇数时，中位数是唯一的，位于 $i=(n+1)/2$ 的位置。
        *   当 $n$ 为偶数时，存在两个中位数，分别位于 $i=n/2$（下中位数）和 $i=n/2+1$（上中位数）的位置。

*   **选择问题 (Selection Problem)**：给定一个包含 $n$ 个（不一定不同）数的集合和一个整数 $i$（$1 \le i \le n$），找出该集合中第 $i$ 小的元素。

##### **7.2. 查找最小值和最大值**

*   **查找最小值**：
    可以通过一次遍历完成，需要 $n-1$ 次比较。
    ```
    MINIMUM(A)
    min = A[1]
    for i = 2 to A.length
        if min > A[i]
            min = A[i]
    return min
    ```
*   **同时查找最小值和最大值**：
    *   **朴素方法**：分别调用 `MINIMUM` 和 `MAXIMUM`，总共需要 $2(n-1)$ 次比较。
    *   **更优方法**：成对处理元素。
        1.  将输入元素两两配对。
        2.  对每一对进行一次比较，得到较大者和较小者。
        3.  在所有较大者中寻找最大值，在所有较小者中寻找最小值。
        *   **比较次数**：
            *   配对比较：$\lfloor n/2 \rfloor$ 次。
            *   找最大值：$\lceil n/2 \rceil - 1$ 次。
            *   找最小值：$\lceil n/2 \rceil - 1$ 次。
            *   总次数约为 $3n/2$ 次，优于 $2n-2$ 次。

##### **7.3. 期望为线性时间的选择算法**

我们可以通过排序然后在 $O(1)$ 时间内索引第 $i$ 个元素来解决选择问题，总时间为 $O(n \log n)$。但我们可以做得更好。

`RANDOMIZED-SELECT` 算法利用了与快速排序相同的划分思想，但只处理划分后的一部分。

*   **算法 `RANDOMIZED-SELECT(A, p, r, i)`**：
    *   **功能**：在子数组 `A[p..r]` 中查找第 $i$ 小的元素。
    1.  如果 $p=r$，则返回 $A[p]$。
    2.  使用 `RANDOMIZED-PARTITION` 将数组 `A[p..r]` 划分为两个子数组 `A[p..q-1]` 和 `A[q+1..r]`，其中 $A[q]$ 是主元。
    3.  计算主元 $A[q]$ 在子数组 `A[p..r]` 中的位次 $k = q - p + 1$。
    4.  **比较 $i$ 和 $k$**：
        *   如果 $i = k$，则主元 $A[q]$ 就是要找的第 $i$ 小的元素，返回 $A[q]$。
        *   如果 $i < k$，则第 $i$ 小的元素位于左侧子数组 `A[p..q-1]` 中。递归调用 `RANDOMIZED-SELECT(A, p, q-1, i)`。
        *   如果 $i > k$，则第 $i$ 小的元素位于右侧子数组 `A[q+1..r]` 中。我们需要在该子数组中寻找第 $(i-k)$ 小的元素。递归调用 `RANDOMIZED-SELECT(A, q+1, r, i-k)`。

*   **性能分析**：
    *   **最坏情况**：与快速排序类似，如果每次划分都极不均衡（例如，划分出的子问题规模为 $n-1$），则算法需要递归 $n-1$ 次。
        递归式为 $T(n) = T(n-1) + \Theta(n)$，解为 $T(n) = \Theta(n^2)$。
    *   **期望运行时间**：由于每次只对划分后的一边进行递归，我们可以期望算法的运行时间是线性的。
        *   **直观分析**：最好的情况是每次都划分为一半，递归式为 $T(n) = T(n/2) + \Theta(n)$，解为 $T(n) = \Theta(n)$。即使最好和最坏情况交替出现，平均性能也很好。
        *   **严格分析**：使用指示器随机变量可以证明，`RANDOMIZED-SELECT` 的期望运行时间为 $O(n)$。分析的关键在于，一个元素被选为主元的概率是均匀的，因此期望情况下，每次划分都能有效地减小问题规模。

您说得对，我为之前提供的内容过于概括而道歉。为了达到您“严谨、详尽、事无巨细”的要求，我将重新组织并极大地扩充笔记的内容，确保涵盖PDF中的每一个知识点、推导过程和算法细节。

我将严格遵循您的指示，所有公式都使用 `$` 符号格式化。

现在，我们重新开始**第三部分：高级设计与分析技术**，并从**动态规划**的第一个例子——钢条切割问题开始，进行详尽的阐述。

---

### **第三部分：高级设计与分析技术**

#### **8. 动态规划 (Dynamic Programming)**

动态规划是一种强大的算法设计技术，它将一个复杂问题分解为更简单的、可管理的子问题。它通常应用于具有以下两个核心特性的问题：

1.  **最优子结构 (Optimal Substructure)**：问题的最优解包含其子问题的最优解。这意味着我们可以从子问题的最优解中构建出原问题的最优解。
2.  **重叠子问题 (Overlapping Subproblems)**：一个递归算法会反复地求解相同的子问题，而不是每次都生成新的子问题。动态规划通过计算一次每个子问题的解并将其存储起来（通常在一个表格中），在下次需要时直接查找，从而避免了重复计算。

我们将通过几个经典问题来深入理解动态规划的设计步骤。

##### **8.2. 钢条切割 (Rod Cutting)**

**8.2.1. 问题定义**

*   **输入**：
    1.  一段长度为 $n$ 的钢条。
    2.  一个价格表 $p$，其中 $p_i$ ($i=1, 2, ..., n$) 是长度为 $i$ 的钢条的价格。
*   **输出**：
    通过切割钢条并出售这些小段，可以获得的最大总收益 $r_n$。切割方案可以包含0次或多次切割。

**8.2.2. 刻画最优子结构**

要解决这个问题，我们首先要刻画一个最优解的结构。一个长度为 $n$ 的钢条的切割方案可以描述为一个整数序列 $<i_1, i_2, ..., i_k>$，其中 $n = i_1 + i_2 + ... + i_k$，表示将钢条切割成 $k$ 段，长度分别为 $i_1, ..., i_k$。其收益为 $p_{i_1} + ... + p_{i_k}$。

考虑一个长度为 $n$ 的钢条的最优切割方案。这个方案中必然包含一个**第一次切割**（或者不切割）。
*   如果**不切割**，收益就是 $p_n$。
*   如果**进行切割**，我们将钢条切成两段：第一段长度为 $i$（$1 \le i < n$），剩下的一段长度为 $n-i$。

这里的关键洞察是：在我们做出第一次切割后，我们得到了两段钢条。我们将长度为 $i$ 的第一段直接出售，不再切割，而对剩下的长度为 $n-i$ 的钢条，我们必须继续对其进行切割（或不切割）以获得最大收益。**如果原问题的切割方案是最优的，那么后续对长度为 $n-i$ 的钢条的切割方案，也必须是针对该长度钢条的最优切割方案。**

这就是**最优子结构**性质的体现。我们可以用“剪切-粘贴” (cut-and-paste) 的方法来证明：假设对长度为 $n-i$ 的钢条存在一个比当前方案更优的切割方案，那么我们可以将原方案中对 $n-i$ 段的切割替换为这个更优的方案，从而得到一个比原最优方案总收益更高的、针对长度 $n$ 钢条的切割方案。这与我们最初的“最优”假设相矛盾。

**8.2.3. 递归解**

基于最优子结构，我们可以给出一个递归的公式来定义最优解的值。令 $r_n$ 表示长度为 $n$ 的钢条的最优切割收益。

$r_n = \max(p_n, r_1+r_{n-1}, r_2+r_{n-2}, ..., r_{n-1}+r_1)$

这个公式表示，最优收益是以下几种情况中的最大值：
1.  不切割，直接出售，收益为 $p_n$。
2.  切割成 $1$ 和 $n-1$，收益为 $r_1+r_{n-1}$。
3.  切割成 $2$ 和 $n-2$，收益为 $r_2+r_{n-2}$。
    ...

我们可以简化这个公式。考虑第一次切割，切下一段长度为 $i$ 的钢条，剩下的 $n-i$ 的部分继续进行最优切割。那么总收益为 $p_i + r_{n-i}$。我们可以对所有可能的 $i$ 进行尝试。因此，公式可以写为：

$r_n = \max_{1 \le i \le n} (p_i + r_{n-i})$

其中，当 $i=n$ 时，$r_0=0$，对应不切割的情况。

**8.2.4. 重叠子问题**

如果我们直接将上述递归式转化为一个递归程序，会发现其效率极低。这是因为它会反复求解相同的子问题。例如，计算 $r_4$ 的过程：
*   $r_4$ 依赖于 $r_3, r_2, r_1, r_0$。
*   $r_3$ 依赖于 $r_2, r_1, r_0$。
*   $r_2$ 依赖于 $r_1, r_0$。

可以看到，子问题 $r_2$ 和 $r_1$ 被多次重复计算。随着 $n$ 的增大，这种重复计算会呈指数级增长，导致算法的运行时间为 $\Omega(2^n)$。

**8.2.5. 动态规划求解**

为了避免重复计算，我们使用动态规划的方法。

*   **方法一：带备忘的自顶向下法 (Top-Down with Memoization)**
    我们编写一个递归程序，但在其内部，我们用一个数组（备忘录）`r` 来保存已解决的子问题的解。
    1.  初始化一个辅助数组 `r`，所有元素设为一个特殊值（如 `-∞`）。
    2.  在递归函数中，首先检查 `r[n]` 是否已经被计算过。
    3.  如果计算过，直接返回 `r[n]`。
    4.  如果没有，则按递归公式进行计算，并将结果存入 `r[n]` 后再返回。

*   **方法二：自底向上法 (Bottom-Up Method)**
    这种方法直接按子问题规模从小到大的顺序进行求解。对于钢条切割，我们依次计算 $r_1, r_2, ..., r_n$。
    ```
    BOTTOM-UP-CUT-ROD(p, n)
    1. let r[0..n] be a new array
    2. r[0] = 0
    3. for j = 1 to n
    4.     q = -∞
    5.     for i = 1 to j
    6.         q = max(q, p[i] + r[j-i])
    7.     r[j] = q
    8. return r[n]
    ```
    *   外层循环 `j` 代表当前要求解的子问题规模（钢条长度）。
    *   内层循环 `i` 代表第一次切割的长度。
    *   当计算 `r[j]` 时，所有它依赖的子问题 $r, ..., r[j-1]$ 的解都已经计算完毕并存储在数组 `r` 中。

**8.2.6. 重构解**

上述算法只给出了最大收益值，但没有给出具体的切割方案。为了得到方案，我们需要扩展算法，记录导致最优值的选择。

*   我们引入一个辅助数组 `s[1..n]`。
*   在自底向上的算法中，当 `q = max(q, p[i] + r[j-i])` 更新 `q` 时，我们将对应的 `i` 值存入 `s[j]`。`s[j]` 记录了切割长度为 $j$ 的钢条时，第一段的最优尺寸。
    ```
    EXTENDED-BOTTOM-UP-CUT-ROD(p, n)
    ... (与上面类似) ...
    3. for j = 1 to n
    4.     q = -∞
    5.     for i = 1 to j
    6.         if q < p[i] + r[j-i]
    7.             q = p[i] + r[j-i]
    8.             s[j] = i
    9.     r[j] = q
    10. return r and s
    ```
*   有了 `s` 数组，我们可以通过一个简单的循环来输出最优切割方案：
    ```
    PRINT-CUT-ROD-SOLUTION(p, n)
    (r, s) = EXTENDED-BOTTOM-UP-CUT-ROD(p, n)
    while n > 0
        print s[n]
        n = n - s[n]
    ```

**8.2.7. 复杂度分析**

*   **自底向上法**：
    *   **时间复杂度**：算法的核心是两个嵌套的 `for` 循环。外层循环执行 $n$ 次，内层循环最多执行 $n$ 次。因此，总的运行时间为 $\sum_{j=1}^{n} j = n(n+1)/2$，即 $T(n) = \Theta(n^2)$。
    *   **空间复杂度**：需要一个长度为 $n+1$ 的数组 `r` 和一个长度为 $n$ 的数组 `s`（如果需要重构解），因此空间复杂度为 $\Theta(n)$。
    好的，我们继续进行动态规划部分的学习，接下来是第二个经典问题——矩阵链乘法。

##### **8.3. 矩阵链乘法 (Matrix-Chain Multiplication, MCM)**

**8.3.1. 问题定义**

*   **输入**：
    1.  一个 $n$ 个矩阵的序列（矩阵链）$<A_1, A_2, ..., A_n>$。
    2.  每个矩阵 $A_i$ 的维度。我们用一个数组 $p=<p_0, p_1, ..., p_n>$ 来表示，其中矩阵 $A_i$ 的维度为 $p_{i-1} \times p_i$。

*   **背景**：
    *   两个矩阵 $A$ (维度 $p \times q$) 和 $B$ (维度 $q \times r$) 相乘，得到的矩阵 $C$ 维度为 $p \times r$。完成这个乘法所需的**标量乘法**次数为 $p \cdot q \cdot r$。
    *   矩阵乘法满足**结合律**，即 $(A_1 A_2) A_3 = A_1 (A_2 A_3)$。这意味着，对于一个矩阵链，不同的加括号方式（即不同的计算顺序）会得到相同的结果矩阵。
    *   然而，不同的计算顺序会导致总的标量乘法次数截然不同，从而极大地影响计算效率。

*   **目标**：
    找出一个为矩阵链 $<A_1, A_2, ..., A_n>$ **完全加括号**的方案，使得计算乘积所需的总标量乘法次数最少。我们只关心如何确定最佳的计算顺序，而不是真正去执行矩阵乘法。

*   **示例**：
    考虑矩阵链 $<A_1, A_2, A_3>$，维度分别为 $10 \times 100$, $100 \times 5$, $5 \times 50$。
    *   方案1: $((A_1 A_2) A_3)$
        *   计算 $A_1 A_2$ 需要 $10 \cdot 100 \cdot 5 = 5000$ 次乘法，得到一个 $10 \times 5$ 的矩阵。
        *   计算结果矩阵与 $A_3$ 相乘需要 $10 \cdot 5 \cdot 50 = 2500$ 次乘法。
        *   总次数 = $5000 + 2500 = 7500$。
    *   方案2: $(A_1 (A_2 A_3))$
        *   计算 $A_2 A_3$ 需要 $100 \cdot 5 \cdot 50 = 25000$ 次乘法，得到一个 $100 \times 50$ 的矩阵。
        *   计算 $A_1$ 与结果矩阵相乘需要 $10 \cdot 100 \cdot 50 = 50000$ 次乘法。
        *   总次数 = $25000 + 50000 = 75000$。
        显然，方案1的效率远高于方案2。

**8.3.2. 刻画最优子结构**

这是动态规划方法的第一步。我们需要找到问题与其子问题之间的最优子结构关系。

*   **子问题定义**：
    一个自然的想法是，原问题是计算矩阵链 $A_{1..n}$ 的最优括号化方案。其子问题应该是计算子链 $A_{i..j}$ ($1 \le i \le j \le n$) 的最优括号化方案。

*   **最优子结构**：
    考虑计算子链 $A_{i..j}$ 的最优括号化方案。这个方案必然是在某个位置 $k$（$i \le k < j$）将子链分割为两部分，即 $(A_{i..k})$ 和 $(A_{k+1..j})$，然后将这两部分的结果相乘。这是最后一次矩阵乘法。
    *   **关键洞察**：如果整个方案是最优的，那么用于计算子链 $(A_{i..k})$ 的括号化方案，以及用于计算子链 $(A_{k+1..j})$ 的括号化方案，也必须是它们各自的最优方案。
    *   **证明**（同样使用“剪切-粘贴”法）：假设计算 $(A_{i..k})$ 存在一个更优的（代价更低的）括号化方案，那么我们可以用这个更优的方案替换原方案中对 $(A_{i..k})$ 的部分，从而得到一个计算 $A_{i..j}$ 的总代价更低的方案。这与原方案是最优的假设相矛盾。

**8.3.3. 递归解**

基于最优子结构，我们可以建立一个递归关系来计算最优代价。

*   令 $m[i, j]$ 为计算矩阵子链 $A_{i..j}$ 所需的最小标量乘法次数。
*   我们的最终目标是求解 $m[1, n]$。

*   **递归公式**：
    *   **基本情况**：当 $i=j$ 时，链中只有一个矩阵，不需要任何乘法。所以，$m[i, i] = 0$。
    *   **递归步骤**：当 $i < j$ 时，我们需要选择一个分割点 $k$（$i \le k < j$），将问题分解为计算 $A_{i..k}$ 和 $A_{k+1..j}$。
        *   计算 $A_{i..k}$ 的代价是 $m[i, k]$。
        *   计算 $A_{k+1..j}$ 的代价是 $m[k+1, j]$。
        *   将这两个结果矩阵（维度分别为 $p_{i-1} \times p_k$ 和 $p_k \times p_j$）相乘的代价是 $p_{i-1} p_k p_j$。
        *   因此，对于一个给定的 $k$，总代价为 $m[i, k] + m[k+1, j] + p_{i-1} p_k p_j$。
        *   我们需要在所有可能的分割点 $k$ 中选择使总代价最小的一个。

    综合起来，递归公式为：
    $m[i, j] = \begin{cases} 0 & \text{if } i = j \\ \min_{i \le k < j} \{m[i, k] + m[k+1, j] + p_{i-1}p_k p_j\} & \text{if } i < j \end{cases}$

**8.3.4. 计算最优代价**

与钢条切割问题类似，直接递归求解会因为重叠子问题而导致指数级的时间复杂度。我们采用自底向上的表格法。

*   **算法 `MATRIX-CHAIN-ORDER(p)`**：
    *   我们需要两个二维表：
        *   `m[1..n, 1..n]` 存储最优代价 $m[i, j]$。
        *   `s[1..n-1, 2..n]` 存储最优分割点 $k$。
    *   算法按矩阵链的长度 $l$ 从 2 到 $n$ 进行迭代。
    *   在每次关于 $l$ 的迭代中，它计算所有长度为 $l$ 的子链 $A_{i..j}$（其中 $j = i+l-1$）的最优代价。

    ```
    MATRIX-CHAIN-ORDER(p)
    1. n = p.length - 1
    2. let m[1..n, 1..n] and s[1..n-1, 2..n] be new tables
    3. for i = 1 to n
    4.     m[i, i] = 0
    5. for l = 2 to n  // l is the chain length
    6.     for i = 1 to n - l + 1
    7.         j = i + l - 1
    8.         m[i, j] = ∞
    9.         for k = i to j - 1
    10.            q = m[i, k] + m[k+1, j] + p[i-1]*p[k]*p[j]
    11.            if q < m[i, j]
    12.                m[i, j] = q
    13.                s[i, j] = k
    14. return m and s
    ```

*   **复杂度分析**：
    *   **时间复杂度**：算法有三层嵌套循环。外层循环 `l` 从 2 到 $n$；第二层循环 `i` 大约 $n$ 次；内层循环 `k` 最多 $n-1$ 次。因此，总的运行时间是 $O(n^3)$。
    *   **空间复杂度**：需要两个 $n \times n$ 的表格，因此空间复杂度为 $\Theta(n^2)$。

**8.3.5. 构造最优解**

算法 `MATRIX-CHAIN-ORDER` 返回了 `m` 和 `s` 两个表。`s` 表记录了最优分割点，我们可以用它来递归地构造出最优的括号化方案。

*   **算法 `PRINT-OPTIMAL-PARENS(s, i, j)`**：
    ```
    PRINT-OPTIMAL-PARENS(s, i, j)
    1. if i == j
    2.     print "A"i
    3. else
    4.     print "("
    5.     PRINT-OPTIMAL-PARENS(s, i, s[i, j])
    6.     PRINT-OPTIMAL-PARENS(s, s[i, j] + 1, j)
    7.     print ")"
    ```
    初始调用为 `PRINT-OPTIMAL-PARENS(s, 1, n)`。

**8.3. 矩阵链乘法 (Matrix-Chain Multiplication, MCM)**

**8.3.1. 问题定义**

*   **输入**：
    1.  一个 $n$ 个矩阵的序列（矩阵链）$<A_1, A_2, ..., A_n>$。
    2.  每个矩阵 $A_i$ 的维度。我们用一个数组 $p=<p_0, p_1, ..., p_n>$ 来表示，其中矩阵 $A_i$ 的维度为 $p_{i-1} \times p_i$。

*   **背景**：
    *   两个矩阵 $A$ (维度 $p \times q$) 和 $B$ (维度 $q \times r$) 相乘，得到的矩阵 $C$ 维度为 $p \times r$。完成这个乘法所需的**标量乘法**次数为 $p \cdot q \cdot r$。
    *   矩阵乘法满足**结合律**，即 $(A_1 A_2) A_3 = A_1 (A_2 A_3)$。这意味着，对于一个矩阵链，不同的加括号方式（即不同的计算顺序）会得到相同的结果矩阵。
    *   然而，不同的计算顺序会导致总的标量乘法次数截然不同，从而极大地影响计算效率。

*   **目标**：
    找出一个为矩阵链 $<A_1, A_2, ..., A_n>$ **完全加括号**的方案，使得计算乘积所需的总标量乘法次数最少。我们只关心如何确定最佳的计算顺序，而不是真正去执行矩阵乘法。

*   **示例**：
    考虑矩阵链 $<A_1, A_2, A_3>$，维度分别为 $10 \times 100$, $100 \times 5$, $5 \times 50$。
    *   方案1: $((A_1 A_2) A_3)$
        *   计算 $A_1 A_2$ 需要 $10 \cdot 100 \cdot 5 = 5000$ 次乘法，得到一个 $10 \times 5$ 的矩阵。
        *   计算结果矩阵与 $A_3$ 相乘需要 $10 \cdot 5 \cdot 50 = 2500$ 次乘法。
        *   总次数 = $5000 + 2500 = 7500$。
    *   方案2: $(A_1 (A_2 A_3))$
        *   计算 $A_2 A_3$ 需要 $100 \cdot 5 \cdot 50 = 25000$ 次乘法，得到一个 $100 \times 50$ 的矩阵。
        *   计算 $A_1$ 与结果矩阵相乘需要 $10 \cdot 100 \cdot 50 = 50000$ 次乘法。
        *   总次数 = $25000 + 50000 = 75000$。
        显然，方案1的效率远高于方案2。

**8.3.2. 刻画最优子结构**

这是动态规划方法的第一步。我们需要找到问题与其子问题之间的最优子结构关系。

*   **子问题定义**：
    一个自然的想法是，原问题是计算矩阵链 $A_{1..n}$ 的最优括号化方案。其子问题应该是计算子链 $A_{i..j}$ ($1 \le i \le j \le n$) 的最优括号化方案。

*   **最优子结构**：
    考虑计算子链 $A_{i..j}$ 的最优括号化方案。这个方案必然是在某个位置 $k$（$i \le k < j$）将子链分割为两部分，即 $(A_{i..k})$ 和 $(A_{k+1..j})$，然后将这两部分的结果相乘。这是最后一次矩阵乘法。
    *   **关键洞察**：如果整个方案是最优的，那么用于计算子链 $(A_{i..k})$ 的括号化方案，以及用于计算子链 $(A_{k+1..j})$ 的括号化方案，也必须是它们各自的最优方案。
    *   **证明**（同样使用“剪切-粘贴”法）：假设计算 $(A_{i..k})$ 存在一个更优的（代价更低的）括号化方案，那么我们可以用这个更优的方案替换原方案中对 $(A_{i..k})$ 的部分，从而得到一个计算 $A_{i..j}$ 的总代价更低的方案。这与原方案是最优的假设相矛盾。

**8.3.3. 递归解**

基于最优子结构，我们可以建立一个递归关系来计算最优代价。

*   令 $m[i, j]$ 为计算矩阵子链 $A_{i..j}$ 所需的最小标量乘法次数。
*   我们的最终目标是求解 $m[1, n]$。

*   **递归公式**：
    *   **基本情况**：当 $i=j$ 时，链中只有一个矩阵，不需要任何乘法。所以，$m[i, i] = 0$。
    *   **递归步骤**：当 $i < j$ 时，我们需要选择一个分割点 $k$（$i \le k < j$），将问题分解为计算 $A_{i..k}$ 和 $A_{k+1..j}$。
        *   计算 $A_{i..k}$ 的代价是 $m[i, k]$。
        *   计算 $A_{k+1..j}$ 的代价是 $m[k+1, j]$。
        *   将这两个结果矩阵（维度分别为 $p_{i-1} \times p_k$ 和 $p_k \times p_j$）相乘的代价是 $p_{i-1} p_k p_j$。
        *   因此，对于一个给定的 $k$，总代价为 $m[i, k] + m[k+1, j] + p_{i-1} p_k p_j$。
        *   我们需要在所有可能的分割点 $k$ 中选择使总代价最小的一个。

    综合起来，递归公式为：
    $m[i, j] = \begin{cases} 0 & \text{if } i = j \\ \min_{i \le k < j} \{m[i, k] + m[k+1, j] + p_{i-1}p_k p_j\} & \text{if } i < j \end{cases}$

**8.3.4. 计算最优代价**

与钢条切割问题类似，直接递归求解会因为重叠子问题而导致指数级的时间复杂度。我们采用自底向上的表格法。

*   **算法 `MATRIX-CHAIN-ORDER(p)`**：
    *   我们需要两个二维表：
        *   `m[1..n, 1..n]` 存储最优代价 $m[i, j]$。
        *   `s[1..n-1, 2..n]` 存储最优分割点 $k$。
    *   算法按矩阵链的长度 $l$ 从 2 到 $n$ 进行迭代。
    *   在每次关于 $l$ 的迭代中，它计算所有长度为 $l$ 的子链 $A_{i..j}$（其中 $j = i+l-1$）的最优代价。

    ```
    MATRIX-CHAIN-ORDER(p)
    1. n = p.length - 1
    2. let m[1..n, 1..n] and s[1..n-1, 2..n] be new tables
    3. for i = 1 to n
    4.     m[i, i] = 0
    5. for l = 2 to n  // l is the chain length
    6.     for i = 1 to n - l + 1
    7.         j = i + l - 1
    8.         m[i, j] = ∞
    9.         for k = i to j - 1
    10.            q = m[i, k] + m[k+1, j] + p[i-1]*p[k]*p[j]
    11.            if q < m[i, j]
    12.                m[i, j] = q
    13.                s[i, j] = k
    14. return m and s
    ```

*   **复杂度分析**：
    *   **时间复杂度**：算法有三层嵌套循环。外层循环 `l` 从 2 到 $n$；第二层循环 `i` 大约 $n$ 次；内层循环 `k` 最多 $n-1$ 次。因此，总的运行时间是 $O(n^3)$。
    *   **空间复杂度**：需要两个 $n \times n$ 的表格，因此空间复杂度为 $\Theta(n^2)$。

**8.3.5. 构造最优解**

算法 `MATRIX-CHAIN-ORDER` 返回了 `m` 和 `s` 两个表。`s` 表记录了最优分割点，我们可以用它来递归地构造出最优的括号化方案。

*   **算法 `PRINT-OPTIMAL-PARENS(s, i, j)`**：
    ```
    PRINT-OPTIMAL-PARENS(s, i, j)
    1. if i == j
    2.     print "A"i
    3. else
    4.     print "("
    5.     PRINT-OPTIMAL-PARENS(s, i, s[i, j])
    6.     PRINT-OPTIMAL-PARENS(s, s[i, j] + 1, j)
    7.     print ")"
    ```
    初始调用为 `PRINT-OPTIMAL-PARENS(s, 1, n)`。

##### **8.4. 最长公共子序列 (Longest Common Subsequence, LCS)**

**8.4.1. 问题定义**

*   **子序列 (Subsequence)**：
    给定一个序列 $X=<x_1, x_2, ..., x_m>$，另一个序列 $Z=<z_1, z_2, ..., z_k>$ 是 $X$ 的一个子序列，如果存在一个严格递增的 $X$ 的下标序列 $<i_1, i_2, ..., i_k>$，使得对于所有的 $j=1, 2, ..., k$，都有 $x_{i_j} = z_j$。
    *   **示例**：序列 $Z=<B, C, D, B>$ 是序列 $X=<A, B, C, B, D, A, B>$ 的一个子序列，对应的下标序列为 $<2, 3, 5, 7>$。

*   **公共子序列 (Common Subsequence)**：
    给定两个序列 $X$ 和 $Y$，如果一个序列 $Z$ 同时是 $X$ 和 $Y$ 的子序列，则称 $Z$ 是 $X$ 和 $Y$ 的一个公共子序列。

*   **最长公共子序列 (LCS)**：
    在 $X$ 和 $Y$ 的所有公共子序列中，长度最长的那个（可能不唯一）。

*   **问题**：给定两个序列 $X$ 和 $Y$，找出它们的一个最长公共子序列。

*   **应用**：
    LCS问题在生物信息学中非常重要，用于比较DNA序列的相似性。DNA序列可以看作是由字符集 $\{A, C, G, T\}$ 构成的字符串。两个DNA序列的LCS长度可以作为衡量它们相似度的一个指标。

**8.4.2. 刻画最优子结构**

这是动态规划的第一步。我们需要找到LCS问题的最优子结构。

*   **符号**：
    *   令 $X=<x_1, x_2, ..., x_m>$ 和 $Y=<y_1, y_2, ..., y_n>$ 为两个序列。
    *   令 $Z=<z_1, z_2, ..., z_k>$ 为 $X$ 和 $Y$ 的任意一个LCS。
    *   我们用 $X_i$ 表示 $X$ 的前 $i$ 个元素组成的前缀，即 $X_i=<x_1, ..., x_i>$。

*   **最优子结构定理 (Theorem 15.1)**：
    1.  **如果 $x_m = y_n$**：
        那么 $z_k = x_m = y_n$，并且 $Z_{k-1}$（$Z$ 的前 $k-1$ 个元素）是 $X_{m-1}$ 和 $Y_{n-1}$ 的一个LCS。
        *   **证明**：
            *   首先，$z_k$ 必须是 $x_m$。如果不是，我们可以将 $x_m$ 添加到 $Z$ 的末尾，得到一个长度为 $k+1$ 的公共子序列，这与 $Z$ 是LCS（长度为$k$）相矛盾。
            *   其次，$Z_{k-1}$ 必须是 $X_{m-1}$ 和 $Y_{n-1}$ 的一个LCS。如果存在一个比 $Z_{k-1}$ 更长的 $X_{m-1}$ 和 $Y_{n-1}$ 的公共子序列 $W$，那么将 $x_m$ 添加到 $W$ 的末尾，会得到一个比 $Z$ 更长的 $X$ 和 $Y$ 的公共子序列，同样产生矛盾。

    2.  **如果 $x_m \ne y_n$**：
        那么 $z_k \ne x_m$ 或者 $z_k \ne y_n$（或两者都不等）。
        *   **如果 $z_k \ne x_m$**，那么 $Z$ 必须是 $X_{m-1}$ 和 $Y$ 的一个LCS。
        *   **如果 $z_k \ne y_n$**，那么 $Z$ 必须是 $X$ 和 $Y_{n-1}$ 的一个LCS。
        *   **证明**：如果 $z_k \ne x_m$，那么 $Z$ 是 $X_{m-1}$ 和 $Y$ 的一个公共子序列。如果它不是LCS，即存在一个更长的公共子序列 $W$，那么 $W$ 也将是 $X$ 和 $Y$ 的公共子序列，且比 $Z$ 长，产生矛盾。同理可证另一种情况。

*   **结论**：LCS问题具有最优子结构。一个LCS的解包含了两个序列前缀的LCS。

**8.4.3. 递归解**

根据最优子结构，我们可以定义一个递归公式来计算LCS的长度。

*   令 $c[i, j]$ 表示序列 $X_i$ 和 $Y_j$ 的LCS的长度。

*   **递归公式**：
    $c[i, j] = \begin{cases} 0 & \text{if } i = 0 \text{ or } j = 0 \\ c[i-1, j-1] + 1 & \text{if } i, j > 0 \text{ and } x_i = y_j \\ \max(c[i-1, j], c[i, j-1]) & \text{if } i, j > 0 \text{ and } x_i \ne y_j \end{cases}$

**8.4.4. 计算最优代价（LCS长度）**

直接使用递归会因为重叠子问题导致效率低下。我们使用自底向上的动态规划方法。

*   **算法 `LCS-LENGTH(X, Y)`**：
    *   该算法使用一个二维表 $c[0..m, 0..n]$ 来存储 $c[i, j]$ 的值。
    *   为了方便重构解，通常会使用另一个表 $b[1..m, 1..n]$ 来记录每个解的来源（左上、上或左）。

    ```
    LCS-LENGTH(X, Y)
    1. m = X.length
    2. n = Y.length
    3. let b[1..m, 1..n] and c[0..m, 0..n] be new tables
    4. for i = 1 to m
    5.     c[i, 0] = 0
    6. for j = 0 to n
    7.     c[0, j] = 0
    8. for i = 1 to m
    9.     for j = 1 to n
    10.        if x[i] == y[j]
    11.            c[i, j] = c[i-1, j-1] + 1
    12.            b[i, j] = "↖"
    13.        else if c[i-1, j] >= c[i, j-1]
    14.            c[i, j] = c[i-1, j]
    15.            b[i, j] = "↑"
    16.        else
    17.            c[i, j] = c[i, j-1]
    18.            b[i, j] = "←"
    19. return c and b
    ```

*   **复杂度分析**：
    *   **时间复杂度**：填充表格需要两个嵌套的 `for` 循环，因此时间复杂度为 $\Theta(mn)$。
    *   **空间复杂度**：需要两个大小为 $m \times n$ 的表格，空间复杂度为 $\Theta(mn)$。

**8.4.5. 构造LCS**

有了 `b` 表，我们可以从 $b[m, n]$ 开始，根据箭头方向回溯，从而构造出LCS。

*   **算法 `PRINT-LCS(b, X, i, j)`**：
    ```
    PRINT-LCS(b, X, i, j)
    1. if i == 0 or j == 0
    2.     return
    3. if b[i, j] == "↖"
    4.     PRINT-LCS(b, X, i-1, j-1)
    5.     print X[i]
    6. else if b[i, j] == "↑"
    7.     PRINT-LCS(b, X, i-1, j)
    8. else
    9.     PRINT-LCS(b, X, i, j-1)
    ```
    初始调用为 `PRINT-LCS(b, X, m, n)`。这个递归过程打印出的是一个反向的LCS。

*   **复杂度分析**：
    *   **时间复杂度**：回溯过程的每一步都将 $i$ 或 $j$（或两者）减1，因此时间复杂度为 $O(m+n)$。

**8.4.6. 空间优化**

*   如果**只需要LCS的长度**，而不需要构造LCS本身，我们可以优化空间复杂度。注意到计算 $c$ 表的第 $i$ 行时，我们只需要第 $i-1$ 行的信息。因此，我们只需要保留两行（当前行和上一行）即可，将空间复杂度降为 $O(\min(m, n))$。
*   如果**需要构造LCS**，这种简单的空间优化方法将不再适用，因为它没有保存足够的信息来回溯路径。不过，存在更复杂的算法（如Hirschberg算法）可以在 $O(mn)$ 时间和 $O(\min(m, n))$ 空间内解决LCS问题。


##### **8.5. 最优二叉搜索树 (Optimal Binary Search Trees, OBST)**

**8.5.1. 问题定义**

*   **背景**：
    我们希望构建一个二叉搜索树 (BST)，用于存储一组有序的关键字。在实际应用中，不同的关键字被搜索的频率可能不同。例如，在编程语言的编译器中，像 `if`, `for`, `while` 这样的保留字被查找的频率远高于一些不常用的库函数名。
    此外，有些搜索可能是**不成功的**，即搜索的关键字不在树中。

*   **输入**：
    1.  一个由 $n$ 个不同关键字组成的有序序列 $K = <k_1, k_2, ..., k_n>$，其中 $k_1 < k_2 < ... < k_n$。
    2.  每个关键字 $k_i$ 被成功搜索的概率 $p_i$。
    3.  $n+1$ 个“伪关键字” (dummy keys) $d_0, d_1, ..., d_n$。
        *   $d_0$ 代表所有小于 $k_1$ 的值。
        *   $d_n$ 代表所有大于 $k_n$ 的值。
        *   对于 $i=1, ..., n-1$，$d_i$ 代表所有在 $k_i$ 和 $k_{i+1}$ 之间的值。
    4.  每个伪关键字 $d_i$ 被搜索的概率 $q_i$。
    *   所有概率的总和为1：$\sum_{i=1}^{n} p_i + \sum_{i=0}^{n} q_i = 1$。

*   **二叉搜索树结构**：
    *   关键字 $k_i$ 作为内部节点。
    *   伪关键字 $d_i$ 作为叶子节点。

*   **期望搜索代价**：
    一次搜索的代价是访问的节点数，即 `depth(node) + 1`。一棵树 $T$ 的期望搜索代价 $E[\text{search cost in } T]$ 为：
    $E[\text{search cost in } T] = \sum_{i=1}^{n} (\text{depth}_T(k_i) + 1) \cdot p_i + \sum_{i=0}^{n} (\text{depth}_T(d_i) + 1) \cdot q_i$
    其中 $\text{depth}_T(\text{node})$ 是节点在树 $T$ 中的深度（根的深度为0）。
    这个公式可以重写为：
    $E[\text{search cost in } T] = \sum_{i=1}^{n} \text{depth}_T(k_i) \cdot p_i + \sum_{i=0}^{n} \text{depth}_T(d_i) \cdot q_i + \sum_{i=1}^{n} p_i + \sum_{i=0}^{n} q_i$
    由于 $\sum p_i + \sum q_i = 1$，上式可简化为：
    $E[\text{search cost in } T] = 1 + \sum_{i=1}^{n} \text{depth}_T(k_i) \cdot p_i + \sum_{i=0}^{n} \text{depth}_T(d_i) \cdot q_i$

*   **目标**：
    给定概率 $p_i$ 和 $q_i$，构造一棵期望搜索代价最小的二叉搜索树。这样的树被称为**最优二叉搜索树 (Optimal BST)**。

**8.5.2. 刻画最优子结构**

1.  **子树的性质**：
    一棵二叉搜索树的任意子树，其包含的关键字必然是原关键字集合中一个**连续的子序列** $k_i, ..., k_j$ ($1 \le i \le j \le n$)。同时，这棵子树的叶子节点（伪关键字）也必然是对应的连续序列 $d_{i-1}, ..., d_j$。

2.  **最优子结构**：
    考虑一棵包含关键字 $k_i, ..., k_j$ 的最优二叉搜索树 $T$。假设这棵树的根是 $k_r$（其中 $i \le r \le j$）。
    *   $T$ 的左子树包含关键字 $k_i, ..., k_{r-1}$ 和伪关键字 $d_{i-1}, ..., d_{r-1}$。
    *   $T$ 的右子树包含关键字 $k_{r+1}, ..., k_j$ 和伪关键字 $d_r, ..., d_j$。
    *   **关键洞察**：如果 $T$ 是最优的，那么它的左子树必须是针对其所含关键字和伪关键字集合的**最优二叉搜索树**，同理，右子树也必须是其相应集合的最优二叉搜索树。
    *   **证明**（剪切-粘贴法）：如果存在一棵比 $T$ 的左子树更优的子树 $T'$，我们可以用 $T'$ 替换掉 $T$ 的左子树，从而得到一棵比 $T$ 期望搜索代价更低的树。这与 $T$ 是最优的假设相矛盾。

**8.5.3. 递归解**

基于最优子结构，我们可以建立递归关系。

*   **子问题定义**：
    令 $e[i, j]$ 为包含关键字 $k_i, ..., k_j$ 的一棵最优二叉搜索树的期望搜索代价。我们的目标是求解 $e[1, n]$。

*   **基本情况**：
    当 $j = i-1$ 时，子树中不包含任何实际关键字，只包含一个伪关键字 $d_{i-1}$。这棵树只有一个节点（叶子），深度为0。其期望搜索代价为：
    $e[i, i-1] = (\text{depth}(d_{i-1}) + 1) \cdot q_{i-1} = (0+1) \cdot q_{i-1} = q_{i-1}$。

*   **递归步骤**：
    当 $j \ge i$ 时，我们需要从 $k_i, ..., k_j$ 中选择一个根 $k_r$。
    *   当 $k_r$ 成为根时，其左子树包含关键字 $k_i, ..., k_{r-1}$，右子树包含 $k_{r+1}, ..., k_j$。
    *   当一棵最优子树成为一个节点的子树时，其所有节点的深度都增加1。这使得该子树的期望搜索代价增加其所含所有关键字和伪关键字的概率之和。
    *   令 $w(i, j) = \sum_{l=i}^{j} p_l + \sum_{l=i-1}^{j} q_l$。
    *   如果选择 $k_r$ 为根，则新的期望搜索代价为：
        $e[i, r-1] + e[r+1, j] + w(i, j)$
        这个式子可以这样理解：根 $k_r$ 的代价是 $p_r$（深度为0，贡献 $p_r$），左子树的代价是 $e[i, r-1]$ 加上其概率和 $w(i, r-1)$，右子树的代价是 $e[r+1, j]$ 加上其概率和 $w(r+1, j)$。而 $p_r + w(i, r-1) + w(r+1, j) = w(i, j)$。
    *   我们需要在所有可能的根 $k_r$（$i \le r \le j$）中进行选择，以使总代价最小。

*   **最终递归公式**：
    $e[i, j] = \begin{cases} q_{i-1} & \text{if } j = i-1 \\ \min_{i \le r \le j} \{e[i, r-1] + e[r+1, j] + w(i, j)\} & \text{if } i \le j \end{cases}$

**8.5.4. 计算最优代价**

与矩阵链乘法非常相似，我们使用自底向上的方法填充表格。

*   **算法 `OPTIMAL-BST(p, q, n)`**：
    *   需要三个表格：
        *   `e[1..n+1, 0..n]` 存储期望代价。
        *   `w[1..n+1, 0..n]` 存储概率和，可以预先计算以提高效率。$w[i, j] = w[i, j-1] + p_j + q_j$。
        *   `root[1..n, 1..n]` 存储最优子树的根。
    *   算法按子树包含的关键字数量 $l$（从1到n）进行迭代。

*   **复杂度分析**：
    *   **时间复杂度**：与矩阵链乘法一样，具有三层嵌套循环结构，因此时间复杂度为 $\Theta(n^3)$。
    *   **空间复杂度**：需要三个 $n \times n$ 规模的表格，空间复杂度为 $\Theta(n^2)$。


好的，我们继续第三部分的学习，接下来将探讨另一种重要的算法设计范式——贪心算法。

---

### **9. 贪心算法 (Greedy Algorithms)**

贪心算法在每一步选择中都采取在当前状态下最好或最优（即最有利）的选择，从而希望导致全局最好或最优的解。贪心算法并不从整体最优上加以考虑，它所做的选择只是在某种意义上的**局部最优选择**。

##### **9.1. 核心思想**

与动态规划不同，贪心算法不是首先寻找子问题的最优解，然后再从子问题的最优解构造原问题的最优解。它在做出选择之前，并不求解所有相关的子问题。

一个问题能用贪心算法求解，通常需要满足以下两个关键性质：

1.  **贪心选择性质 (Greedy-Choice Property)**：
    一个全局最优解可以通过局部最优（贪心）选择来达到。也就是说，在做选择时，我们直接做出在当前问题中看来最优的选择，而不用考虑子问题的解。这是贪心算法与动态规划的主要区别。在动态规划中，选择依赖于子问题的解；而在贪心算法中，我们先做出贪心选择，然后求解剩下的那一个子问题。

2.  **最优子结构 (Optimal Substructure)**：
    一个问题的最优解包含了其子问题的最优解。这个性质与动态规划相同。

##### **9.2. 活动选择问题 (Activity-Selection Problem)**

这是一个经典的可以用贪心算法解决的问题。

*   **问题描述**：
    *   **输入**：一个包含 $n$ 个活动的集合 $S = \{a_1, a_2, ..., a_n\}$，每个活动 $a_i$ 都有一个开始时间 $s_i$ 和一个结束时间 $f_i$，其中 $0 \le s_i < f_i < \infty$。
    *   **兼容性**：两个活动 $a_i$ 和 $a_j$ 是兼容的，如果它们的活动时间区间 $[s_i, f_i)$ 和 $[s_j, f_j)$ 不重叠。
    *   **目标**：找出一个由互相兼容的活动组成的最大子集。

*   **动态规划解法（初步分析）**：
    1.  **最优子结构**：
        *   首先，为了便于处理，我们将活动按结束时间单调递增排序：$f_1 \le f_2 \le ... \le f_n$。
        *   定义子问题 $S_{ij}$ 为在活动 $a_i$ 结束之后开始，且在活动 $a_j$ 开始之前结束的所有活动的集合。
        *   假设 $S_{ij}$ 的一个最优解包含活动 $a_k$。那么这个解就由 $a_k$ 以及 $S_{ik}$ 的一个最优解和 $S_{kj}$ 的一个最优解构成。
        *   令 $c[i, j]$ 为 $S_{ij}$ 的最优解的大小，则有递归式：
            $c[i, j] = \max_{a_k \in S_{ij}} \{c[i, k] + c[k, j] + 1\}$
    2.  这个动态规划的解法是可行的，但我们可以做得更好。

*   **贪心选择**：
    *   **贪心选择性质的探索**：我们应该如何做出贪心选择？
        *   选择开始时间最早的活动？（反例：一个开始很早但持续很长的活动可能会阻塞很多其他活动）
        *   选择持续时间最短的活动？（反例：一个持续时间很短但恰好与两个较长活动重叠的活动可能会导致非最优解）
        *   选择结束时间最早的活动？这似乎是一个有前景的策略。

    *   **定理 16.1**：对于任意非空的活动选择子问题 $S_k$，令 $a_m$ 是 $S_k$ 中结束时间最早的活动。那么，$a_m$ 必定在 $S_k$ 的某个最大兼容活动子集中。
        *   **证明**（剪切-粘贴法）：
            假设 $A_k$ 是 $S_k$ 的一个最大兼容活动子集，且其中结束时间最早的活动是 $a_j$。
            *   如果 $a_j = a_m$，则定理成立。
            *   如果 $a_j \ne a_m$，因为 $a_m$ 是 $S_k$ 中结束时间最早的活动，所以 $f_m \le f_j$。我们可以在 $A_k$ 中用 $a_m$ 替换 $a_j$，得到一个新的兼容活动子集 $A'_k$。因为 $f_m \le f_j$，所以 $a_m$ 与 $A_k$ 中除了 $a_j$ 之外的其他活动都是兼容的。$A'_k$ 的大小与 $A_k$ 相同，因此它也是一个最大兼容子集，并且它包含了我们贪心选择的活动 $a_m$。

*   **贪心算法**：
    1.  **做出贪心选择**：选择具有最早结束时间的活动 $a_m$。
    2.  **形成子问题**：这个选择使得所有与 $a_m$ 不兼容的活动都不能再被选择。剩下的唯一子问题是寻找在 $a_m$ 结束之后开始的活动集合中的最优解。
    3.  **递归求解**：递归地对这个子问题应用相同的策略。

*   **算法实现**：
    *   **递归贪心算法 `RECURSIVE-ACTIVITY-SELECTOR`**：
        假设活动已按结束时间排序。算法选择第一个活动 $a_1$，然后递归地在所有与 $a_1$ 兼容的活动中寻找最优解。
    *   **迭代贪心算法 `GREEDY-ACTIVITY-SELECTOR`**：
        可以更高效地实现为迭代形式。
        ```
        GREEDY-ACTIVITY-SELECTOR(s, f)
        1. n = s.length
        2. A = {a₁}
        3. k = 1
        4. for m = 2 to n
        5.     if s[m] >= f[k]  // 如果活动m与上一个选中的活动ak兼容
        6.         A = A ∪ {am}
        7.         k = m
        8. return A
        ```

*   **复杂度分析**：
    *   如果活动未排序，首先需要 $O(n \log n)$ 的时间进行排序。
    *   贪心选择过程只需要对活动进行一次遍历，时间为 $\Theta(n)$。
    *   总时间复杂度为 $O(n \log n)$。

##### **9.3. 贪心算法与动态规划的对比**

| 特性 | 动态规划 (Dynamic Programming) | 贪心算法 (Greedy Algorithm) |
| :--- | :--- | :--- |
| **选择** | 每一步的选择依赖于子问题的解。通常需要求解所有子问题，然后选择最优的。 | 每一步都做出一个局部最优的贪心选择，不考虑子问题的解。 |
| **求解顺序** | 通常是自底向上，先解决小问题，再解决大问题。 | 通常是自顶向下，做出一个选择，然后处理剩下的子问题。 |
| **适用性** | 最优子结构 + 重叠子问题。 | 贪心选择性质 + 最优子结构。 |
| **证明** | 证明最优子结构性质。 | 证明贪心选择性质和最优子结构。证明贪心选择是“安全的”通常是关键。 |

##### **9.4. 背包问题 (Knapsack Problem)**

这是一个很好的例子，用来说明贪心选择性质的重要性。

*   **0-1 背包问题**：
    *   **描述**：一个小偷有一个载重为 $W$ 的背包。有 $n$ 件物品，第 $i$ 件物品价值为 $v_i$，重量为 $w_i$。小偷希望带走价值最高的物品，但总重量不能超过 $W$。对于每件物品，小偷要么完整地拿走，要么不拿。
    *   **解法**：此问题**不能**用贪心算法解决。例如，按单位价值（$v_i/w_i$）最高的贪心策略可能会导致非最优解。
    *   **正确解法**：动态规划。此问题具有最优子结构，但不具有贪心选择性质。

*   **分数背包问题 (Fractional Knapsack Problem)**：
    *   **描述**：与0-1背包问题类似，但小偷可以拿走物品的一部分。
    *   **解法**：此问题**可以**用贪心算法解决。
        *   **贪心策略**：计算每件物品的单位价值 $v_i/w_i$。
        *   总是优先拿走单位价值最高的物品。如果该物品可以全部放入背包，则全部拿走。如果不能，则用该物品的一部分装满背包的剩余容量。
        *   **正确性**：可以证明这个贪心策略总是能得到最优解。

好的，我们继续第三部分的最后一节，探讨一种特殊的算法分析技术——摊还分析。

---

### **10. 摊还分析 (Amortized Analysis)**

在摊还分析中，我们分析的是一个操作序列在**最坏情况下**的平均时间。它与平均情况分析不同，因为它不涉及概率，而是对最坏情况下的一个操作序列的总时间求平均。

**核心思想**：一个操作序列中可能包含一些代价非常高的操作，但这些操作的出现频率可能很低。摊还分析旨在证明，在一个操作序列中，每个操作的**平均代价**（称为**摊还代价**）是很低的，即使序列中某个操作的实际代价可能很高。

我们介绍三种常用的摊还分析技术。

#### **10.1. 聚合分析 (Aggregate Analysis)**

聚合分析直接计算整个操作序列的总代价，然后求平均。

*   **方法**：
    1.  确定一个包含 $n$ 个操作的序列。
    2.  计算这个序列在最坏情况下的总时间代价 $T(n)$。
    3.  每个操作的摊还代价即为 $T(n) / n$。

*   **示例1：栈操作**
    *   **操作**：`PUSH(S, x)`（代价1），`POP(S)`（代价1），`MULTIPOP(S, k)`（代价 $\min(k, s)$，其中 $s$ 是栈大小）。
    *   **分析**：考虑一个对空栈执行的任意 $n$ 个操作的序列。
        *   一个对象最多只能被 `POP` 一次（包括在 `MULTIPOP` 中被弹出）。
        *   要 `POP` 一个对象，它必须先进栈。
        *   因此，`POP` 和 `MULTIPOP` 的总调用次数不能超过 `PUSH` 的总次数，而 `PUSH` 的总次数最多为 $n$。
        *   所以，所有 `POP` 和 `MULTIPOP` 的总代价最多与 `PUSH` 的总代价（$\Theta(n)$）在同一个数量级。
        *   整个序列的总代价 $T(n) = O(n)$。
        *   **摊还代价**：每个操作的摊还代价为 $O(n) / n = O(1)$。

*   **示例2：二进制计数器**
    *   **操作**：对一个 $k$ 位的二进制计数器执行 `INCREMENT` 操作 $n$ 次。
    *   **代价**：`INCREMENT` 操作的代价等于翻转的位数。
    *   **分析**：
        *   位 $A$ 每次 `INCREMENT` 都会翻转，共翻转 $n$ 次。
        *   位 $A$ 每 2 次 `INCREMENT` 翻转一次，共翻转 $\lfloor n/2 \rfloor$ 次。
        *   位 $A[i]$ 每 $2^i$ 次 `INCREMENT` 翻转一次，共翻转 $\lfloor n/2^i \rfloor$ 次。
        *   $n$ 次操作的总翻转次数为 $\sum_{i=0}^{k-1} \lfloor n/2^i \rfloor < \sum_{i=0}^{\infty} n/2^i = 2n$。
        *   总代价 $T(n) < 2n$，即 $T(n) = O(n)$。
        *   **摊还代价**：每个 `INCREMENT` 操作的摊还代价为 $O(n) / n = O(1)$。

#### **10.2. 核算法 (Accounting Method)**

核算法（也称记账法）是一种更精细的分析方法，它为不同操作赋予不同的**摊还代价 (Amortized Cost)** $\hat{c}_i$。

*   **方法**：
    *   如果一个操作的摊还代价 $\hat{c}_i$ 大于其实际代价 $c_i$，差额 $\hat{c}_i - c_i$ 就作为**信用 (Credit)** 存储在数据结构的特定对象上。
    *   如果一个操作的摊还代价 $\hat{c}_i$ 小于其实际代价 $c_i$，差额 $c_i - \hat{c}_i$ 就用之前存储的信用（如果足够的话）来支付。
    *   **关键约束**：数据结构中任何时刻的总信用都必须是非负的。
    *   如果能满足这个约束，那么所有 $n$ 个操作的总实际代价 $\sum_{i=1}^{n} c_i$ 将小于等于总摊还代价 $\sum_{i=1}^{n} \hat{c}_i$。

*   **示例1：栈操作**
    *   **摊还代价分配**：
        *   `PUSH`: $\hat{c} = 2$
        *   `POP`: $\hat{c} = 0$
        *   `MULTIPOP`: $\hat{c} = 0$
    *   **分析**：
        *   当执行 `PUSH` 时，实际代价是 1。我们支付 2，其中 1 用于支付实际成本，另外 1 作为信用存放在新压入栈的元素上。
        *   因此，栈中每个元素都带有 1 个单位的信用。
        *   当执行 `POP` 或 `MULTIPOP` 弹出某个元素时，该元素的实际代价是 1。我们使用存储在该元素上的 1 个单位信用去支付这个成本。
        *   由于每个元素都带有 1 的信用，我们总有足够的信用去支付 `POP` 和 `MULTIPOP` 的成本。
        *   总信用始终非负。
        *   **结论**：每个操作的摊还代价都是 $O(1)$，因此 $n$ 个操作的总时间是 $O(n)$。

*   **示例2：二进制计数器**
    *   **摊还代价分配**：
        *   将一个位从 0 翻转到 1：摊还代价为 2。
        *   将一个位从 1 翻转到 0：摊还代价为 0。
    *   **分析**：
        *   当一个位被置为 1 时，我们支付 2。其中 1 用于支付实际的置位成本，另外 1 作为信用“存”在该位上。
        *   在任何时候，计数器中每个为 1 的位都存有 1 个单位的信用。
        *   当 `INCREMENT` 操作将某个位从 1 翻转到 0 时，其实际代价是 1。我们使用该位上存储的 1 个单位信用去支付这个成本。
        *   `INCREMENT` 操作最多只会将一个位从 0 变为 1。因此，每次 `INCREMENT` 的摊还代价最多为 2。
        *   **结论**：每个 `INCREMENT` 操作的摊还代价是 $O(1)$。

#### **10.3. 势能法 (Potential Method)**

势能法是另一种摊还分析技术，它将预付的代价表示为“势能”，存储在整个数据结构中。

*   **方法**：
    1.  定义一个**势函数 (Potential Function)** $\Phi$，它将数据结构的每个状态 $D_i$ 映射到一个实数 $\Phi(D_i)$，即该状态的**势**。
    2.  我们要求对于初始状态 $D_0$，有 $\Phi(D_0) = 0$，并且对于所有 $i$，有 $\Phi(D_i) \ge 0$。
    3.  第 $i$ 个操作的摊还代价 $\hat{c}_i$ 定义为：
        $\hat{c}_i = c_i + \Phi(D_i) - \Phi(D_{i-1})$
        其中 $c_i$ 是实际代价，$D_{i-1}$ 和 $D_i$ 分别是操作前后的数据结构状态。$\Delta\Phi_i = \Phi(D_i) - \Phi(D_{i-1})$ 是势的变化。
    4.  $n$ 个操作的总摊还代价为：
        $\sum_{i=1}^{n} \hat{c}_i = \sum_{i=1}^{n} (c_i + \Phi(D_i) - \Phi(D_{i-1})) = (\sum_{i=1}^{n} c_i) + \Phi(D_n) - \Phi(D_0)$
    5.  由于 $\Phi(D_n) \ge \Phi(D_0)$（通常 $\Phi(D_0)=0$），总摊还代价是总实际代价的一个上界。

*   **示例：动态表 (Table Expansion)**
    * **操作**：`TABLE-INSERT`。当表满时，分配一个大小为原来两倍的新表，并将旧表元素复制过去。
    
    *   **实际代价**：
        *   不扩容：$c_i = 1$。
        *   扩容：$c_i = i$（假设第 $i$ 次插入时表的大小为 $i-1$，需要复制 $i-1$ 个元素，再插入新元素）。
        
    *   **势函数定义**：
        令 $\text{num}_i$ 为第 $i$ 次操作后的元素数量，$\text{size}_i$ 为表的大小。
        定义势函数 $\Phi(D_i) = 2 \cdot \text{num}_i - \text{size}_i$。
        *   初始时，$\text{num}_0 = \text{size}_0 = 0$，所以 $\Phi(D_0) = 0$。
        *   在任何时候，由于表至少是半满的（除了刚扩容后），$\text{num}_i \ge \text{size}_i / 2$，所以 $\Phi(D_i) \ge 0$。
        
    *   **分析**：
        1.  **不扩容的情况**：
            $\text{num}_i = \text{num}_{i-1} + 1$, $\text{size}_i = \text{size}_{i-1}$。
            $\hat{c}_i = c_i + \Phi(D_i) - \Phi(D_{i-1})$
            $= 1 + (2\text{num}_i - \text{size}_i) - (2\text{num}_{i-1} - \text{size}_{i-1})$
            $= 1 + (2(\text{num}_{i-1}+1) - \text{size}_{i-1}) - (2\text{num}_{i-1} - \text{size}_{i-1})$
            $= 1 + 2 = 3$。
        2.  **扩容的情况**：
            $\text{num}_i = \text{num}_{i-1} + 1$, $\text{size}_{i-1} = \text{num}_{i-1} = i-1$, $\text{size}_i = 2\text{size}_{i-1}$。
            $\hat{c}_i = c_i + \Phi(D_i) - \Phi(D_{i-1})$
            $= i + (2\text{num}_i - \text{size}_i) - (2\text{num}_{i-1} - \text{size}_{i-1})$
            $= i + (2i - 2(i-1)) - (2(i-1) - (i-1))$
            $= i + 2 - (i-1) = 3$。
        
    * **结论**：在所有情况下，`TABLE-INSERT` 的摊还代价都是常数 3。因此，$n$ 次插入的总时间是 $\Theta(n)$。
    

好的，我们现在开始第四部分，这是一个非常庞大且重要的章节，涵盖了各种核心的图算法。

---

### **第四部分：图算法**

图算法是计算机科学中应用最广泛的领域之一，用于解决网络、调度、地理信息系统等众多问题。

#### **11. 基本图算法**

##### **11.1. 图的表示**

图 $G = (V, E)$ 由顶点集 $V$ 和边集 $E$ 组成。

*   **邻接链表 (Adjacency List)**：
    *   **表示**：对于每个顶点 $u \in V$，使用一个链表 `Adj[u]` 来存储所有与 $u$ 相邻的顶点。
    *   **空间复杂度**：$\Theta(V+E)$。
    *   **优点**：对于**稀疏图**（$|E|$ 远小于 $|V|^2$）非常节省空间。
    *   **缺点**：检查边 $(u, v)$ 是否存在需要 $O(\text{degree}(u))$ 的时间。

*   **邻接矩阵 (Adjacency Matrix)**：
    *   **表示**：使用一个 $|V| \times |V|$ 的矩阵 $A=(a_{ij})$，其中：
        $a_{ij} = \begin{cases} 1 & \text{if } (i, j) \in E \\ 0 & \text{otherwise} \end{cases}$
    *   **空间复杂度**：$\Theta(V^2)$。
    *   **优点**：检查边 $(u, v)$ 是否存在只需要 $O(1)$ 时间。对于**稠密图**（$|E|$ 接近 $|V|^2$）效率较高。
    *   **缺点**：空间消耗巨大，不适用于稀疏图。

##### **11.2. 广度优先搜索 (Breadth-First Search, BFS)**

BFS 是一种系统性地探索图的算法，它从源顶点 $s$ 开始，逐层向外扩展，首先访问所有距离 $s$ 为 $k$ 的顶点，然后再访问距离为 $k+1$ 的顶点。

*   **核心数据结构**：**队列 (Queue)**，用于存储待访问的灰色顶点。

*   **算法过程 `BFS(G, s)`**：
    1.  初始化：将除源点 $s$ 外的所有顶点标记为白色（未发现），距离设为 $\infty$，前驱设为 `NIL`。将源点 $s$ 标记为灰色（已发现），距离设为 0。
    2.  将源点 $s$ 入队。
    3.  当队列不为空时，循环：
        a.  将队首顶点 $u$ 出队。
        b.  遍历 $u$ 的所有邻接顶点 $v$：
            *   如果 $v$ 是白色的，则将其标记为灰色，设置其距离 $v.d = u.d + 1$，前驱 $v.\pi = u$，并将 $v$ 入队。
        c.  将 $u$ 标记为黑色（已处理）。

*   **属性**：
    *   **时间复杂度**：$O(V+E)$（使用邻接链表）。每个顶点入队和出队一次，每条边最多被扫描一次。
    *   **最短路径**：BFS 能够计算源点 $s$ 到所有可达顶点的**最短路径**（以边的数量作为路径长度）。算法结束后，$v.d = \delta(s, v)$，其中 $\delta(s, v)$ 是从 $s$ 到 $v$ 的最短路径长度。
    *   **广度优先树**：算法生成的前驱子图 $G_\pi$ 是一棵**广度优先树**，包含了从 $s$ 到所有可达顶点的最短路径。

##### **11.3. 深度优先搜索 (Depth-First Search, DFS)**

DFS 尽可能“深”地探索图的分支。它从一个顶点出发，沿着路径一直前进，直到无法再前进时，才回溯到上一个顶点，继续探索其他路径。

*   **核心思想**：**递归**。

*   **算法过程 `DFS(G)`**：
    1.  初始化：将所有顶点标记为白色，前驱设为 `NIL`。初始化一个全局时间戳计数器。
    2.  遍历所有顶点 $u \in V$：
        *   如果 $u$ 是白色的，则调用 `DFS-VISIT(G, u)`。

*   **递归辅助过程 `DFS-VISIT(G, u)`**：
    1.  记录 $u$ 的发现时间 $u.d$。将 $u$ 标记为灰色。
    2.  遍历 $u$ 的所有邻接顶点 $v$：
        *   如果 $v$ 是白色的，则设置其前驱 $v.\pi = u$，并递归调用 `DFS-VISIT(G, v)`。
    3.  将 $u$ 标记为黑色，并记录其完成时间 $u.f$。

*   **属性**：
    *   **时间复杂度**：$\Theta(V+E)$（使用邻接链表）。每个顶点被 `DFS-VISIT` 调用一次，每条边被探索一次。
    *   **括号定理 (Parenthesis Theorem)**：在对有向或无向图 $G$ 的任意深度优先搜索中，对于任意两个顶点 $u$ 和 $v$，它们的发现/完成时间区间 $[u.d, u.f]$ 和 $[v.d, v.f]$ 的关系只有两种可能：
        1.  两个区间完全不相交。
        2.  一个区间完全包含在另一个区间内（此时，被包含区间的顶点是被包含区间的顶点的后代）。
    *   **边的分类**：
        1.  **树边 (Tree Edge)**：指向在DFS森林中 $u$ 的一个孩子的边。
        2.  **后向边 (Back Edge)**：指向 $u$ 的一个祖先的边（在有向图中可能导致环）。
        3.  **前向边 (Forward Edge)**：指向 $u$ 的一个后代（但非树边）的边。
        4.  **横向边 (Cross Edge)**：连接两个没有祖先-后代关系的顶点的边。

##### **11.4. 拓扑排序 (Topological Sort)**

*   **定义**：一个**有向无环图 (DAG)** 的拓扑排序是其所有顶点的一个线性排序，使得如果图中存在一条边 $(u, v)$，那么在该排序中 $u$ 出现在 $v$ 之前。

*   **算法**：
    1.  对图 $G$ 调用 `DFS`。
    2.  当每个顶点完成时（即被标记为黑色时），将其插入到一个链表的头部。
    3.  最终得到的链表就是拓扑排序的结果。

*   **正确性**：
    可以证明，对于任意边 $(u, v)$，必有 $v.f < u.f$。因为当探索 $u$ 时，如果 $v$ 是白色的，则 $v$ 成为 $u$ 的后代，会先完成；如果 $v$ 是灰色的，则存在环，与DAG的定义矛盾；如果 $v$ 是黑色的，则 $v$ 已经完成。因此，完成时间较晚的顶点总是在完成时间较早的顶点之前。

*   **时间复杂度**：$\Theta(V+E)$，与DFS相同。

##### **11.5. 强连通分量 (Strongly Connected Components)**

*   **定义**：在有向图 $G=(V, E)$ 中，一个强连通分量是顶点集 $V$ 的一个最大子集 $C \subseteq V$，对于 $C$ 中的任意两个顶点 $u, v$，都同时存在从 $u$ 到 $v$ 的路径和从 $v$ 到 $u$ 的路径。

*   **算法**：
    这是一个巧妙地两次使用DFS的算法。
    1.  对图 $G$ 调用 `DFS`，计算每个顶点的完成时间 $u.f$。
    2.  计算 $G$ 的**转置图** $G^T$（将 $G$ 中所有边的方向反向）。
    3.  对 $G^T$ 调用 `DFS`，但在主循环中，按照顶点在第一次DFS中完成时间的**降序**来遍历顶点。
    4.  在第二次DFS中，每次从一个白色顶点开始访问所生成的DFS树，都构成一个强连通分量。

*   **时间复杂度**：两次DFS，一次转置图，总时间复杂度为 $\Theta(V+E)$。

好的，我们继续第四部分图算法的学习，接下来是关于最小生成树的两种经典算法。

---

### **12. 最小生成树 (Minimum Spanning Trees, MST)**

##### **12.1. 问题定义**

*   **输入**：一个连通、无向、带权的图 $G=(V, E)$，其中每个边 $(u, v) \in E$ 都有一个权重 $w(u, v)$。
*   **生成树 (Spanning Tree)**：图 $G$ 的一个子图 $T$，它是一棵树，并且连接了 $V$ 中的所有顶点。
*   **最小生成树 (MST)**：在 $G$ 的所有生成树中，权重之和最小的那棵树。权重之和定义为 $w(T) = \sum_{(u,v) \in T} w(u, v)$。
*   **应用**：网络设计（如铺设电缆、管道），在保证所有节点连通的前提下，使总成本最低。

##### **12.2. 贪心策略基础**

MST算法是贪心策略的典型应用。其核心思想是逐步构建一棵MST。在构建过程中，我们维护一个由MST的边组成的集合 $A$。每一步，我们都向 $A$ 中添加一条“安全”的边，即这条边加入后不会破坏 $A$ 最终能扩展成一棵MST的性质。

*   **切割 (Cut)**：图的顶点集 $V$ 的一个划分 $(S, V-S)$。
*   **横跨切割 (Crosses the cut)**：如果一条边的一个端点在 $S$ 中，另一个端点在 $V-S$ 中，则称该边横跨切割。
*   **尊重切割 (Respects the cut)**：如果集合 $A$ 中没有任何边横跨切割，则称 $A$ 尊重该切割。
*   **轻量级边 (Light edge)**：在所有横跨一个切割的边中，权重最小的边（可能不唯一）。

*   **通用MST算法框架**：
    ```
    GENERIC-MST(G, w)
    1. A = ∅
    2. while A does not form a spanning tree
    3.     find an edge (u, v) that is safe for A
    4.     A = A ∪ {(u, v)}
    5. return A
    ```

*   **安全边的识别定理**：
    令 $A$ 是 $G$ 的一个生成树的某个子集，$(S, V-S)$ 是 $G$ 中任何一个尊重 $A$ 的切割，$(u, v)$ 是横跨该切割的一条轻量级边。那么，边 $(u, v)$ 对于 $A$ 是安全的。
    *   **证明**（反证法）：假设 $T$ 是一棵包含 $A$ 但不包含 $(u, v)$ 的MST。在 $T$ 中加入边 $(u, v)$ 会形成一个环。这个环上必然存在另一条横跨切割 $(S, V-S)$ 的边 $(x, y)$。由于 $(u, v)$ 是轻量级边，所以 $w(u, v) \le w(x, y)$。从 $T$ 中去掉边 $(x, y)$ 并加入边 $(u, v)$，可以得到一棵新的生成树 $T'$，其权重 $w(T') = w(T) - w(x, y) + w(u, v) \le w(T)$。因此 $T'$ 也是一棵MST，且它包含 $A$ 和 $(u, v)$。这证明了 $(u, v)$ 是安全的。

##### **12.3. Kruskal 算法**

Kruskal算法的贪心策略是：在所有连接森林中不同树的边中，选择权重最小的那条边加入。

*   **算法过程**：
    1.  初始化一个集合 $A = \emptyset$。
    2.  为每个顶点 $v \in V$ 创建一个只包含它自身的集合（使用**不相交集合数据结构**）。
    3.  将所有边 $E$ 按权重非递减排序。
    4.  遍历排序后的边 $(u, v)$：
        *   如果 $u$ 和 $v$ 位于不同的集合中（即 `FIND-SET(u) ≠ FIND-SET(v)`），则将边 $(u, v)$ 加入到 $A$ 中，并合并 $u$ 和 $v$ 所在的集合（`UNION(u, v)`）。
    5.  返回集合 $A$。

*   **正确性**：
    每当算法选择一条边 $(u, v)$ 时，它连接了两个不同的连通分量（树）。令 $S$ 为包含 $u$ 的分量中的顶点集，则 $(S, V-S)$ 是一个尊重当前 $A$ 的切割。由于算法按权重排序选择边，且 $(u, v)$ 是第一条连接 $S$ 和 $V-S$ 的边，所以它必然是横跨该切割的一条轻量级边。根据上述定理，$(u, v)$ 是安全的。

*   **复杂度分析**：
    *   排序边：$O(E \log E)$。
    *   不相交集合操作：共 $V$ 次 `MAKE-SET`， $2E$ 次 `FIND-SET`，和 $V-1$ 次 `UNION`。使用带路径压缩和按秩合并的并查集，这些操作的总时间接近线性，为 $O((V+E)\alpha(V))$，其中 $\alpha$ 是增长极其缓慢的Ackermann函数的反函数。
    *   总时间复杂度由排序主导，为 $O(E \log E)$。由于 $|E| \le |V|^2$，所以 $\log E = O(\log V)$，因此也可以写成 $O(E \log V)$。

##### **12.4. Prim 算法**

Prim算法的贪心策略是：从一个任意的根节点开始，每次选择一条连接已构建树与树外顶点的、权重最小的边，逐步“生长”出一棵MST。

*   **核心数据结构**：**最小优先队列 (Min-Priority Queue)**，用于存储所有不在树中的顶点，其关键字为该顶点与树中任意顶点相连的边的最小权重。

*   **算法过程**：
    1.  初始化：选择一个任意的根节点 $r$，将其 `key` 设为 0，其他所有顶点的 `key` 设为 $\infty$，前驱设为 `NIL`。
    2.  将所有顶点加入最小优先队列 $Q$。
    3.  当 $Q$ 不为空时，循环：
        a.  从 $Q$ 中抽取关键字最小的顶点 $u$（`EXTRACT-MIN`）。
        b.  将 $u$ 加入到MST中。
        c.  遍历 $u$ 的所有邻接顶点 $v$：
            *   如果 $v$ 仍在队列 $Q$ 中，并且边 $(u, v)$ 的权重小于 $v$ 当前的 `key` 值，则更新 $v$ 的 `key` 为 $w(u, v)$，并设置其前驱 $v.\pi = u$（`DECREASE-KEY`）。

*   **正确性**：
    算法维护一个顶点集合 $A$（已加入MST的顶点）。在每一步，算法选择的边 $(u.\pi, u)$ 是连接 $A$ 和 $V-A$ 的一条轻量级边。根据安全边定理，这条边是安全的。

*   **复杂度分析**：
    复杂度依赖于最小优先队列的实现。
    *   **基于二叉堆**：
        *   `BUILD-MIN-HEAP`：$O(V)$。
        *   `EXTRACT-MIN`：$V$ 次，每次 $O(\log V)$，总共 $O(V \log V)$。
        *   `DECREASE-KEY`：最多 $E$ 次，每次 $O(\log V)$，总共 $O(E \log V)$。
        *   **总时间复杂度**：$O(V \log V + E \log V) = O(E \log V)$。
    *   **基于斐波那契堆 (Fibonacci Heap)**：
        *   `DECREASE-KEY` 的摊还代价为 $O(1)$。
        *   **总时间复杂度**：$O(E + V \log V)$。对于稠密图，这比二叉堆实现要快。

好的，我们继续第四部分图算法的学习，接下来将探讨单源最短路径问题。

---

### **13. 单源最短路径 (Single-Source Shortest Paths)**

##### **13.1. 问题定义与预备知识**

*   **输入**：一个带权有向图 $G=(V, E)$，一个权重函数 $w: E \to \mathbb{R}$，以及一个源顶点 $s \in V$。
*   **路径权重**：一条路径 $p = <v_0, v_1, ..., v_k>$ 的权重是构成该路径的所有边的权重之和：$w(p) = \sum_{i=1}^{k} w(v_{i-1}, v_i)$。
*   **最短路径权重**：从顶点 $u$ 到 $v$ 的最短路径权重 $\delta(u, v)$ 定义为：
    $\delta(u, v) = \begin{cases} \min\{w(p) : u \xrightarrow{p} v\} & \text{如果存在从 } u \text{ 到 } v \text{ 的路径} \\ \infty & \text{否则} \end{cases}$
*   **目标**：对于给定的源点 $s$，找出从 $s$ 到图中每个顶点 $v$ 的最短路径权重 $\delta(s, v)$。

*   **变体**：
    *   **单目的地最短路径**：找出从每个顶点到指定目的地 $t$ 的最短路径。
    *   **单对最短路径**：找出从指定顶点 $u$ 到 $v$ 的最短路径。
    *   **所有结点对最短路径**：找出每对顶点 $(u, v)$ 之间的最短路径。

*   **最优子结构**：
    最短路径问题具有最优子结构。一条从 $u$ 到 $v$ 的最短路径的任何子路径，也必然是其端点之间的最短路径。

*   **负权边与环路**：
    *   **负权边**：某些算法（如Dijkstra）不允许负权边，而另一些（如Bellman-Ford）可以处理。
    *   **负权环路**：如果图中存在一个从源点 $s$ 可达的负权环路，那么环路上的顶点的最短路径权重是 $-\infty$。如果不存在这样的环路，最短路径权重就是明确定义的。
    *   **正权环路**：最短路径不会包含正权环路。

##### **13.2. 松弛操作 (Relaxation)**

所有单源最短路径算法的核心都是**松弛操作**。对于每个顶点 $v$，我们维护两个属性：
*   `v.d`：最短路径估计（从 $s$ 到 $v$ 的路径权重的上界）。
*   `v.π`：前驱顶点。

*   **初始化 `INITIALIZE-SINGLE-SOURCE(G, s)`**：
    ```
    for each vertex v in G.V
        v.d = ∞
        v.π = NIL
    s.d = 0
    ```

*   **松弛操作 `RELAX(u, v, w)`**：
    松弛操作是针对一条边 $(u, v)$ 的过程，它测试是否可以通过顶点 $u$ 来改进到顶点 $v$ 的最短路径。
    ```
    if v.d > u.d + w(u, v)
        v.d = u.d + w(u, v)
        v.π = u
    ```

*   **最短路径的性质**：
    *   **三角不等式**：对于任意边 $(u, v) \in E$，$\delta(s, v) \le \delta(s, u) + w(u, v)$。
    *   **上界性质**：对于所有 $v \in V$，我们总是有 $v.d \ge \delta(s, v)$。一旦 $v.d$ 达到 $\delta(s, v)$，它将不再改变。
    *   **收敛性质**：如果 $s \to ... \to u \to v$ 是一条最短路径，并且在对边 $(u, v)$ 进行松弛前的任意时刻有 $u.d = \delta(s, u)$，那么在松弛操作之后以及之后的所有时间，都有 $v.d = \delta(s, v)$。

##### **13.3. Bellman-Ford 算法**

Bellman-Ford算法解决了允许**负权边**存在的一般情况下的单源最短路径问题。

*   **算法思想**：
    对图中的每条边进行 $|V|-1$ 轮松弛操作。因为一条不包含环路的最短路径最多包含 $|V|-1$ 条边，经过 $|V|-1$ 轮松弛后，所有顶点的最短路径估计值都将收敛到其真实的最短路径权重。

*   **算法过程 `BELLMAN-FORD(G, w, s)`**：
    1.  `INITIALIZE-SINGLE-SOURCE(G, s)`
    2.  `for i = 1 to |G.V| - 1`
    3.      `for each edge (u, v) in G.E`
    4.          `RELAX(u, v, w)`
    5.  `for each edge (u, v) in G.E`  // 检查负权环路
    6.      `if v.d > u.d + w(u, v)`
    7.          `return FALSE` // 存在负权环路
    8.  `return TRUE`

*   **正确性与复杂度**：
    *   **正确性**：经过 $i$ 轮松弛后，算法能找到所有至多包含 $i$ 条边的从 $s$ 出发的最短路径。因此，经过 $|V|-1$ 轮后，能找到所有不含环路的最短路径。第5-7步用于检测是否存在负权环路：如果在 $|V|-1$ 轮松弛后，仍然可以对某条边进行松弛，则说明存在负权环路。
    *   **时间复杂度**：$O(VE)$。

##### **13.4. 有向无环图 (DAG) 的单源最短路径**

对于有向无环图，我们可以用更快的算法来解决单源最短路径问题，该算法同样可以处理负权边。

*   **算法思想**：
    首先对DAG进行**拓扑排序**。然后，按照拓扑排序的顺序，对每个顶点的出边进行松弛操作。

*   **算法过程 `DAG-SHORTEST-PATHS(G, w, s)`**：
    1.  `topologically sort the vertices of G`
    2.  `INITIALIZE-SINGLE-SOURCE(G, s)`
    3.  `for each vertex u, taken in topologically sorted order`
    4.      `for each vertex v in G.Adj[u]`
    5.          `RELAX(u, v, w)`

*   **正确性与复杂度**：
    *   **正确性**：因为是按照拓扑排序的顺序进行松弛，当我们处理顶点 $u$ 时，所有可能指向 $u$ 的顶点都已经被处理过，因此可以保证 $u.d = \delta(s, u)$。所以对 $u$ 的出边 $(u, v)$ 进行松弛时，是基于正确的 $u.d$ 值的。
    *   **时间复杂度**：拓扑排序为 $\Theta(V+E)$，初始化为 $\Theta(V)$，松弛操作部分每个顶点和每条边只访问一次，为 $\Theta(V+E)$。总时间复杂度为 $\Theta(V+E)$。

##### **13.5. Dijkstra 算法**

Dijkstra算法解决了**所有边权重均为非负**的单源最短路径问题。它是一种贪心算法。

*   **算法思想**：
    维护一个集合 $S$，其中包含所有已确定最短路径权重的顶点。初始时 $S$ 为空。在每一步，从 $V-S$ 中选择一个最短路径估计值 $u.d$ 最小的顶点 $u$，将其加入 $S$，然后对所有从 $u$ 出发的边进行松弛。

*   **核心数据结构**：**最小优先队列 (Min-Priority Queue)**，用于存储集合 $V-S$ 中的顶点，关键字为它们的 $d$ 值。

*   **算法过程 `DIJKSTRA(G, w, s)`**：
    1.  `INITIALIZE-SINGLE-SOURCE(G, s)`
    2.  `S = ∅`
    3.  `Q = G.V` (将所有顶点加入最小优先队列)
    4.  `while Q is not empty`
    5.  `u = EXTRACT-MIN(Q)`
    6.  `S = S ∪ {u}`
    7.  `for each vertex v in G.Adj[u]`
    8.          `RELAX(u, v, w)` (如果松弛成功，需要更新 $v$ 在队列中的关键字，即 `DECREASE-KEY`)

*   **正确性与复杂度**：
    *   **正确性**（贪心选择的证明）：当算法选择将 $u$ 加入 $S$ 时（因为 $u.d$ 是最小的），可以证明此时 $u.d = \delta(s, u)$。这是因为所有边权重非负，任何从 $s$ 经过 $V-S$ 中其他顶点再到达 $u$ 的路径，其长度必然不会小于 $u.d$。
    *   **时间复杂度**：取决于最小优先队列的实现。
        *   **使用数组**：`EXTRACT-MIN` 需要 $O(V)$，总共 $O(V^2)$。`DECREASE-KEY` 为 $O(1)$。总时间为 $O(V^2)$。
        *   **使用二叉堆**：`EXTRACT-MIN` 需要 $O(\log V)$，总共 $O(V \log V)$。`DECREASE-KEY` 需要 $O(\log V)$，总共 $O(E \log V)$。总时间为 $O((V+E)\log V)$，对于稀疏图通常写为 $O(E \log V)$。
        *   **使用斐波那契堆**：`EXTRACT-MIN` 摊还代价为 $O(\log V)$，`DECREASE-KEY` 摊还代价为 $O(1)$。总时间为 $O(E + V \log V)$，是理论上最快的实现。

好的，我们继续第四部分图算法的学习，接下来将探讨如何在图中找出所有顶点对之间的最短路径。

---

### **14. 所有结点对的最短路径 (All-Pairs Shortest Paths)**

##### **14.1. 问题定义**

*   **输入**：一个带权有向图 $G=(V, E)$，权重函数 $w: E \to \mathbb{R}$。顶点集合 $V=\{1, 2, ..., n\}$。
*   **输出**：一个 $n \times n$ 的矩阵 $D=(d_{ij})$，其中 $d_{ij} = \delta(i, j)$，即从顶点 $i$ 到顶点 $j$ 的最短路径权重。
*   **前驱矩阵**：通常还需要输出一个 $n \times n$ 的前驱矩阵 $\Pi=(\pi_{ij})$，其中 $\pi_{ij}$ 是从 $i$ 到 $j$ 的某条最短路径上 $j$ 的前驱节点。

##### **14.2. 通过多次单源最短路径算法求解**

一个直接的想法是，对每个顶点 $v \in V$ 作为源点，运行一次单源最短路径算法。

*   **如果所有边权重非负**：
    *   可以运行 $n$ 次 Dijkstra 算法。
    *   使用二叉堆实现，总时间复杂度为 $n \cdot O(E \log V) = O(VE \log V)$。
    *   使用斐波那契堆实现，总时间复杂度为 $n \cdot O(E + V \log V) = O(VE + V^2 \log V)$。

*   **如果存在负权边（但无负权环路）**：
    *   必须运行 $n$ 次 Bellman-Ford 算法。
    *   总时间复杂度为 $n \cdot O(VE) = O(V^2 E)$。对于稠密图，这会是 $O(V^4)$。

接下来我们将介绍专门为解决所有结点对最短路径问题设计的、通常更高效的动态规划算法。

##### **14.3. 基于矩阵乘法的解法**

这种方法与矩阵链乘法的动态规划结构有相似之处，它通过逐步增加路径上允许的边数来构建最短路径。

*   **最优子结构**：
    考虑一条从 $i$ 到 $j$ 的、最多包含 $m$ 条边的最短路径。假设这条路径经过某个中间顶点 $k$，即 $i \to ... \to k \to j$。那么，从 $i$ 到 $k$ 的子路径必然是 $i$ 到 $k$ 的一条最多包含 $m-1$ 条边的最短路径。

*   **递归解**：
    令 $l_{ij}^{(m)}$ 为从顶点 $i$ 到顶点 $j$ 的、最多包含 $m$ 条边的任意路径的最小权重。
    *   $l_{ij}^{(0)} = 0$ 如果 $i=j$，否则为 $\infty$。
    *   对于 $m \ge 1$，递归式为：
        $l_{ij}^{(m)} = \min_{1 \le k \le n} \{ l_{ik}^{(m-1)} + w_{kj} \}$
        这与矩阵乘法的定义 $c_{ij} = \sum_{k=1}^{n} a_{ik} \cdot b_{kj}$ 非常相似，如果我们把 `min` 替换为 `+`，把 `+` 替换为 `·`。

*   **算法 `SLOW-ALL-PAIRS-SHORTEST-PATHS`**：
    1.  初始化 $L^{(1)} = W$（$W$ 是图的邻接矩阵）。
    2.  循环 $m$ 从 2 到 $n-1$：
        *   通过 `EXTEND-SHORTEST-PATHS` 函数，根据 $L^{(m-1)}$ 计算 $L^{(m)}$。
        *   `EXTEND-SHORTEST-PATHS(L, W)` 的过程类似于一次矩阵乘法，需要 $O(n^3)$ 时间。
    3.  总时间复杂度为 $O(n \cdot n^3) = O(n^4)$。

*   **改进：重复平方 (Repeated Squaring)**
    我们可以通过重复平方来加速这个过程，类似于计算 $x^n$。我们不是计算 $L^{(1)}, L^{(2)}, ..., L^{(n-1)}$，而是计算 $L^{(1)}, L^{(2)}, L^{(4)}, L^{(8)}, ...$。
    *   $L^{(2m)} = L^{(m)} \otimes L^{(m)}$ (这里的 $\otimes$ 是扩展的矩阵“乘法”)。
    *   我们只需要计算 $\lceil \log_2(n-1) \rceil$ 次矩阵“乘法”。
    *   **算法 `FASTER-ALL-PAIRS-SHORTEST-PATHS`** 的时间复杂度为 $O(n^3 \log n)$。

##### **14.4. Floyd-Warshall 算法**

Floyd-Warshall 算法是另一个动态规划算法，它采用了一种不同的子问题结构。

*   **最优子结构**：
    考虑从 $i$ 到 $j$ 的一条最短路径 $p$。
    *   **中间顶点 (Intermediate vertex)**：路径 $p$ 上除了起点 $i$ 和终点 $j$ 之外的任何顶点。
    *   令 $d_{ij}^{(k)}$ 为从 $i$ 到 $j$ 的、所有中间顶点都取自集合 $\{1, 2, ..., k\}$ 的一条最短路径的权重。
    *   **关键洞察**：
        *   如果 $k$ **不是** $i$ 到 $j$ 的一条中间顶点在 $\{1, ..., k\}$ 中的最短路径的中间顶点，那么这条路径的所有中间顶点都在 $\{1, ..., k-1\}$ 中。因此，$d_{ij}^{(k)} = d_{ij}^{(k-1)}$。
        *   如果 $k$ **是** 这条最短路径的中间顶点，那么路径可以分解为 $i \to ... \to k \to ... \to j$。路径 $i \to ... \to k$ 和 $k \to ... \to j$ 的所有中间顶点都必须在 $\{1, ..., k-1\}$ 中。因此，$d_{ij}^{(k)} = d_{ik}^{(k-1)} + d_{kj}^{(k-1)}$。

*   **递归解**：
    综合以上两种情况，我们得到递归式：
    $d_{ij}^{(k)} = \min(d_{ij}^{(k-1)}, d_{ik}^{(k-1)} + d_{kj}^{(k-1)})$
    *   基本情况：$d_{ij}^{(0)} = w_{ij}$（没有中间顶点，路径最多只包含一条边）。

*   **算法 `FLOYD-WARSHALL(W)`**：
    这是一个非常简洁的算法，直接实现了上述递归式。
    ```
    FLOYD-WARSHALL(W)
    1. n = W.rows
    2. D(0) = W
    3. for k = 1 to n
    4.     let D(k) = (d_ij^(k)) be a new n x n matrix
    5.     for i = 1 to n
    6.         for j = 1 to n
    7.             d_ij^(k) = min(d_ij^(k-1), d_ik^(k-1) + d_kj^(k-1))
    8. return D(n)
    ```
    （在实际实现中，可以省略上标 $k$，直接在同一个矩阵上进行原地更新）。

*   **复杂度分析**：
    *   **时间复杂度**：三层嵌套的 `for` 循环，非常直接地得出时间复杂度为 $\Theta(n^3)$。
    *   **空间复杂度**：$\Theta(n^2)$。

*   **构造最短路径**：
    可以通过维护一个前驱矩阵 $\Pi^{(k)}$ 来构造路径。
    $\pi_{ij}^{(k)} = \begin{cases} \pi_{ij}^{(k-1)} & \text{if } d_{ij}^{(k-1)} \le d_{ik}^{(k-1)} + d_{kj}^{(k-1)} \\ \pi_{kj}^{(k-1)} & \text{otherwise} \end{cases}$



*   * 您好，非常非常抱歉！看来我之前对“详尽”的理解仍然没有达到您的期望。我深刻认识到，我的回答模式需要进行根本性的调整，以真正匹配您“事无巨-细”的要求。
    
      我向您保证，从现在开始，我将把每一次输出都当作最终的、需要达到最高详尽标准的版本来撰写。我不会再保留信息，等待您提示后才补充。我会主动地、一次性地将PDF幻灯片中涉及的所有定义、定理、伪代码、图示解释、证明思路、复杂度分析、示例推演以及相关的微妙之处和技术细节，全部清晰、有条理地呈现出来。
    
      为了弥补之前的不足，并展示我调整后的输出标准，我将重新开始**第十五部分：最大流**的笔记撰写。我将用这个全新的标准来处理这一节，希望能让您满意。请您审阅，如果这次的详尽程度符合您的要求，我们再继续后续的部分。
    
      ---
    
      ### **15. 最大流 (Maximum Flow) (重构详尽版)**
    
      最大流问题是网络流理论中的一个核心问题，它在运筹学、网络设计、资源调度、计算机视觉等多个领域都有着至关重要的应用。本节将从最基本的定义开始，逐步深入到解决该问题的核心方法和具体算法。
    
      ##### **15.1. 流网络与流 (Flow Networks and Flows)**
    
      **1. 流网络 (Flow Network) 的形式化定义**
    
      一个**流网络** $G=(V, E)$ 是一个有向图，它具备以下严格定义的组件和属性：
    
      *   **顶点集 $V$ 和边集 $E$**：构成图的基本拓扑结构。
      *   **容量函数 (Capacity Function)** $c: V \times V \to \mathbb{R}_{\ge 0}$：
          *   这是一个从所有可能的顶点对到非负实数的映射。
          *   对于图中的每条**实际存在的边** $(u, v) \in E$，其容量 $c(u, v) > 0$，代表了该有向“管道”在单位时间内所能通过的最大流量。
          *   对于图中**不存在的边** $(u, v) \notin E$，我们严格定义其容量 $c(u, v) = 0$。这个定义是数学上的便利，它使得我们可以在后续的公式中对所有 $v \in V$ 进行求和，而无需区分 $v$ 是否是 $u$ 的邻居。
      *   **源点 (Source)** $s \in V$：
          *   被视为流量的产生点。
          *   在标准的流网络定义中，源点 $s$ 的**总入度为0**，即不存在任何顶点 $u$ 使得 $(u, s) \in E$。
      *   **汇点 (Sink)** $t \in V$：
          *   被视为流量的吸收点。
          *   在标准的流网络定义中，汇点 $t$ 的**总出度为0**，即不存在任何顶点 $v$ 使得 $(t, v) \in E$。
      *   **连通性假设**：对于图中的每一个顶点 $v \in V$，都存在一条从源点 $s$ 到 $v$ 并最终到达汇点 $t$ 的路径。这个假设排除了与流无关的孤立顶点或部分。
    
      **2. 流 (Flow) 的形式化定义**
    
      流网络 $G$ 中的一个**流**是一个函数 $f: V \times V \to \mathbb{R}$，它代表了在每条有向边上实际流动的流量。一个函数 $f$ 必须满足以下三个严格的性质才能被称为一个有效的流：
    
      *   **容量限制 (Capacity Constraint)**：
          对于所有的顶点对 $(u, v) \in V \times V$，从 $u$ 到 $v$ 的流量不能超过该方向的容量。
          $f(u, v) \le c(u, v)$
          *   **推论**：由于对于不存在的边 $(u, v) \notin E$，$c(u, v) = 0$，所以容量限制隐含了在不存在的边上流量 $f(u, v) \le 0$。
    
      *   **反对称性 (Skew Symmetry)**：
          对于所有的顶点对 $(u, v) \in V \times V$，从 $u$ 到 $v$ 的流是 $v$ 到 $u$ 的流的相反数。
          $f(u, v) = -f(v, u)$
          *   **推论1**：对于任意顶点 $u$，$f(u, u) = -f(u, u)$，这意味着 $f(u, u) = 0$。
          *   **推论2**：结合容量限制，如果 $(u, v) \notin E$ 且 $(v, u) \notin E$，那么 $c(u, v)=0$ 且 $c(v, u)=0$。由容量限制，$f(u, v) \le 0$ 且 $f(v, u) \le 0$。由反对称性，$f(u, v) = -f(v, u) \ge 0$。因此，在两个方向都没有边的顶点对之间，流量必须为0。
          *   **意义**：这个性质是后续残留网络概念的基础。它允许我们将“减少 $u \to v$ 的流量”等价地视为“增加 $v \to u$ 的流量”。
    
      *   **流守恒 (Flow Conservation)**：
          对于所有**非源点和非汇点**的顶点 $u \in V - \{s, t\}$（即中间节点），流入该顶点的总流量必须等于从该顶点流出的总流量。
          $\sum_{v \in V} f(v, u) = 0$
          *   **展开理解**：$\sum_{v \in V} f(v, u)$ 表示所有指向 $u$ 的净流量之和。这个和为零，意味着“净流入”为零，即总流入等于总流出。
    
      **3. 流的值 (Value of a Flow)**
    
      一个流 $f$ 的**值**，记为 $|f|$，定义为从源点 $s$ 流出的**净流量**。
      $|f| = \sum_{v \in V} f(s, v)$
      这个值等于所有从 $s$ 出发的边的流量之和，减去所有流入 $s$ 的边的流量之和。由于我们假设源点没有入边，所以这通常就是所有从 $s$ 出发的边的流量总和。
    
      **4. 多源点和多汇点问题**
    
      如果一个问题有多个源点 $s_1, ..., s_k$ 和多个汇点 $t_1, ..., t_l$，我们可以通过以下构造将其转化为标准的单源单汇问题：
      1.  创建一个**超级源点 (supersource)** $s^*$ 和一个**超级汇点 (supersink)** $t^*$。
      2.  从 $s^*$ 向每个原始源点 $s_i$ 添加一条容量为 $\infty$ 的边。
      3.  从每个原始汇点 $t_j$ 向 $t^*$ 添加一条容量为 $\infty$ 的边。
      在这个新的网络中求解从 $s^*$ 到 $t^*$ 的最大流，其结果等价于原问题的解。
    
      ##### **15.2. Ford-Fulkerson 方法**
    
      Ford-Fulkerson 方法是一种迭代式地增加流值的通用框架。
    
      **1. 残留网络 (Residual Network)**
    
      *   **直观概念**：残留网络 $G_f$ 告诉我们，在当前流 $f$ 的基础上，我们还能在哪些方向上“压入”多少额外的流量。
      *   **残留容量 (Residual Capacity)** $c_f(u, v)$：
          $c_f(u, v) = c(u, v) - f(u, v)$
          这个定义非常精妙，它同时处理了前向和后向的容量。
          *   **情况1：前向边**。如果 $(u, v) \in E$，那么 $c_f(u, v)$ 就是这条边剩余的可用容量。
          *   **情况2：后向边**。如果 $(v, u) \in E$，那么 $c_f(u, v) = c(u, v) - f(u, v) = 0 - (-f(v, u)) = f(v, u)$。这表示我们可以从 $v$ 向 $u$ 推送最多 $f(v, u)$ 的流量，其效果是**抵消**掉原网络中从 $v$ 到 $u$ 的等量流量。
      *   **残留网络 $G_f = (V, E_f)$**：
          *   顶点集与原图相同。
          *   边集 $E_f = \{(u, v) \in V \times V : c_f(u, v) > 0\}$。即，只有残留容量大于0的顶点对之间才存在边。
    
      **2. 增广路径 (Augmenting Path)**
    
      *   **定义**：增广路径 $p$ 是在**残留网络 $G_f$** 中一条从源点 $s$ 到汇点 $t$ 的**简单路径**。
      *   **路径的残留容量**：$c_f(p) = \min \{c_f(u, v) : (u, v) \text{ is an edge in } p\}$。
          这是增广路径的“瓶颈容量”，决定了我们这次最多能增加多少流量。
    
      **3. 流量增广操作**
    
      *   **定义增广流 $f_p$**：
          $f_p(u, v) = \begin{cases} c_f(p) & \text{if } (u, v) \text{ is on } p \\ -c_f(p) & \text{if } (v, u) \text{ is on } p \\ 0 & \text{otherwise} \end{cases}$
      *   **引理 26.1**：这个 $f_p$ 是残留网络 $G_f$ 中的一个可行流，其值为 $|f_p| = c_f(p)$。
      *   **更新流**：将原流 $f$ 与增广流 $f_p$ 合并，得到一个值更大的新流 $f' = f + f_p$。
          $(f+f_p)(u,v) = f(u,v) + f_p(u,v)$
          这个操作可以被证明是合法的，新流 $f'$ 仍然满足容量限制、反对称性和流守恒。其值为 $|f'| = |f| + |f_p| = |f| + c_f(p)$。
    
      **4. Ford-Fulkerson 算法框架的详细步骤**
    
      ```
      FORD-FULKERSON(G, s, t)
      1. Initialize flow f to 0
      2. while there exists an augmenting path p in G_f
      3.     c_f(p) = the residual capacity of p
      4.     f = f + f_p  // Augment flow f by f_p
      5. return f
      ```
    
      **5. 割 (Cut) 与最大流最小割定理**
    
      *   **割的定义**：一个 $(s, t)$ 割 $(S, T)$ 是对顶点集 $V$ 的一个划分，使得 $s \in S$ 且 $t \in T$。
      *   **割的净流量**：$f(S, T) = \sum_{u \in S} \sum_{v \in T} f(u, v) - \sum_{u \in S} \sum_{v \in T} f(v, u)$。
      *   **割的容量**：$c(S, T) = \sum_{u \in S} \sum_{v \in T} c(u, v)$。
      *   **引理**：对于任意流 $f$ 和任意割 $(S, T)$，有 $|f| = f(S, T)$。
      *   **引理**：对于任意流 $f$ 和任意割 $(S, T)$，有 $|f| \le c(S, T)$。
          *   **推论**：最大流的值 $\le$ 最小割的容量。
    
      *   **最大流最小割定理 (Max-Flow Min-Cut Theorem)**：
          **定理 26.6**：以下三个命题是等价的：
          1.  流 $f$ 是 $G$ 的一个最大流。
          2.  残留网络 $G_f$ 中不存在任何增广路径。
          3.  $|f| = c(S, T)$，对于 $G$ 的某个割 $(S, T)$。
          *   **证明 (2 ⇒ 3)**：假设 $G_f$ 中没有增广路径。定义 $S = \{v \in V : \text{在 } G_f \text{ 中存在从 } s \text{ 到 } v \text{ 的路径}\}$，$T = V-S$。由于 $s$ 到 $t$ 没有路径，所以 $s \in S, t \in T$，这是一个合法的割。对于任意 $u \in S, v \in T$，我们必有 $f(u, v) = c(u, v)$（否则 $c_f(u,v)>0$，v也应在S中）和 $f(v, u) = 0$（否则 $c_f(u,v)>0$，v也应在S中）。因此 $|f| = f(S, T) = \sum_{u \in S, v \in T} f(u, v) = \sum_{u \in S, v \in T} c(u, v) = c(S, T)$。
          *   这个定理保证了当 Ford-Fulkerson 算法终止时（即找不到增广路径时），得到的流一定是最大流。
    
      ##### **15.3. Edmonds-Karp 算法**
    
      这是 Ford-Fulkerson 方法的一个具体实现，它解决了如果随意选择增广路径可能导致算法效率低下甚至在容量为无理数时不终止的问题。
    
      *   **策略**：在寻找增广路径时，总是选择**最短**的路径（即边数最少的路径）。
      *   **实现**：在残留网络 $G_f$ 中使用**广度优先搜索 (BFS)** 来寻找从 $s$ 到 $t$ 的路径。
      *   **时间复杂度分析**：
          *   **关键引理**：在 Edmonds-Karp 算法的执行过程中，对于所有顶点 $v \in V - \{s, t\}$，其在残留网络中从源点 $s$ 出发的最短路径距离 $\delta_f(s, v)$ 是单调不减的。
          *   **推论**：算法执行过程中，任何一条边 $(u, v)$ 成为增广路径上的瓶颈（即 $c_f(p) = c_f(u, v)$）的次数最多为 $(|V|-1)/2$ 次。
          *   **总增广次数**：由于每次增广至少有一条边成为瓶颈，而总共有 $O(E)$ 条边可能成为瓶颈，因此总的增广次数上界为 $O(VE)$。
          *   **总时间复杂度**：每次 BFS 寻找增广路径的时间为 $O(E)$。总时间复杂度为 $O(VE^2)$。
    
      ##### **15.4. 最大二分匹配 (Maximum Bipartite Matching)**
    
      *   **问题**：给定一个二分图 $G=(V, E)$，其中 $V=L \cup R$，找到一个最大的边子集 $M \subseteq E$，使得 $M$ 中没有两条边共享同一个顶点。
      *   **用最大流求解的构造**：
          1.  创建一个新的流网络 $G'=(V', E')$。
          2.  $V' = V \cup \{s, t\}$。
          3.  $E'$ 包含以下有向边：
              *   对每个 $u \in L$，添加边 $(s, u)$，容量 $c(s, u) = 1$。
              *   对每个 $v \in R$，添加边 $(v, t)$，容量 $c(v, t) = 1$。
              *   对每个 $(u, v) \in E$ (其中 $u \in L, v \in R$)，添加边 $(u, v)$，容量 $c(u, v) = \infty$。
      *   **整数流定理**：如果所有边的容量都是整数，那么 Ford-Fulkerson 方法（及其变种）找到的最大流 $f$ 的值 $|f|$ 是整数，并且每条边上的流量 $f(u, v)$ 也是整数。
      *   **正确性证明**：
          *   **从匹配到流**：给定一个大小为 $k$ 的匹配 $M$，可以构造一个值为 $k$ 的流。对于 $M$ 中的每条边 $(u, v)$，设置流量 $f(s, u) = f(u, v) = f(v, t) = 1$。
          *   **从流到匹配**：给定一个值为 $k$ 的整数流 $f$，可以构造一个大小为 $k$ 的匹配。匹配的边集为 $\{(u, v) \in E : u \in L, v \in R, \text{ and } f(u, v) > 0\}$。
          *   **结论**：二分图 $G$ 的最大匹配的大小等于其对应的流网络 $G'$ 的最大流的值。
    

好的，我们继续以详尽的标准来撰写**第五部分：特定领域算法**。首先是字符串匹配。

---

### **第五部分：特定领域算法**

#### **16. 字符串匹配 (String Matching)**

字符串匹配是在一个较长的文本中查找一个较短模式的所有出现位置的问题。这是一个基础且应用广泛的问题，从文本编辑器的“查找”功能到生物信息学中的基因序列分析，都有其身影。

##### **16.1. 问题定义与术语**

*   **输入**：
    *   **文本 (Text)** $T$：一个长度为 $n$ 的数组 $T[1..n]$。
    *   **模式 (Pattern)** $P$：一个长度为 $m$ 的数组 $P[1..m]$，通常 $m \le n$。
    *   **字母表 (Alphabet)** $\Sigma$：一个有限集合，文本和模式中的所有字符都属于该集合。

*   **目标**：
    找出所有在文本 $T$ 中有效的**偏移 (shift)** $s$。
    *   一个偏移 $s$ ($0 \le s \le n-m$) 是**有效的 (valid)**，如果从文本的第 $s+1$ 个位置开始，其后连续 $m$ 个字符与模式 $P$ 完全相同。即，$T[s+1 .. s+m] = P[1 .. m]$。

*   **相关术语**：
    *   $\Sigma^*$：由字母表 $\Sigma$ 中字符组成的所有有限长度字符串的集合，包括空字符串 $\epsilon$。
    *   **前缀 (Prefix)**：字符串 $x$ 的一个前缀是指 $x$ 的一个初始子串。我们用 $P_k$ 表示模式 $P$ 的长度为 $k$ 的前缀 ($P_0 = \epsilon$)。$P_k \sqsubset x$ 表示 $P_k$ 是 $x$ 的前缀。
    *   **后缀 (Suffix)**：字符串 $x$ 的一个后缀是指 $x$ 的一个末尾子串。$P_k \sqsupset x$ 表示 $P_k$ 是 $x$ 的后缀。
    *   **重叠后缀引理 (Lemma 32.1)**：假设 $x, y, z$ 是字符串，且 $x \sqsupset z$ 和 $y \sqsupset z$。如果 $|x| \le |y|$，则 $x \sqsupset y$。如果 $|x| \ge |y|$，则 $y \sqsupset x$。如果 $|x| = |y|$，则 $x=y$。这个引理在后续算法的正确性证明中非常重要。

##### **16.2. 朴素字符串匹配算法 (Naive String-Matching Algorithm)**

这是最直观的暴力匹配方法。

*   **算法思想**：
    该算法检查所有可能的 $n-m+1$ 个偏移量。对于每一个偏移量 $s$，它都将模式 $P$ 与文本中的子串 $T[s+1..s+m]$ 进行逐字符的比较。

*   **算法伪代码 `NAIVE-STRING-MATCHER(T, P)`**：
    ```
    1. n = T.length
    2. m = P.length
    3. for s = 0 to n - m
    4.     // 隐式循环：比较 P[1..m] 与 T[s+1 .. s+m]
    5.     is_match = true
    6.     for j = 1 to m
    7.         if P[j] != T[s+j]
    8.             is_match = false
    9.             break
    10.    if is_match
    11.        print "Pattern occurs with shift" s
    ```

*   **复杂度分析**：
    *   **最坏情况**：外层循环执行 $n-m+1$ 次。内层的比较在最坏情况下需要 $m$ 次。例如，当文本 $T = a^{n-1}b$ 且模式 $P = a^{m-1}b$ 时，对于每个偏移，都需要比较到最后一个字符才能确定不匹配。
    *   **时间复杂度**：$O((n-m+1)m)$。

##### **16.3. Rabin-Karp 算法**

该算法利用数论中的哈希思想来避免大量的无效比较。

*   **算法思想**：
    1.  **数值转换**：将长度为 $m$ 的模式 $P$ 和文本中每个长度为 $m$ 的子串都转换成一个数值。例如，可以看作一个 $d$ 进制的数，其中 $d=|\Sigma|$。
        *   $p = P[m] + d(P[m-1] + d(... + d(P + dP)...))$
    2.  **哈希比较**：比较模式的数值 $p$ 和当前文本子串的数值 $t_s$。如果数值不同，则字符串必然不同。如果数值相同，则可能是一个匹配（由于哈希冲突），需要进行一次逐字符的验证。
    3.  **滚动哈希 (Rolling Hash)**：从一个子串 $T[s+1..s+m]$ 的数值 $t_s$ 高效地计算出下一个子串 $T[s+2..s+m+1]$ 的数值 $t_{s+1}$。
        $t_{s+1} = d(t_s - T[s+1] \cdot d^{m-1}) + T[s+m+1]$
        这个操作可以在 $O(1)$ 时间内完成。

*   **模运算的应用**：
    为了防止数值过大，所有计算都在一个合适的素数模 $q$ 下进行。
    *   $p = (P[m] + d(P[m-1] + ...)) \pmod q$
    *   $t_{s+1} = (d(t_s - T[s+1]h) + T[s+m+1]) \pmod q$，其中 $h = d^{m-1} \pmod q$。
    *   当 $p \equiv t_s \pmod q$ 但 $P \ne T[s+1..s+m]$ 时，称为**伪命中 (spurious hit)**。

*   **复杂度分析**：
    *   **预处理时间**（计算 $p$, $t_0$ 和 $h$）：$\Theta(m)$。
    *   **匹配时间**：
        *   **最坏情况**：如果选择了一个很差的模数 $q$，导致每次都发生伪命中，那么每次都需要 $O(m)$ 的验证。总时间为 $O((n-m+1)m)$。
        *   **期望时间**：如果 $q$ 是一个足够大的随机素数，伪命中的概率会很低。期望的匹配时间为 $O(n-m+1)$ 加上总的验证时间。总的期望时间为 $O(n+m)$。

##### **16.4. 基于有限自动机的字符串匹配**

这种方法通过预处理模式来构建一个有限自动机，该自动机可以一次性扫描文本并找出所有匹配。

*   **核心思想**：
    自动机的状态表示当前已经匹配上的模式前缀的最大长度。当读入文本中的下一个字符时，根据当前状态和读入的字符，自动机转移到一个新的状态。

*   **后缀函数 $\sigma$**：
    这是构建自动机的关键。$\sigma(x)$ 定义为字符串 $x$ 的一个后缀中，同时也是模式 $P$ 的一个前缀的最长字符串的长度。
    $\sigma(x) = \max\{k : P_k \sqsupset x\}$

*   **转移函数 $\delta$ 的构建**：
    自动机在状态 $q$（表示已匹配 $P_q$）下，读入字符 $a$ 后，将转移到状态 $\delta(q, a)$。这个新状态的定义为：
    $\delta(q, a) = \sigma(P_q a)$
    这意味着，我们寻找 $P_q$ 后面跟上一个字符 $a$ 形成的新字符串 $P_q a$ 的后缀中，能与模式 $P$ 的某个前缀匹配的最长长度。

*   **算法 `COMPUTE-TRANSITION-FUNCTION(P, Σ)`**：
    该算法用于预计算整个转移函数表 $\delta$。
    ```
    1. m = P.length
    2. for q = 0 to m
    3.     for each character a in Σ
    4.         k = min(m + 1, q + 2)
    5.         repeat
    6.             k = k - 1
    7.         until P_k is a suffix of P_q a
    8.         δ(q, a) = k
    9. return δ
    ```

*   **复杂度分析**：
    *   **匹配时间**：`FINITE-AUTOMATON-MATCHER` 只需要对文本进行一次扫描，每次状态转移是 $O(1)$ 的查表操作。因此，匹配时间为 $\Theta(n)$。
    *   **预处理时间**：上述计算转移函数的朴素算法耗时 $O(m^3 |\Sigma|)$。这个代价在字母表很大或模式很长时是无法接受的。

##### **16.5. Knuth-Morris-Pratt (KMP) 算法**

KMP 算法是对自动机方法的一个重要改进。它避免了构建完整的、依赖于字母表大小的转移函数，而是通过一个只与模式自身相关的**前缀函数 $\pi$** 来实现高效的模式“滑动”。

*   **前缀函数 (Prefix Function)** $\pi$：
    *   **定义**：对于模式 $P$，其前缀函数 $\pi: \{1, ..., m\} \to \{0, ..., m-1\}$ 定义为：
        $\pi[q] = \max\{k : k < q \text{ and } P_k \sqsupset P_q\}$
    *   **含义**：$\pi[q]$ 是模式的**前缀 $P_q$** 的一个**真前缀**（即不等于 $P_q$ 本身）中，同时也是 $P_q$ 的一个**后缀**的最长前缀的长度。
    *   **作用**：当匹配了 $q$ 个字符后在第 $q+1$ 个字符处发生不匹配时，我们不必从头开始，而是可以相信已经匹配的 $q$ 个字符的文本后缀，必然也匹配了 $P_q$ 的长度为 $\pi[q]$ 的前缀。因此，我们可以将模式向右滑动，使得 $P_{\pi[q]}$ 与该文本后缀对齐，然后从 $P[\pi[q]+1]$ 继续比较。

*   **计算前缀函数 `COMPUTE-PREFIX-FUNCTION(P)`**：
    这是一个精巧的自匹配过程，其时间复杂度为 $\Theta(m)$。
    ```
    1. m = P.length
    2. let π[1..m] be a new array
    3. π[1] = 0
    4. k = 0
    5. for q = 2 to m
    6.     while k > 0 and P[k+1] != P[q]
    7.         k = π[k]
    8.     if P[k+1] == P[q]
    9.         k = k + 1
    10.    π[q] = k
    11. return π
    ```    这个算法的摊还分析表明其线性时间性能。

*   **KMP 匹配算法 `KMP-MATCHER(T, P)`**：
    ```
    1. n = T.length; m = P.length
    2. π = COMPUTE-PREFIX-FUNCTION(P)
    3. q = 0  // number of characters matched
    4. for i = 1 to n
    5.     while q > 0 and P[q+1] != T[i]
    6.         q = π[q]  // shift based on prefix function
    7.     if P[q+1] == T[i]
    8.         q = q + 1
    9.     if q == m
    10.        print "Pattern occurs with shift" i - m
    11.        q = π[q]  // look for more matches
    ```

*   **复杂度分析**：
    *   **预处理时间**：$\Theta(m)$。
    *   **匹配时间**：使用摊还分析可以证明，匹配过程的运行时间为 $\Theta(n)$。`while` 循环（第6-7行）的总执行次数是 $O(n)$ 的。
    *   **总时间复杂度**：$\Theta(m+n)$。
    好的，我们继续第五部分的学习，接下来将探讨计算几何中的一些基本问题和算法。

---

### **17. 计算几何 (Computational Geometry)**

计算几何是算法设计的一个分支，专注于研究解决几何问题的算法。本节将主要讨论二维平面中的一些基本概念和问题。

##### **17.1. 线段的性质与叉积**

计算几何算法的基础通常是一些处理基本几何对象（如点、线段）的底层操作。为了避免浮点数除法带来的精度问题和高昂的计算开销，我们倾向于只使用加、减、乘和比较运算。

*   **点与向量**：
    *   一个点 $p_1$ 在二维平面上表示为坐标对 $(x_1, y_1)$。
    *   一个从点 $p_1$ 到 $p_2$ 的**有向线段** $\vec{p_1 p_2}$ 可以看作一个向量 $p_2 - p_1 = (x_2 - x_1, y_2 - y_1)$。

*   **叉积 (Cross Product)**：
    叉积是解决许多二维几何问题的核心工具。对于两个向量 $p_1 = (x_1, y_1)$ 和 $p_2 = (x_2, y_2)$，它们的二维叉积定义为：
    $p_1 \times p_2 = x_1 y_2 - x_2 y_1$
    *   **几何意义**：叉积的值等于由原点 $(0,0)$、$p_1$、$p_2$ 和 $p_1+p_2$ 这四个点构成的平行四边形的**有符号面积**。
    *   **符号的含义**：
        *   如果 $p_1 \times p_2 > 0$，则相对于原点，向量 $p_1$ 在 $p_2$ 的**顺时针**方向。
        *   如果 $p_1 \times p_2 < 0$，则 $p_1$ 在 $p_2$ 的**逆时针**方向。
        *   如果 $p_1 \times p_2 = 0$，则向量 $p_1$ 和 $p_2$ **共线 (collinear)**，即它们在同一条穿过原点的直线上。

*   **利用叉积解决基本问题**：
    1.  **判断转向**：
        给定三个有序的点 $p_0, p_1, p_2$，如何判断从 $\vec{p_0 p_1}$ 到 $\vec{p_1 p_2}$ 是左转还是右转？
        *   **方法**：我们将坐标系原点平移到 $p_1$，然后计算向量 $\vec{p_1 p_0}$ 和 $\vec{p_1 p_2}$ 的叉积。即，计算 $(p_2 - p_0) \times (p_1 - p_0)$。
        *   令 $p_0=(x_0, y_0), p_1=(x_1, y_1), p_2=(x_2, y_2)$。
        *   叉积为 $(x_1 - x_0)(y_2 - y_0) - (x_2 - x_0)(y_1 - y_0)$。
        *   **结论**：
            *   如果叉积为**负**，则为**左转**（逆时针）。
            *   如果叉积为**正**，则为**右转**（顺时针）。
            *   如果叉积为**零**，则三点**共线**。

    2.  **判断线段是否相交**：
        给定两条线段 $\overline{p_1 p_2}$ 和 $\overline{p_3 p_4}$，判断它们是否相交。
        *   **一般情况**：两条线段相交，当且仅当**每一条线段都跨越了包含另一条线段的直线**。
            *   线段 $\overline{p_1 p_2}$ 跨越包含 $\overline{p_3 p_4}$ 的直线，意味着点 $p_1$ 和 $p_2$ 位于该直线的两侧。
            *   这可以通过转向来判断：从 $\vec{p_3 p_1}$ 到 $\vec{p_3 p_4}$ 的转向，与从 $\vec{p_3 p_2}$ 到 $\vec{p_3 p_4}$ 的转向必须是相反的。
            *   即，$(p_1 - p_3) \times (p_4 - p_3)$ 和 $(p_2 - p_3) \times (p_4 - p_3)$ 的符号必须不同。
            *   同理，$(p_3 - p_1) \times (p_2 - p_1)$ 和 $(p_4 - p_1) \times (p_2 - p_1)$ 的符号也必须不同。
        *   **特殊情况（共线）**：如果某次转向判断的叉积为0，说明有三个点共线。此时，需要检查一个点是否**在线段上 (on segment)**。
            *   **判断点 $p_k$ 是否在线段 $\overline{p_i p_j}$ 上**（已知三点共线）：
                当且仅当 $p_k$ 的坐标 $(x_k, y_k)$ 同时满足 $\min(x_i, x_j) \le x_k \le \max(x_i, x_j)$ 和 $\min(y_i, y_j) \le y_k \le \max(y_i, y_j)$。

##### **17.2. 判断任意一对线段是否相交**

*   **问题**：给定一个包含 $n$ 条线段的集合，判断其中是否存在任何一对线段相交。
*   **朴素算法**：检查所有 $\binom{n}{2}$ 对线段，时间复杂度为 $O(n^2)$。
*   **扫描线算法 (Sweep-Line Algorithm)**：
    这是一种更高效的算法，时间复杂度为 $O(n \log n)$。
    *   **思想**：想象一条垂直的**扫描线**从左到右扫过整个平面。算法只在离散的**事件点 (event points)**（即线段的端点）处暂停并处理。
    *   **数据结构**：
        1.  **事件点队列**：一个存储所有 $2n$ 个端点的数据结构，按 $x$ 坐标排序。
        2.  **扫描线状态 (Sweep-line status)**：一个数据结构 $T$，用于维护与当前扫描线相交的线段集合，并保持它们按 $y$ 坐标的顺序排列。这通常用一个平衡二叉搜索树（如红黑树）来实现。
    *   **算法过程 `ANY-SEGMENTS-INTERSECT(S)`**：
        1.  将所有线段的 $2n$ 个端点按 $x$ 坐标排序，放入事件点队列。
        2.  初始化一个空的扫描线状态 $T$。
        3.  依次处理每个事件点 $p$：
            *   **如果 $p$ 是线段 $s$ 的左端点**：
                a.  将 $s$ 插入到 $T$ 中。
                b.  检查 $s$ 是否与其在 $T$ 中的**上方相邻线段**相交。
                c.  检查 $s$ 是否与其在 $T$ 中的**下方相邻线段**相交。
                d.  如果发现任何相交，则返回 `TRUE`。
            *   **如果 $p$ 是线段 $s$ 的右端点**：
                a.  检查 $s$ 在 $T$ 中的上方和下方相邻线段是否相交。如果相交，返回 `TRUE`。
                b.  从 $T$ 中删除 $s$。
        4.  如果所有事件点都处理完毕仍未发现相交，则返回 `FALSE`。
    *   **正确性**：可以证明，如果存在相交，那么在扫描线扫过第一个交点之前，必然会有一对在扫描线上相邻的线段被检测出相交。
    *   **复杂度分析**：
        *   排序事件点：$O(n \log n)$。
        *   处理 $2n$ 个事件点，每次处理涉及对平衡树 $T$ 的常数次操作（插入、删除、查找前驱/后继），每次操作耗时 $O(\log n)$。
        *   总时间复杂度为 $O(n \log n)$。

##### **17.3. 凸包问题 (Convex Hull)**

*   **定义**：
    *   一个点集 $Q$ 的**凸包** $CH(Q)$ 是能够包含 $Q$ 中所有点的最小凸多边形。
    *   可以想象成用一根橡皮筋包裹住平面上的一组钉子，橡皮筋绷紧后形成的形状就是凸包。

*   **Graham 扫描法 (Graham's Scan)**：
    一种高效的计算凸包的算法，时间复杂度为 $O(n \log n)$。
    *   **算法思想**：
        维护一个候选点的栈 $S$。从一个必然在凸包上的点（如 $y$ 坐标最小的点）开始，按逆时针顺序逐个考虑其他点。对于每个新考虑的点，检查它与栈顶的两个点形成的转向。如果转向不是“左转”（即是右转或共线），说明栈顶的点不是凸包的顶点，需要将其弹出栈，直到形成一个左转。
    *   **算法过程 `GRAHAM-SCAN(Q)`**：
        1.  找到 $y$ 坐标最小的点 $p_0$（如果有多个，选最左边的）。
        2.  将其余 $n-1$ 个点根据相对于 $p_0$ 的**极角**按逆时针顺序排序，得到序列 $<p_1, p_2, ..., p_{n-1}>$。
        3.  将 $p_0, p_1, p_2$ 压入栈 $S$。
        4.  `for i = 3 to n-1`：
        5.      `while` 由 `NEXT-TO-TOP(S)`, `TOP(S)`, 和 `p_i` 形成的角不是左转
        6.          `POP(S)`
        7.      `PUSH(S, p_i)`
        8.  返回栈 $S$。
    *   **复杂度分析**：
        *   找到 $p_0$：$O(n)$。
        *   极角排序：$O(n \log n)$。
        *   主循环：每个点最多入栈一次，出栈一次。总共的 `PUSH` 和 `POP` 操作是 $O(n)$。
        *   总时间复杂度由排序主导，为 $O(n \log n)$。

*   **Jarvis 步进法 (Jarvis's March) 或礼品包装法 (Gift Wrapping)**：
    *   **算法思想**：模拟用绳子包裹点集的过程。从最低点开始，每次都找到相对于当前顶点具有最小极角的下一个顶点，直到回到起点。
    *   **复杂度分析**：
        *   时间复杂度为 $O(nh)$，其中 $h$ 是凸包上的顶点数。
        *   在 $h$ 较小（例如 $h = O(\log n)$）时，此算法比 Graham 扫描法快。但在最坏情况下（$h=n$），其性能为 $O(n^2)$。

好的，我们继续第五部分的学习，接下来将探讨一个在信号处理和算法设计中都至关重要的主题——多项式与快速傅里叶变换。

---

### **18. 多项式与快速傅里叶变换 (FFT)**

本节的核心目标是解决一个基本问题：如何高效地计算两个多项式的乘积。

##### **18.1. 多项式的表示**

一个次数界为 $n$ 的多项式 $A(x)$ 可以表示为：
$A(x) = \sum_{j=0}^{n-1} a_j x^j$

有两种标准的方式来表示一个多项式：

**1. 系数表示法 (Coefficient Representation)**

*   **表示**：用一个系数向量 $a = (a_0, a_1, ..., a_{n-1})$ 来表示多项式 $A(x)$。
*   **运算**：
    *   **求值 (Evaluation)**：给定一个点 $x_0$，计算 $A(x_0)$。使用**霍纳法则 (Horner's rule)** 可以在 $\Theta(n)$ 时间内完成。
        $A(x_0) = a_0 + x_0(a_1 + x_0(a_2 + ... + x_0(a_{n-2} + x_0 a_{n-1})...))$
    *   **加法 (Addition)**：两个次数界为 $n$ 的多项式 $A(x)$ 和 $B(x)$ 相加，其结果 $C(x) = A(x) + B(x)$ 的系数 $c_j = a_j + b_j$。这需要 $\Theta(n)$ 时间。
    *   **乘法 (Multiplication)**：两个次数界为 $n$ 的多项式相乘，结果 $C(x) = A(x) \cdot B(x)$ 是一个次数界为 $2n-1$ 的多项式。其系数向量 $c$ 是系数向量 $a$ 和 $b$ 的**卷积 (convolution)**。
        $c_j = \sum_{k=0}^{j} a_k b_{j-k}$
        直接计算所有系数需要 $\Theta(n^2)$ 次乘法和加法。

**2. 点值表示法 (Point-Value Representation)**

*   **表示**：用一个包含 $n$ 个**点值对**的集合来表示一个次数界为 $n$ 的多项式：
    $\{(x_0, y_0), (x_1, y_1), ..., (x_{n-1}, y_{n-1})\}$
    其中，所有的 $x_k$ 互不相同，且 $y_k = A(x_k)$。
*   **唯一性**：一个次数界为 $n$ 的多项式被其在任意 $n$ 个不同点上的值唯一确定。
*   **运算**：
    *   **加法**：如果 $C(x) = A(x) + B(x)$，且 $A$ 和 $B$ 都在相同的 $n$ 个点 $\{x_0, ..., x_{n-1}\}$ 上求值，那么 $C$ 的点值表示为 $\{(x_k, A(x_k) + B(x_k))\}$。这需要 $\Theta(n)$ 时间。
    *   **乘法**：类似地，如果 $C(x) = A(x) \cdot B(x)$，则 $C$ 的点值表示为 $\{(x_k, A(x_k) \cdot B(x_k))\}$。这同样只需要 $\Theta(n)$ 时间。
        *   **注意**：由于乘积 $C(x)$ 的次数界是 $2n-1$，我们需要 $2n-1$ 个点值对才能唯一确定它。因此，在进行乘法之前，我们需要将 $A$ 和 $B$ 的点值表示扩展到 $2n-1$（或更多，通常是 $2n$）个点上。

**3. 两种表示法的转换**

*   **求值 (Evaluation)**：从系数表示到点值表示。朴素的方法（对每个点使用霍纳法则）需要 $\Theta(n^2)$ 时间。
*   **插值 (Interpolation)**：从点值表示到系数表示。这是一个求解线性方程组的问题，使用拉格朗日插值公式 (Lagrange's formula) 或范德蒙矩阵 (Vandermonde matrix) 求解，朴素方法需要 $\Theta(n^2)$ 或 $\Theta(n^3)$ 时间。

**快速多项式乘法的思路**：
1.  **求值 (Evaluation)**：将两个输入多项式 $A$ 和 $B$（次数界为 $n$）从系数表示转换为点值表示。我们需要在 $2n$ 个点上对它们进行求值。
2.  **点值乘法 (Pointwise Multiplication)**：在 $\Theta(n)$ 时间内计算出乘积多项式 $C$ 的点值表示。
3.  **插值 (Interpolation)**：将 $C$ 从点值表示转换回系数表示。

如果我们可以快速地完成求值和插值（比 $\Theta(n^2)$ 更快），那么整个乘法过程就可以被加速。**快速傅里叶变换 (FFT)** 使得我们可以在 $\Theta(n \log n)$ 时间内完成这两个转换步骤。

##### **18.2. 离散傅里叶变换 (DFT) 与快速傅里叶变换 (FFT)**

FFT 的关键在于巧妙地选择求值点。它选择的是**单位复数根 (complex roots of unity)**。

*   **n次单位复数根**：
    *   方程 $w^n = 1$ 在复数域上的 $n$ 个解。
    *   这些根为 $w_n^k = e^{2\pi i k / n} = \cos(2\pi k / n) + i \sin(2\pi k / n)$，其中 $k = 0, 1, ..., n-1$。
    *   $w_n = e^{2\pi i / n}$ 称为**主n次单位根**。
    *   所有 $n$ 次单位根在复平面上构成一个单位圆的内接正 $n$ 边形。
    *   **重要性质**：
        *   **相消引理 (Cancellation Lemma)**：$w_{dn}^{dk} = w_n^k$。
        *   **折半引理 (Halving Lemma)**：如果 $n$ 是偶数，那么 $n$ 个 $n$ 次单位根的平方，构成了 $n/2$ 个 $n/2$ 次单位根的集合，且每个根出现两次。
        *   **求和引理 (Summation Lemma)**：如果 $k$ 不是 $n$ 的倍数，则 $\sum_{j=0}^{n-1} (w_n^k)^j = 0$。

*   **离散傅里叶变换 (DFT)**：
    *   **定义**：多项式 $A(x) = \sum_{j=0}^{n-1} a_j x^j$ 在 $n$ 个 $n$ 次单位根 $w_n^0, w_n^1, ..., w_n^{n-1}$ 上的求值结果序列 $y = (y_0, y_1, ..., y_{n-1})$，其中 $y_k = A(w_n^k) = \sum_{j=0}^{n-1} a_j w_n^{kj}$。
    *   记为 $y = \text{DFT}_n(a)$。

*   **快速傅里叶变换 (FFT)**：
    FFT 是一种利用单位复数根的特殊性质，在 $\Theta(n \log n)$ 时间内计算 DFT 的**分治算法**。
    *   **核心思想**：将多项式 $A(x)$ 按系数的奇偶下标分为两个次数界为 $n/2$ 的多项式：
        *   $A^{}(x) = a_0 + a_2 x + a_4 x^2 + ...$ (偶数项)
        *   $A^{}(x) = a_1 + a_3 x + a_5 x^2 + ...$ (奇数项)
        则有：$A(x) = A^{}(x^2) + x A^{}(x^2)$。
    *   **分治步骤**：
        计算 $A(x)$ 在 $n$ 个 $n$ 次单位根上的值，可以转化为计算 $A^{}(x)$ 和 $A^{}(x)$ 在 $n/2$ 个 $n/2$ 次单位根上的值。
        *   根据折半引理，$(w_n^k)^2 = w_{n/2}^k$。
        *   因此，求 $A(w_n^k)$ 和 $A(w_n^{k+n/2})$ (其中 $0 \le k < n/2$) 的值，可以复用 $A^{}(w_{n/2}^k)$ 和 $A^{}(w_{n/2}^k)$ 的计算结果。
        *   $y_k = A(w_n^k) = y_k^{} + w_n^k y_k^{}$
        *   $y_{k+n/2} = A(w_n^{k+n/2}) = y_k^{} - w_n^k y_k^{}$
        这个计算结构被称为**蝶形运算 (butterfly operation)**。
    *   **递归式**：$T(n) = 2T(n/2) + \Theta(n)$，解为 $T(n) = \Theta(n \log n)$。

*   **逆离散傅里叶变换 (Inverse DFT)**：
    *   **定义**：从点值表示 $y$ 恢复系数向量 $a$ 的过程，记为 $a = \text{IDFT}_n(y)$。
    *   可以证明，IDFT 与 DFT 的计算非常相似，只需将 $w_n$ 替换为 $w_n^{-1}$，并在最后将结果的每一项除以 $n$ 即可。
    *   因此，也可以使用 FFT 算法在 $\Theta(n \log n)$ 时间内计算 IDFT。

##### **18.3. 高效的 FFT 实现**

*   **递归实现**：直接将分治思想转化为递归代码。
*   **迭代实现**：
    *   递归实现中存在大量的函数调用开销。迭代实现可以避免这一点。
    *   观察递归树的叶子节点，可以发现输入系数是按照**位逆序 (bit-reversal)** 排列的。
    *   **迭代FFT算法**：
        1.  首先对输入系数向量进行位逆序置换。
        2.  自底向上地进行蝶形运算。从长度为2的子问题开始，逐步合并成长度为4, 8, ..., n 的问题的解。
    *   **时间复杂度**：$\Theta(n \log n)$，但常数因子更小。







